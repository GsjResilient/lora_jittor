==================================================================================================== - 2025-03-26 22:12:38,142 - log
        - random_seed : 2025 - 2025-03-26 22:12:38,142 - log
        - lr : 0.0002 - 2025-03-26 22:12:38,142 - log
        - weight_decay : 0.01 - 2025-03-26 22:12:38,142 - log
        - correct_bias : False - 2025-03-26 22:12:38,142 - log
        - adam_epislon : 1e-06 - 2025-03-26 22:12:38,142 - log
        - no_decay_bias : False - 2025-03-26 22:12:38,142 - log
        - adam_beta1 : 0.9 - 2025-03-26 22:12:38,142 - log
        - adam_beta2 : 0.999 - 2025-03-26 22:12:38,142 - log
        - scheduler : linear - 2025-03-26 22:12:38,142 - log
        - max_step : None - 2025-03-26 22:12:38,142 - log
        - max_epoch : 5 - 2025-03-26 22:12:38,142 - log
        - warmup_step : 500 - 2025-03-26 22:12:38,142 - log
        - i_steps : 0 - 2025-03-26 22:12:38,142 - log
        - i_lrs : 0.00025 - 2025-03-26 22:12:38,142 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-26 22:12:38,142 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-26 22:12:38,142 - log
        - train_batch_size : 2 - 2025-03-26 22:12:38,142 - log
        - valid_batch_size : 1 - 2025-03-26 22:12:38,142 - log
        - grad_acc : 2 - 2025-03-26 22:12:38,142 - log
        - clip : 0.0 - 2025-03-26 22:12:38,142 - log
        - seq_len : 64 - 2025-03-26 22:12:38,142 - log
        - model_card : gpt2.sm - 2025-03-26 22:12:38,142 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-26 22:12:38,142 - log
        - fp16 : False - 2025-03-26 22:12:38,142 - log
        - log_interval : 100 - 2025-03-26 22:12:38,142 - log
        - eval_interval : 2000 - 2025-03-26 22:12:38,142 - log
        - save_interval : 1000 - 2025-03-26 22:12:38,142 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-26 22:12:38,142 - log
        - lora_dim : 4 - 2025-03-26 22:12:38,142 - log
        - lora_alpha : 32 - 2025-03-26 22:12:38,142 - log
        - obj : clm - 2025-03-26 22:12:38,142 - log
        - lora_dropout : 0.1 - 2025-03-26 22:12:38,142 - log
        - label_smooth : 0.1 - 2025-03-26 22:12:38,142 - log
        - roll_interval : -1 - 2025-03-26 22:12:38,143 - log
        - roll_lr : 1e-05 - 2025-03-26 22:12:38,143 - log
        - roll_step : 100 - 2025-03-26 22:12:38,143 - log
        - eval_epoch : 1 - 2025-03-26 22:12:38,143 - log
        - device : cuda - 2025-03-26 22:12:38,143 - log
==================================================================================================== - 2025-03-26 22:12:38,143 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-26 22:12:38,143 - log
loading model pretrained weight. - 2025-03-26 22:12:38,253 - log
set max_step: 1050 - 2025-03-26 22:12:39,312 - log
start to train the model................ 1 - 2025-03-26 22:12:39,312 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 42.80 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-26 22:12:43,592 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 38.96 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-26 22:12:47,488 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pkl - 2025-03-26 22:12:47,877 - log
start to train the model................ 2 - 2025-03-26 22:12:49,813 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 35.45 | loss  3.23 | avg loss  3.57 | ppl 35.69 - 2025-03-26 22:12:53,358 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 39.03 | loss  4.11 | avg loss  3.40 | ppl 30.03 - 2025-03-26 22:12:57,262 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pkl - 2025-03-26 22:12:58,043 - log
start to train the model................ 3 - 2025-03-26 22:12:59,910 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 31.67 | loss  3.38 | avg loss  3.23 | ppl 25.37 - 2025-03-26 22:13:03,077 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 39.10 | loss  3.04 | avg loss  3.20 | ppl 24.49 - 2025-03-26 22:13:06,987 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pkl - 2025-03-26 22:13:08,160 - log
start to train the model................ 4 - 2025-03-26 22:13:10,040 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 27.44 | loss  3.46 | avg loss  3.15 | ppl 23.22 - 2025-03-26 22:13:12,784 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 39.17 | loss  3.04 | avg loss  3.01 | ppl 20.38 - 2025-03-26 22:13:16,702 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pkl - 2025-03-26 22:13:18,269 - log
start to train the model................ 5 - 2025-03-26 22:13:20,136 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 23.59 | loss  2.47 | avg loss  3.04 | ppl 20.95 - 2025-03-26 22:13:22,495 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 39.51 | loss  3.83 | avg loss  3.01 | ppl 20.36 - 2025-03-26 22:13:26,447 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-03-26 22:13:26,447 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pkl - 2025-03-26 22:13:28,426 - log
---------------------------------------------------------------------------------------------------- - 2025-03-26 22:13:30,337 - log
End of training - 2025-03-26 22:13:30,337 - log
ms/batch 35.67 - 2025-03-26 22:13:30,337 - log
==================================================================================================== - 2025-03-26 22:15:53,558 - log
        - random_seed : 2025 - 2025-03-26 22:15:53,558 - log
        - lr : 0.0002 - 2025-03-26 22:15:53,558 - log
        - weight_decay : 0.01 - 2025-03-26 22:15:53,558 - log
        - correct_bias : False - 2025-03-26 22:15:53,558 - log
        - adam_epislon : 1e-06 - 2025-03-26 22:15:53,558 - log
        - no_decay_bias : False - 2025-03-26 22:15:53,558 - log
        - adam_beta1 : 0.9 - 2025-03-26 22:15:53,558 - log
        - adam_beta2 : 0.999 - 2025-03-26 22:15:53,558 - log
        - scheduler : linear - 2025-03-26 22:15:53,558 - log
        - max_step : None - 2025-03-26 22:15:53,559 - log
        - max_epoch : 5 - 2025-03-26 22:15:53,559 - log
        - warmup_step : 500 - 2025-03-26 22:15:53,559 - log
        - i_steps : 0 - 2025-03-26 22:15:53,559 - log
        - i_lrs : 0.00025 - 2025-03-26 22:15:53,559 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-26 22:15:53,559 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-26 22:15:53,559 - log
        - train_batch_size : 2 - 2025-03-26 22:15:53,559 - log
        - valid_batch_size : 1 - 2025-03-26 22:15:53,559 - log
        - grad_acc : 2 - 2025-03-26 22:15:53,559 - log
        - clip : 0.0 - 2025-03-26 22:15:53,559 - log
        - seq_len : 64 - 2025-03-26 22:15:53,559 - log
        - model_card : gpt2.sm - 2025-03-26 22:15:53,559 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-26 22:15:53,559 - log
        - fp16 : False - 2025-03-26 22:15:53,559 - log
        - log_interval : 100 - 2025-03-26 22:15:53,559 - log
        - eval_interval : 2000 - 2025-03-26 22:15:53,559 - log
        - save_interval : 1000 - 2025-03-26 22:15:53,559 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-26 22:15:53,559 - log
        - lora_dim : 4 - 2025-03-26 22:15:53,559 - log
        - lora_alpha : 32 - 2025-03-26 22:15:53,559 - log
        - obj : clm - 2025-03-26 22:15:53,559 - log
        - lora_dropout : 0.1 - 2025-03-26 22:15:53,559 - log
        - label_smooth : 0.1 - 2025-03-26 22:15:53,559 - log
        - roll_interval : -1 - 2025-03-26 22:15:53,559 - log
        - roll_lr : 1e-05 - 2025-03-26 22:15:53,559 - log
        - roll_step : 100 - 2025-03-26 22:15:53,559 - log
        - eval_epoch : 1 - 2025-03-26 22:15:53,559 - log
        - device : cuda - 2025-03-26 22:15:53,559 - log
==================================================================================================== - 2025-03-26 22:15:53,559 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-26 22:15:53,559 - log
loading model pretrained weight. - 2025-03-26 22:15:53,670 - log
set max_step: 1050 - 2025-03-26 22:15:54,754 - log
start to train the model................ 1 - 2025-03-26 22:15:54,755 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 43.00 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-26 22:15:59,055 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 38.92 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-26 22:16:02,947 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pkl - 2025-03-26 22:16:03,300 - log
start to train the model................ 2 - 2025-03-26 22:16:05,318 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 35.29 | loss  3.23 | avg loss  3.57 | ppl 35.69 - 2025-03-26 22:16:08,847 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 39.00 | loss  4.11 | avg loss  3.40 | ppl 30.03 - 2025-03-26 22:16:12,748 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pkl - 2025-03-26 22:16:13,493 - log
start to train the model................ 3 - 2025-03-26 22:16:15,417 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 31.61 | loss  3.38 | avg loss  3.23 | ppl 25.37 - 2025-03-26 22:16:18,578 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 39.12 | loss  3.04 | avg loss  3.20 | ppl 24.49 - 2025-03-26 22:16:22,491 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pkl - 2025-03-26 22:16:23,634 - log
start to train the model................ 4 - 2025-03-26 22:16:25,521 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 27.72 | loss  3.46 | avg loss  3.15 | ppl 23.22 - 2025-03-26 22:16:28,293 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 39.15 | loss  3.04 | avg loss  3.01 | ppl 20.38 - 2025-03-26 22:16:32,208 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pkl - 2025-03-26 22:16:33,736 - log
start to train the model................ 5 - 2025-03-26 22:16:35,640 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 24.29 | loss  2.47 | avg loss  3.04 | ppl 20.95 - 2025-03-26 22:16:38,069 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 39.12 | loss  3.83 | avg loss  3.01 | ppl 20.36 - 2025-03-26 22:16:41,981 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-03-26 22:16:41,982 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pkl - 2025-03-26 22:16:43,916 - log
---------------------------------------------------------------------------------------------------- - 2025-03-26 22:16:45,938 - log
End of training - 2025-03-26 22:16:45,939 - log
ms/batch 35.72 - 2025-03-26 22:16:45,939 - log
==================================================================================================== - 2025-03-27 07:17:39,865 - log
        - random_seed : 2025 - 2025-03-27 07:17:39,866 - log
        - lr : 0.0002 - 2025-03-27 07:17:39,866 - log
        - weight_decay : 0.01 - 2025-03-27 07:17:39,866 - log
        - correct_bias : False - 2025-03-27 07:17:39,866 - log
        - adam_epislon : 1e-06 - 2025-03-27 07:17:39,866 - log
        - no_decay_bias : False - 2025-03-27 07:17:39,866 - log
        - adam_beta1 : 0.9 - 2025-03-27 07:17:39,866 - log
        - adam_beta2 : 0.999 - 2025-03-27 07:17:39,866 - log
        - scheduler : linear - 2025-03-27 07:17:39,866 - log
        - max_step : None - 2025-03-27 07:17:39,866 - log
        - max_epoch : 5 - 2025-03-27 07:17:39,866 - log
        - warmup_step : 500 - 2025-03-27 07:17:39,866 - log
        - i_steps : 0 - 2025-03-27 07:17:39,866 - log
        - i_lrs : 0.00025 - 2025-03-27 07:17:39,866 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-27 07:17:39,866 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-27 07:17:39,866 - log
        - train_batch_size : 2 - 2025-03-27 07:17:39,866 - log
        - valid_batch_size : 1 - 2025-03-27 07:17:39,866 - log
        - grad_acc : 2 - 2025-03-27 07:17:39,866 - log
        - clip : 0.0 - 2025-03-27 07:17:39,866 - log
        - seq_len : 64 - 2025-03-27 07:17:39,866 - log
        - model_card : gpt2.sm - 2025-03-27 07:17:39,866 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-27 07:17:39,866 - log
        - fp16 : False - 2025-03-27 07:17:39,866 - log
        - log_interval : 100 - 2025-03-27 07:17:39,866 - log
        - eval_interval : 2000 - 2025-03-27 07:17:39,866 - log
        - save_interval : 1000 - 2025-03-27 07:17:39,866 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-27 07:17:39,866 - log
        - lora_dim : 4 - 2025-03-27 07:17:39,866 - log
        - lora_alpha : 32 - 2025-03-27 07:17:39,866 - log
        - obj : clm - 2025-03-27 07:17:39,866 - log
        - lora_dropout : 0.1 - 2025-03-27 07:17:39,866 - log
        - label_smooth : 0.1 - 2025-03-27 07:17:39,866 - log
        - roll_interval : -1 - 2025-03-27 07:17:39,866 - log
        - roll_lr : 1e-05 - 2025-03-27 07:17:39,866 - log
        - roll_step : 100 - 2025-03-27 07:17:39,866 - log
        - eval_epoch : 1 - 2025-03-27 07:17:39,866 - log
        - device : cuda - 2025-03-27 07:17:39,866 - log
==================================================================================================== - 2025-03-27 07:17:39,866 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-27 07:17:39,866 - log
loading model pretrained weight. - 2025-03-27 07:17:39,995 - log
set max_step: 1050 - 2025-03-27 07:17:41,110 - log
start to train the model................ 1 - 2025-03-27 07:17:41,111 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 36.71 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-27 07:17:44,782 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.23 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-27 07:17:48,005 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pkl - 2025-03-27 07:17:48,299 - log
start to train the model................ 2 - 2025-03-27 07:17:50,019 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.64 | loss  3.23 | avg loss  3.57 | ppl 35.69 - 2025-03-27 07:17:52,983 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.41 | loss  4.11 | avg loss  3.40 | ppl 30.03 - 2025-03-27 07:17:56,224 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pkl - 2025-03-27 07:17:56,840 - log
start to train the model................ 3 - 2025-03-27 07:17:58,576 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.51 | loss  3.38 | avg loss  3.23 | ppl 25.37 - 2025-03-27 07:18:01,227 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.38 | loss  3.04 | avg loss  3.20 | ppl 24.49 - 2025-03-27 07:18:04,465 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pkl - 2025-03-27 07:18:05,411 - log
start to train the model................ 4 - 2025-03-27 07:18:07,095 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.95 | loss  3.46 | avg loss  3.15 | ppl 23.22 - 2025-03-27 07:18:09,391 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.51 | loss  3.04 | avg loss  3.01 | ppl 20.38 - 2025-03-27 07:18:12,642 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pkl - 2025-03-27 07:18:13,910 - log
start to train the model................ 5 - 2025-03-27 07:18:15,607 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.77 | loss  2.47 | avg loss  3.04 | ppl 20.95 - 2025-03-27 07:18:17,584 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.53 | loss  3.83 | avg loss  3.01 | ppl 20.36 - 2025-03-27 07:18:20,838 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-03-27 07:18:20,838 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pkl - 2025-03-27 07:18:22,434 - log
---------------------------------------------------------------------------------------------------- - 2025-03-27 07:18:24,106 - log
End of training - 2025-03-27 07:18:24,106 - log
ms/batch 29.76 - 2025-03-27 07:18:24,106 - log
==================================================================================================== - 2025-03-27 07:19:34,470 - log
        - random_seed : 2025 - 2025-03-27 07:19:34,470 - log
        - lr : 0.0002 - 2025-03-27 07:19:34,470 - log
        - weight_decay : 0.01 - 2025-03-27 07:19:34,470 - log
        - correct_bias : False - 2025-03-27 07:19:34,470 - log
        - adam_epislon : 1e-06 - 2025-03-27 07:19:34,470 - log
        - no_decay_bias : False - 2025-03-27 07:19:34,470 - log
        - adam_beta1 : 0.9 - 2025-03-27 07:19:34,470 - log
        - adam_beta2 : 0.999 - 2025-03-27 07:19:34,470 - log
        - scheduler : linear - 2025-03-27 07:19:34,470 - log
        - max_step : None - 2025-03-27 07:19:34,470 - log
        - max_epoch : 5 - 2025-03-27 07:19:34,470 - log
        - warmup_step : 500 - 2025-03-27 07:19:34,470 - log
        - i_steps : 0 - 2025-03-27 07:19:34,470 - log
        - i_lrs : 0.00025 - 2025-03-27 07:19:34,470 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-27 07:19:34,470 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-27 07:19:34,471 - log
        - train_batch_size : 2 - 2025-03-27 07:19:34,471 - log
        - valid_batch_size : 1 - 2025-03-27 07:19:34,471 - log
        - grad_acc : 2 - 2025-03-27 07:19:34,471 - log
        - clip : 0.0 - 2025-03-27 07:19:34,471 - log
        - seq_len : 64 - 2025-03-27 07:19:34,471 - log
        - model_card : gpt2.sm - 2025-03-27 07:19:34,471 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-27 07:19:34,471 - log
        - fp16 : False - 2025-03-27 07:19:34,471 - log
        - log_interval : 100 - 2025-03-27 07:19:34,471 - log
        - eval_interval : 2000 - 2025-03-27 07:19:34,471 - log
        - save_interval : 1000 - 2025-03-27 07:19:34,471 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-27 07:19:34,471 - log
        - lora_dim : 4 - 2025-03-27 07:19:34,471 - log
        - lora_alpha : 32 - 2025-03-27 07:19:34,471 - log
        - obj : clm - 2025-03-27 07:19:34,471 - log
        - lora_dropout : 0.1 - 2025-03-27 07:19:34,471 - log
        - label_smooth : 0.1 - 2025-03-27 07:19:34,471 - log
        - roll_interval : -1 - 2025-03-27 07:19:34,471 - log
        - roll_lr : 1e-05 - 2025-03-27 07:19:34,471 - log
        - roll_step : 100 - 2025-03-27 07:19:34,471 - log
        - eval_epoch : 1 - 2025-03-27 07:19:34,471 - log
        - device : cuda - 2025-03-27 07:19:34,471 - log
==================================================================================================== - 2025-03-27 07:19:34,471 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-27 07:19:34,471 - log
loading model pretrained weight. - 2025-03-27 07:19:34,585 - log
set max_step: 1050 - 2025-03-27 07:19:35,344 - log
start to train the model................ 1 - 2025-03-27 07:19:35,345 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.64 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-27 07:19:38,809 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.16 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-27 07:19:42,025 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pkl - 2025-03-27 07:19:42,318 - log
start to train the model................ 2 - 2025-03-27 07:19:44,096 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.55 | loss  3.23 | avg loss  3.57 | ppl 35.69 - 2025-03-27 07:19:47,050 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.35 | loss  4.11 | avg loss  3.40 | ppl 30.03 - 2025-03-27 07:19:50,286 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pkl - 2025-03-27 07:19:50,901 - log
start to train the model................ 3 - 2025-03-27 07:19:52,633 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.07 | loss  3.38 | avg loss  3.23 | ppl 25.37 - 2025-03-27 07:19:55,240 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.27 | loss  3.04 | avg loss  3.20 | ppl 24.49 - 2025-03-27 07:19:58,467 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pkl - 2025-03-27 07:19:59,409 - log
start to train the model................ 4 - 2025-03-27 07:20:01,203 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.95 | loss  3.46 | avg loss  3.15 | ppl 23.22 - 2025-03-27 07:20:03,499 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.39 | loss  3.04 | avg loss  3.01 | ppl 20.38 - 2025-03-27 07:20:06,738 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pkl - 2025-03-27 07:20:08,004 - log
start to train the model................ 5 - 2025-03-27 07:20:09,750 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.76 | loss  2.47 | avg loss  3.04 | ppl 20.95 - 2025-03-27 07:20:11,726 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.47 | loss  3.83 | avg loss  3.01 | ppl 20.36 - 2025-03-27 07:20:14,973 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-03-27 07:20:14,973 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pkl - 2025-03-27 07:20:16,573 - log
---------------------------------------------------------------------------------------------------- - 2025-03-27 07:20:18,300 - log
End of training - 2025-03-27 07:20:18,300 - log
ms/batch 29.46 - 2025-03-27 07:20:18,300 - log
==================================================================================================== - 2025-03-27 07:21:47,967 - log
        - random_seed : 2025 - 2025-03-27 07:21:47,967 - log
        - lr : 0.0002 - 2025-03-27 07:21:47,967 - log
        - weight_decay : 0.01 - 2025-03-27 07:21:47,967 - log
        - correct_bias : False - 2025-03-27 07:21:47,967 - log
        - adam_epislon : 1e-06 - 2025-03-27 07:21:47,967 - log
        - no_decay_bias : False - 2025-03-27 07:21:47,967 - log
        - adam_beta1 : 0.9 - 2025-03-27 07:21:47,967 - log
        - adam_beta2 : 0.999 - 2025-03-27 07:21:47,967 - log
        - scheduler : linear - 2025-03-27 07:21:47,967 - log
        - max_step : None - 2025-03-27 07:21:47,967 - log
        - max_epoch : 5 - 2025-03-27 07:21:47,967 - log
        - warmup_step : 500 - 2025-03-27 07:21:47,967 - log
        - i_steps : 0 - 2025-03-27 07:21:47,967 - log
        - i_lrs : 0.00025 - 2025-03-27 07:21:47,967 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-27 07:21:47,967 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-27 07:21:47,967 - log
        - train_batch_size : 2 - 2025-03-27 07:21:47,967 - log
        - valid_batch_size : 1 - 2025-03-27 07:21:47,967 - log
        - grad_acc : 2 - 2025-03-27 07:21:47,967 - log
        - clip : 0.0 - 2025-03-27 07:21:47,967 - log
        - seq_len : 64 - 2025-03-27 07:21:47,967 - log
        - model_card : gpt2.sm - 2025-03-27 07:21:47,967 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-27 07:21:47,967 - log
        - fp16 : False - 2025-03-27 07:21:47,967 - log
        - log_interval : 100 - 2025-03-27 07:21:47,967 - log
        - eval_interval : 2000 - 2025-03-27 07:21:47,967 - log
        - save_interval : 1000 - 2025-03-27 07:21:47,967 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-27 07:21:47,967 - log
        - lora_dim : 4 - 2025-03-27 07:21:47,967 - log
        - lora_alpha : 32 - 2025-03-27 07:21:47,967 - log
        - obj : clm - 2025-03-27 07:21:47,967 - log
        - lora_dropout : 0.1 - 2025-03-27 07:21:47,967 - log
        - label_smooth : 0.1 - 2025-03-27 07:21:47,967 - log
        - roll_interval : -1 - 2025-03-27 07:21:47,967 - log
        - roll_lr : 1e-05 - 2025-03-27 07:21:47,967 - log
        - roll_step : 100 - 2025-03-27 07:21:47,967 - log
        - eval_epoch : 1 - 2025-03-27 07:21:47,967 - log
        - device : cuda - 2025-03-27 07:21:47,967 - log
==================================================================================================== - 2025-03-27 07:21:47,967 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-27 07:21:47,967 - log
loading model pretrained weight. - 2025-03-27 07:21:48,076 - log
set max_step: 1050 - 2025-03-27 07:21:48,862 - log
start to train the model................ 1 - 2025-03-27 07:21:48,862 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.19 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-27 07:21:52,381 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.14 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-27 07:21:55,596 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pkl - 2025-03-27 07:21:55,887 - log
start to train the model................ 2 - 2025-03-27 07:21:57,651 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.53 | loss  3.23 | avg loss  3.57 | ppl 35.69 - 2025-03-27 07:22:00,605 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 33.06 | loss  4.11 | avg loss  3.40 | ppl 30.03 - 2025-03-27 07:22:03,911 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pkl - 2025-03-27 07:22:04,534 - log
start to train the model................ 3 - 2025-03-27 07:22:06,443 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.46 | loss  3.38 | avg loss  3.23 | ppl 25.37 - 2025-03-27 07:22:09,090 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.47 | loss  3.04 | avg loss  3.20 | ppl 24.49 - 2025-03-27 07:22:12,337 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pkl - 2025-03-27 07:22:13,282 - log
start to train the model................ 4 - 2025-03-27 07:22:15,034 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 23.25 | loss  3.46 | avg loss  3.15 | ppl 23.22 - 2025-03-27 07:22:17,360 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.56 | loss  3.04 | avg loss  3.01 | ppl 20.38 - 2025-03-27 07:22:20,616 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pkl - 2025-03-27 07:22:21,884 - log
start to train the model................ 5 - 2025-03-27 07:22:23,608 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 20.05 | loss  2.47 | avg loss  3.04 | ppl 20.95 - 2025-03-27 07:22:25,613 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.57 | loss  3.83 | avg loss  3.01 | ppl 20.36 - 2025-03-27 07:22:28,871 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-03-27 07:22:28,871 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pkl - 2025-03-27 07:22:30,474 - log
---------------------------------------------------------------------------------------------------- - 2025-03-27 07:22:32,204 - log
End of training - 2025-03-27 07:22:32,205 - log
ms/batch 29.73 - 2025-03-27 07:22:32,205 - log
==================================================================================================== - 2025-03-27 07:23:35,376 - log
        - random_seed : 2025 - 2025-03-27 07:23:35,376 - log
        - lr : 0.0002 - 2025-03-27 07:23:35,376 - log
        - weight_decay : 0.01 - 2025-03-27 07:23:35,376 - log
        - correct_bias : False - 2025-03-27 07:23:35,376 - log
        - adam_epislon : 1e-06 - 2025-03-27 07:23:35,376 - log
        - no_decay_bias : False - 2025-03-27 07:23:35,376 - log
        - adam_beta1 : 0.9 - 2025-03-27 07:23:35,376 - log
        - adam_beta2 : 0.999 - 2025-03-27 07:23:35,376 - log
        - scheduler : linear - 2025-03-27 07:23:35,376 - log
        - max_step : None - 2025-03-27 07:23:35,376 - log
        - max_epoch : 5 - 2025-03-27 07:23:35,376 - log
        - warmup_step : 500 - 2025-03-27 07:23:35,376 - log
        - i_steps : 0 - 2025-03-27 07:23:35,376 - log
        - i_lrs : 0.00025 - 2025-03-27 07:23:35,376 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-27 07:23:35,376 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-27 07:23:35,376 - log
        - train_batch_size : 2 - 2025-03-27 07:23:35,376 - log
        - valid_batch_size : 1 - 2025-03-27 07:23:35,376 - log
        - grad_acc : 2 - 2025-03-27 07:23:35,376 - log
        - clip : 0.0 - 2025-03-27 07:23:35,376 - log
        - seq_len : 64 - 2025-03-27 07:23:35,376 - log
        - model_card : gpt2.sm - 2025-03-27 07:23:35,376 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-27 07:23:35,376 - log
        - fp16 : False - 2025-03-27 07:23:35,376 - log
        - log_interval : 100 - 2025-03-27 07:23:35,376 - log
        - eval_interval : 2000 - 2025-03-27 07:23:35,376 - log
        - save_interval : 1000 - 2025-03-27 07:23:35,376 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-27 07:23:35,376 - log
        - lora_dim : 4 - 2025-03-27 07:23:35,376 - log
        - lora_alpha : 32 - 2025-03-27 07:23:35,376 - log
        - obj : clm - 2025-03-27 07:23:35,377 - log
        - lora_dropout : 0.1 - 2025-03-27 07:23:35,377 - log
        - label_smooth : 0.1 - 2025-03-27 07:23:35,377 - log
        - roll_interval : -1 - 2025-03-27 07:23:35,377 - log
        - roll_lr : 1e-05 - 2025-03-27 07:23:35,377 - log
        - roll_step : 100 - 2025-03-27 07:23:35,377 - log
        - eval_epoch : 1 - 2025-03-27 07:23:35,377 - log
        - device : cuda - 2025-03-27 07:23:35,377 - log
==================================================================================================== - 2025-03-27 07:23:35,377 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-27 07:23:35,377 - log
loading model pretrained weight. - 2025-03-27 07:23:35,486 - log
set max_step: 1050 - 2025-03-27 07:23:36,268 - log
start to train the model................ 1 - 2025-03-27 07:23:36,268 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.25 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-27 07:23:39,793 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.33 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-27 07:23:43,026 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pkl - 2025-03-27 07:23:43,319 - log
start to train the model................ 2 - 2025-03-27 07:23:45,097 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.34 | loss  3.23 | avg loss  3.57 | ppl 35.69 - 2025-03-27 07:23:48,031 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.46 | loss  4.11 | avg loss  3.40 | ppl 30.03 - 2025-03-27 07:23:51,277 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pkl - 2025-03-27 07:23:51,895 - log
start to train the model................ 3 - 2025-03-27 07:23:53,647 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.45 | loss  3.38 | avg loss  3.23 | ppl 25.37 - 2025-03-27 07:23:56,292 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.47 | loss  3.04 | avg loss  3.20 | ppl 24.49 - 2025-03-27 07:23:59,540 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pkl - 2025-03-27 07:24:00,485 - log
start to train the model................ 4 - 2025-03-27 07:24:02,282 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 23.21 | loss  3.46 | avg loss  3.15 | ppl 23.22 - 2025-03-27 07:24:04,604 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.55 | loss  3.04 | avg loss  3.01 | ppl 20.38 - 2025-03-27 07:24:07,859 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pkl - 2025-03-27 07:24:09,139 - log
start to train the model................ 5 - 2025-03-27 07:24:10,894 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.79 | loss  2.47 | avg loss  3.04 | ppl 20.95 - 2025-03-27 07:24:12,873 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.62 | loss  3.83 | avg loss  3.01 | ppl 20.36 - 2025-03-27 07:24:16,136 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-03-27 07:24:16,136 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pkl - 2025-03-27 07:24:17,748 - log
---------------------------------------------------------------------------------------------------- - 2025-03-27 07:24:19,477 - log
End of training - 2025-03-27 07:24:19,477 - log
ms/batch 29.65 - 2025-03-27 07:24:19,477 - log
==================================================================================================== - 2025-03-27 07:25:04,921 - log
        - random_seed : 2025 - 2025-03-27 07:25:04,921 - log
        - lr : 0.0002 - 2025-03-27 07:25:04,921 - log
        - weight_decay : 0.01 - 2025-03-27 07:25:04,922 - log
        - correct_bias : False - 2025-03-27 07:25:04,922 - log
        - adam_epislon : 1e-06 - 2025-03-27 07:25:04,922 - log
        - no_decay_bias : False - 2025-03-27 07:25:04,922 - log
        - adam_beta1 : 0.9 - 2025-03-27 07:25:04,922 - log
        - adam_beta2 : 0.999 - 2025-03-27 07:25:04,922 - log
        - scheduler : linear - 2025-03-27 07:25:04,922 - log
        - max_step : None - 2025-03-27 07:25:04,922 - log
        - max_epoch : 5 - 2025-03-27 07:25:04,922 - log
        - warmup_step : 500 - 2025-03-27 07:25:04,922 - log
        - i_steps : 0 - 2025-03-27 07:25:04,922 - log
        - i_lrs : 0.00025 - 2025-03-27 07:25:04,922 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-27 07:25:04,922 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-27 07:25:04,922 - log
        - train_batch_size : 2 - 2025-03-27 07:25:04,922 - log
        - valid_batch_size : 1 - 2025-03-27 07:25:04,922 - log
        - grad_acc : 2 - 2025-03-27 07:25:04,922 - log
        - clip : 0.0 - 2025-03-27 07:25:04,922 - log
        - seq_len : 64 - 2025-03-27 07:25:04,922 - log
        - model_card : gpt2.sm - 2025-03-27 07:25:04,922 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-27 07:25:04,922 - log
        - fp16 : False - 2025-03-27 07:25:04,922 - log
        - log_interval : 100 - 2025-03-27 07:25:04,922 - log
        - eval_interval : 2000 - 2025-03-27 07:25:04,922 - log
        - save_interval : 1000 - 2025-03-27 07:25:04,922 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-27 07:25:04,922 - log
        - lora_dim : 4 - 2025-03-27 07:25:04,922 - log
        - lora_alpha : 32 - 2025-03-27 07:25:04,922 - log
        - obj : clm - 2025-03-27 07:25:04,922 - log
        - lora_dropout : 0.1 - 2025-03-27 07:25:04,922 - log
        - label_smooth : 0.1 - 2025-03-27 07:25:04,922 - log
        - roll_interval : -1 - 2025-03-27 07:25:04,922 - log
        - roll_lr : 1e-05 - 2025-03-27 07:25:04,922 - log
        - roll_step : 100 - 2025-03-27 07:25:04,922 - log
        - eval_epoch : 1 - 2025-03-27 07:25:04,922 - log
        - device : cuda - 2025-03-27 07:25:04,922 - log
==================================================================================================== - 2025-03-27 07:25:04,922 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-27 07:25:04,922 - log
loading model pretrained weight. - 2025-03-27 07:25:05,029 - log
set max_step: 1050 - 2025-03-27 07:25:05,807 - log
start to train the model................ 1 - 2025-03-27 07:25:05,807 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.50 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-27 07:25:09,357 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.51 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-27 07:25:12,608 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pkl - 2025-03-27 07:25:12,903 - log
start to train the model................ 2 - 2025-03-27 07:25:14,695 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.71 | loss  3.23 | avg loss  3.57 | ppl 35.69 - 2025-03-27 07:25:17,667 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.63 | loss  4.11 | avg loss  3.40 | ppl 30.03 - 2025-03-27 07:25:20,930 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pkl - 2025-03-27 07:25:21,550 - log
start to train the model................ 3 - 2025-03-27 07:25:23,299 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.48 | loss  3.38 | avg loss  3.23 | ppl 25.37 - 2025-03-27 07:25:25,947 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.51 | loss  3.04 | avg loss  3.20 | ppl 24.49 - 2025-03-27 07:25:29,199 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pkl - 2025-03-27 07:25:30,146 - log
start to train the model................ 4 - 2025-03-27 07:25:31,943 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 23.25 | loss  3.46 | avg loss  3.15 | ppl 23.22 - 2025-03-27 07:25:34,268 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.51 | loss  3.04 | avg loss  3.01 | ppl 20.38 - 2025-03-27 07:25:37,520 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pkl - 2025-03-27 07:25:38,792 - log
start to train the model................ 5 - 2025-03-27 07:25:40,546 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 20.03 | loss  2.47 | avg loss  3.04 | ppl 20.95 - 2025-03-27 07:25:42,550 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.49 | loss  3.83 | avg loss  3.01 | ppl 20.36 - 2025-03-27 07:25:45,799 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-03-27 07:25:45,799 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pkl - 2025-03-27 07:25:47,398 - log
---------------------------------------------------------------------------------------------------- - 2025-03-27 07:25:49,125 - log
End of training - 2025-03-27 07:25:49,126 - log
ms/batch 29.76 - 2025-03-27 07:25:49,126 - log
==================================================================================================== - 2025-03-27 07:29:09,038 - log
        - random_seed : 2025 - 2025-03-27 07:29:09,038 - log
        - lr : 0.0002 - 2025-03-27 07:29:09,038 - log
        - weight_decay : 0.01 - 2025-03-27 07:29:09,038 - log
        - correct_bias : False - 2025-03-27 07:29:09,038 - log
        - adam_epislon : 1e-06 - 2025-03-27 07:29:09,038 - log
        - no_decay_bias : False - 2025-03-27 07:29:09,038 - log
        - adam_beta1 : 0.9 - 2025-03-27 07:29:09,038 - log
        - adam_beta2 : 0.999 - 2025-03-27 07:29:09,038 - log
        - scheduler : linear - 2025-03-27 07:29:09,038 - log
        - max_step : None - 2025-03-27 07:29:09,038 - log
        - max_epoch : 5 - 2025-03-27 07:29:09,038 - log
        - warmup_step : 500 - 2025-03-27 07:29:09,038 - log
        - i_steps : 0 - 2025-03-27 07:29:09,038 - log
        - i_lrs : 0.00025 - 2025-03-27 07:29:09,038 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-27 07:29:09,038 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-27 07:29:09,038 - log
        - train_batch_size : 2 - 2025-03-27 07:29:09,038 - log
        - valid_batch_size : 1 - 2025-03-27 07:29:09,038 - log
        - grad_acc : 2 - 2025-03-27 07:29:09,038 - log
        - clip : 0.0 - 2025-03-27 07:29:09,038 - log
        - seq_len : 64 - 2025-03-27 07:29:09,038 - log
        - model_card : gpt2.sm - 2025-03-27 07:29:09,038 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-27 07:29:09,038 - log
        - fp16 : False - 2025-03-27 07:29:09,038 - log
        - log_interval : 100 - 2025-03-27 07:29:09,038 - log
        - eval_interval : 2000 - 2025-03-27 07:29:09,038 - log
        - save_interval : 1000 - 2025-03-27 07:29:09,038 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-27 07:29:09,038 - log
        - lora_dim : 4 - 2025-03-27 07:29:09,038 - log
        - lora_alpha : 32 - 2025-03-27 07:29:09,038 - log
        - obj : clm - 2025-03-27 07:29:09,038 - log
        - lora_dropout : 0.1 - 2025-03-27 07:29:09,038 - log
        - label_smooth : 0.1 - 2025-03-27 07:29:09,038 - log
        - roll_interval : -1 - 2025-03-27 07:29:09,038 - log
        - roll_lr : 1e-05 - 2025-03-27 07:29:09,038 - log
        - roll_step : 100 - 2025-03-27 07:29:09,038 - log
        - eval_epoch : 1 - 2025-03-27 07:29:09,038 - log
        - device : cuda - 2025-03-27 07:29:09,038 - log
==================================================================================================== - 2025-03-27 07:29:09,038 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-27 07:29:09,038 - log
loading model pretrained weight. - 2025-03-27 07:29:09,144 - log
set max_step: 1050 - 2025-03-27 07:29:09,907 - log
start to train the model................ 1 - 2025-03-27 07:29:09,907 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.34 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-27 07:29:13,341 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 31.91 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-27 07:29:16,532 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pkl - 2025-03-27 07:29:16,822 - log
start to train the model................ 2 - 2025-03-27 07:29:18,575 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.22 | loss  3.23 | avg loss  3.57 | ppl 35.69 - 2025-03-27 07:29:21,497 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.49 | loss  4.11 | avg loss  3.40 | ppl 30.03 - 2025-03-27 07:29:24,747 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pkl - 2025-03-27 07:29:25,362 - log
start to train the model................ 3 - 2025-03-27 07:29:27,185 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.65 | loss  3.38 | avg loss  3.23 | ppl 25.37 - 2025-03-27 07:29:29,850 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.31 | loss  3.04 | avg loss  3.20 | ppl 24.49 - 2025-03-27 07:29:33,082 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pkl - 2025-03-27 07:29:34,016 - log
start to train the model................ 4 - 2025-03-27 07:29:35,753 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 24.30 | loss  3.46 | avg loss  3.15 | ppl 23.22 - 2025-03-27 07:29:38,183 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.69 | loss  3.04 | avg loss  3.01 | ppl 20.38 - 2025-03-27 07:29:41,452 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pkl - 2025-03-27 07:29:42,720 - log
start to train the model................ 5 - 2025-03-27 07:29:44,487 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 20.02 | loss  2.47 | avg loss  3.04 | ppl 20.95 - 2025-03-27 07:29:46,489 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.69 | loss  3.83 | avg loss  3.01 | ppl 20.36 - 2025-03-27 07:29:49,758 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-03-27 07:29:49,758 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pkl - 2025-03-27 07:29:51,353 - log
---------------------------------------------------------------------------------------------------- - 2025-03-27 07:29:53,128 - log
End of training - 2025-03-27 07:29:53,128 - log
ms/batch 29.66 - 2025-03-27 07:29:53,128 - log
==================================================================================================== - 2025-03-27 07:30:51,263 - log
        - random_seed : 2025 - 2025-03-27 07:30:51,263 - log
        - lr : 0.0002 - 2025-03-27 07:30:51,263 - log
        - weight_decay : 0.01 - 2025-03-27 07:30:51,263 - log
        - correct_bias : False - 2025-03-27 07:30:51,263 - log
        - adam_epislon : 1e-06 - 2025-03-27 07:30:51,263 - log
        - no_decay_bias : False - 2025-03-27 07:30:51,263 - log
        - adam_beta1 : 0.9 - 2025-03-27 07:30:51,263 - log
        - adam_beta2 : 0.999 - 2025-03-27 07:30:51,263 - log
        - scheduler : linear - 2025-03-27 07:30:51,263 - log
        - max_step : None - 2025-03-27 07:30:51,263 - log
        - max_epoch : 5 - 2025-03-27 07:30:51,263 - log
        - warmup_step : 500 - 2025-03-27 07:30:51,263 - log
        - i_steps : 0 - 2025-03-27 07:30:51,263 - log
        - i_lrs : 0.00025 - 2025-03-27 07:30:51,263 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-27 07:30:51,263 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-27 07:30:51,264 - log
        - train_batch_size : 2 - 2025-03-27 07:30:51,264 - log
        - valid_batch_size : 1 - 2025-03-27 07:30:51,264 - log
        - grad_acc : 2 - 2025-03-27 07:30:51,264 - log
        - clip : 0.0 - 2025-03-27 07:30:51,264 - log
        - seq_len : 64 - 2025-03-27 07:30:51,264 - log
        - model_card : gpt2.sm - 2025-03-27 07:30:51,264 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-27 07:30:51,264 - log
        - fp16 : False - 2025-03-27 07:30:51,264 - log
        - log_interval : 100 - 2025-03-27 07:30:51,264 - log
        - eval_interval : 2000 - 2025-03-27 07:30:51,264 - log
        - save_interval : 1000 - 2025-03-27 07:30:51,264 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-27 07:30:51,264 - log
        - lora_dim : 4 - 2025-03-27 07:30:51,264 - log
        - lora_alpha : 32 - 2025-03-27 07:30:51,264 - log
        - obj : clm - 2025-03-27 07:30:51,264 - log
        - lora_dropout : 0.1 - 2025-03-27 07:30:51,264 - log
        - label_smooth : 0.1 - 2025-03-27 07:30:51,264 - log
        - roll_interval : -1 - 2025-03-27 07:30:51,264 - log
        - roll_lr : 1e-05 - 2025-03-27 07:30:51,264 - log
        - roll_step : 100 - 2025-03-27 07:30:51,264 - log
        - eval_epoch : 1 - 2025-03-27 07:30:51,264 - log
        - device : cuda - 2025-03-27 07:30:51,264 - log
==================================================================================================== - 2025-03-27 07:30:51,264 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-27 07:30:51,264 - log
loading model pretrained weight. - 2025-03-27 07:30:51,369 - log
set max_step: 1050 - 2025-03-27 07:30:52,132 - log
start to train the model................ 1 - 2025-03-27 07:30:52,132 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.36 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-27 07:30:55,569 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 31.93 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-27 07:30:58,762 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pkl - 2025-03-27 07:30:59,054 - log
start to train the model................ 2 - 2025-03-27 07:31:00,779 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 28.92 | loss  3.23 | avg loss  3.57 | ppl 35.69 - 2025-03-27 07:31:03,671 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 31.95 | loss  4.11 | avg loss  3.40 | ppl 30.03 - 2025-03-27 07:31:06,866 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pkl - 2025-03-27 07:31:07,473 - log
start to train the model................ 3 - 2025-03-27 07:31:09,168 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 25.78 | loss  3.38 | avg loss  3.23 | ppl 25.37 - 2025-03-27 07:31:11,746 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.08 | loss  3.04 | avg loss  3.20 | ppl 24.49 - 2025-03-27 07:31:14,955 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pkl - 2025-03-27 07:31:15,890 - log
start to train the model................ 4 - 2025-03-27 07:31:17,702 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 23.06 | loss  3.46 | avg loss  3.15 | ppl 23.22 - 2025-03-27 07:31:20,008 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.19 | loss  3.04 | avg loss  3.01 | ppl 20.38 - 2025-03-27 07:31:23,227 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pkl - 2025-03-27 07:31:24,490 - log
start to train the model................ 5 - 2025-03-27 07:31:26,236 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.65 | loss  2.47 | avg loss  3.04 | ppl 20.95 - 2025-03-27 07:31:28,201 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.26 | loss  3.83 | avg loss  3.01 | ppl 20.36 - 2025-03-27 07:31:31,428 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-03-27 07:31:31,428 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pkl - 2025-03-27 07:31:33,019 - log
---------------------------------------------------------------------------------------------------- - 2025-03-27 07:31:34,748 - log
End of training - 2025-03-27 07:31:34,748 - log
ms/batch 29.22 - 2025-03-27 07:31:34,748 - log
==================================================================================================== - 2025-03-27 09:23:03,942 - log
        - random_seed : 2025 - 2025-03-27 09:23:03,943 - log
        - lr : 0.0002 - 2025-03-27 09:23:03,943 - log
        - weight_decay : 0.01 - 2025-03-27 09:23:03,943 - log
        - correct_bias : False - 2025-03-27 09:23:03,943 - log
        - adam_epislon : 1e-06 - 2025-03-27 09:23:03,943 - log
        - no_decay_bias : False - 2025-03-27 09:23:03,943 - log
        - adam_beta1 : 0.9 - 2025-03-27 09:23:03,943 - log
        - adam_beta2 : 0.999 - 2025-03-27 09:23:03,943 - log
        - scheduler : linear - 2025-03-27 09:23:03,943 - log
        - max_step : None - 2025-03-27 09:23:03,943 - log
        - max_epoch : 5 - 2025-03-27 09:23:03,943 - log
        - warmup_step : 500 - 2025-03-27 09:23:03,943 - log
        - i_steps : 0 - 2025-03-27 09:23:03,943 - log
        - i_lrs : 0.00025 - 2025-03-27 09:23:03,943 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-27 09:23:03,943 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-27 09:23:03,943 - log
        - train_batch_size : 2 - 2025-03-27 09:23:03,943 - log
        - valid_batch_size : 1 - 2025-03-27 09:23:03,943 - log
        - grad_acc : 2 - 2025-03-27 09:23:03,943 - log
        - clip : 0.0 - 2025-03-27 09:23:03,943 - log
        - seq_len : 64 - 2025-03-27 09:23:03,943 - log
        - model_card : gpt2.sm - 2025-03-27 09:23:03,943 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-27 09:23:03,943 - log
        - fp16 : False - 2025-03-27 09:23:03,943 - log
        - log_interval : 100 - 2025-03-27 09:23:03,943 - log
        - eval_interval : 2000 - 2025-03-27 09:23:03,943 - log
        - save_interval : 1000 - 2025-03-27 09:23:03,943 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-27 09:23:03,943 - log
        - lora_dim : 4 - 2025-03-27 09:23:03,943 - log
        - lora_alpha : 32 - 2025-03-27 09:23:03,943 - log
        - obj : clm - 2025-03-27 09:23:03,943 - log
        - lora_dropout : 0.1 - 2025-03-27 09:23:03,943 - log
        - label_smooth : 0.1 - 2025-03-27 09:23:03,943 - log
        - roll_interval : -1 - 2025-03-27 09:23:03,943 - log
        - roll_lr : 1e-05 - 2025-03-27 09:23:03,943 - log
        - roll_step : 100 - 2025-03-27 09:23:03,943 - log
        - eval_epoch : 1 - 2025-03-27 09:23:03,943 - log
        - device : cuda - 2025-03-27 09:23:03,943 - log
==================================================================================================== - 2025-03-27 09:23:03,943 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-27 09:23:03,943 - log
loading model pretrained weight. - 2025-03-27 09:23:04,044 - log
set max_step: 1050 - 2025-03-27 09:23:05,229 - log
start to train the model................ 1 - 2025-03-27 09:23:05,230 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.57 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-27 09:23:08,687 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 31.53 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-27 09:23:11,840 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pkl - 2025-03-27 09:23:12,126 - log
start to train the model................ 2 - 2025-03-27 09:23:13,851 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 28.66 | loss  3.23 | avg loss  3.57 | ppl 35.69 - 2025-03-27 09:23:16,717 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 31.65 | loss  4.11 | avg loss  3.40 | ppl 30.03 - 2025-03-27 09:23:19,882 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pkl - 2025-03-27 09:23:20,488 - log
start to train the model................ 3 - 2025-03-27 09:23:22,208 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 25.65 | loss  3.38 | avg loss  3.23 | ppl 25.37 - 2025-03-27 09:23:24,774 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 31.76 | loss  3.04 | avg loss  3.20 | ppl 24.49 - 2025-03-27 09:23:27,950 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pkl - 2025-03-27 09:23:28,875 - log
start to train the model................ 4 - 2025-03-27 09:23:30,586 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.53 | loss  3.46 | avg loss  3.15 | ppl 23.22 - 2025-03-27 09:23:32,839 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 31.86 | loss  3.04 | avg loss  3.01 | ppl 20.38 - 2025-03-27 09:23:36,025 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pkl - 2025-03-27 09:23:37,273 - log
start to train the model................ 5 - 2025-03-27 09:23:39,009 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.39 | loss  2.47 | avg loss  3.04 | ppl 20.95 - 2025-03-27 09:23:40,948 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 31.80 | loss  3.83 | avg loss  3.01 | ppl 20.36 - 2025-03-27 09:23:44,129 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-03-27 09:23:44,129 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pkl - 2025-03-27 09:23:45,692 - log
---------------------------------------------------------------------------------------------------- - 2025-03-27 09:23:47,386 - log
End of training - 2025-03-27 09:23:47,387 - log
ms/batch 28.94 - 2025-03-27 09:23:47,387 - log
