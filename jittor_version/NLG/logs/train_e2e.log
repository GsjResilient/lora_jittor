==================================================================================================== - 2025-03-25 13:04:52,350 - log
        - platform : local - 2025-03-25 13:04:52,350 - log
        - local_rank : 0 - 2025-03-25 13:04:52,350 - log
        - rank : 0 - 2025-03-25 13:04:52,350 - log
        - device : cuda - 2025-03-25 13:04:52,350 - log
        - world_size : 0 - 2025-03-25 13:04:52,350 - log
        - random_seed : 2025 - 2025-03-25 13:04:52,351 - log
        - lr : 0.0002 - 2025-03-25 13:04:52,351 - log
        - weight_decay : 0.01 - 2025-03-25 13:04:52,351 - log
        - correct_bias : True - 2025-03-25 13:04:52,351 - log
        - adam_epislon : 1e-06 - 2025-03-25 13:04:52,351 - log
        - no_decay_bias : False - 2025-03-25 13:04:52,351 - log
        - adam_beta1 : 0.9 - 2025-03-25 13:04:52,351 - log
        - adam_beta2 : 0.999 - 2025-03-25 13:04:52,351 - log
        - scheduler : linear - 2025-03-25 13:04:52,351 - log
        - max_step : None - 2025-03-25 13:04:52,351 - log
        - max_epoch : 5 - 2025-03-25 13:04:52,351 - log
        - warmup_step : 500 - 2025-03-25 13:04:52,351 - log
        - i_steps : 0 - 2025-03-25 13:04:52,351 - log
        - i_lrs : 0.00025 - 2025-03-25 13:04:52,351 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 13:04:52,351 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 13:04:52,351 - log
        - train_batch_size : 2 - 2025-03-25 13:04:52,351 - log
        - valid_batch_size : 1 - 2025-03-25 13:04:52,351 - log
        - grad_acc : 2 - 2025-03-25 13:04:52,351 - log
        - clip : 0.0 - 2025-03-25 13:04:52,351 - log
        - seq_len : 64 - 2025-03-25 13:04:52,351 - log
        - model_card : gpt2.sm - 2025-03-25 13:04:52,351 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 13:04:52,351 - log
        - fp16 : False - 2025-03-25 13:04:52,351 - log
        - log_interval : 100 - 2025-03-25 13:04:52,351 - log
        - eval_interval : 2000 - 2025-03-25 13:04:52,351 - log
        - save_interval : 1000 - 2025-03-25 13:04:52,351 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 13:04:52,351 - log
        - lora_dim : 4 - 2025-03-25 13:04:52,351 - log
        - lora_alpha : 32 - 2025-03-25 13:04:52,351 - log
        - obj : clm - 2025-03-25 13:04:52,351 - log
        - lora_dropout : 0.1 - 2025-03-25 13:04:52,351 - log
        - label_smooth : 0.1 - 2025-03-25 13:04:52,351 - log
        - roll_interval : -1 - 2025-03-25 13:04:52,351 - log
        - roll_lr : 1e-05 - 2025-03-25 13:04:52,351 - log
        - roll_step : 100 - 2025-03-25 13:04:52,351 - log
        - eval_epoch : 1 - 2025-03-25 13:04:52,351 - log
==================================================================================================== - 2025-03-25 13:04:52,351 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 13:04:52,351 - log
loading model pretrained weight. - 2025-03-25 13:04:52,458 - log
set max_step: 1050 - 2025-03-25 13:04:53,260 - log
start to train the model................ 1 - 2025-03-25 13:04:53,260 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.23 | loss 15.19 | avg loss 16.53 | ppl 15075461.89 - 2025-03-25 13:04:56,783 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 31.99 | loss 16.86 | avg loss 16.56 | ppl 15480665.93 - 2025-03-25 13:04:59,982 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 13:05:00,303 - log
start to train the model................ 2 - 2025-03-25 13:05:01,680 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 28.70 | loss 17.93 | avg loss 16.76 | ppl 19042227.74 - 2025-03-25 13:05:04,551 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 31.99 | loss 15.73 | avg loss 16.61 | ppl 16292781.57 - 2025-03-25 13:05:07,750 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 13:05:08,390 - log
start to train the model................ 3 - 2025-03-25 13:05:09,784 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 25.70 | loss 15.37 | avg loss 16.79 | ppl 19555723.34 - 2025-03-25 13:05:12,354 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.24 | loss 16.84 | avg loss 16.25 | ppl 11391461.85 - 2025-03-25 13:05:15,579 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 13:05:16,549 - log
start to train the model................ 4 - 2025-03-25 13:05:18,026 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.62 | loss 16.58 | avg loss 16.73 | ppl 18477074.23 - 2025-03-25 13:05:20,288 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.37 | loss 16.36 | avg loss 15.81 | ppl 7368803.78 - 2025-03-25 13:05:23,525 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 13:05:24,822 - log
start to train the model................ 5 - 2025-03-25 13:05:26,322 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.44 | loss 13.72 | avg loss 15.86 | ppl 7698651.67 - 2025-03-25 13:05:28,266 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.53 | loss 14.54 | avg loss 15.49 | ppl 5358645.68 - 2025-03-25 13:05:31,520 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 13:05:31,520 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 13:05:33,149 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 13:05:34,610 - log
End of training - 2025-03-25 13:05:34,610 - log
==================================================================================================== - 2025-03-25 15:20:38,296 - log
        - lr : 0.0002 - 2025-03-25 15:20:38,296 - log
        - weight_decay : 0.01 - 2025-03-25 15:20:38,296 - log
        - correct_bias : False - 2025-03-25 15:20:38,296 - log
        - adam_epislon : 1e-06 - 2025-03-25 15:20:38,296 - log
        - no_decay_bias : False - 2025-03-25 15:20:38,296 - log
        - adam_beta1 : 0.9 - 2025-03-25 15:20:38,296 - log
        - adam_beta2 : 0.999 - 2025-03-25 15:20:38,296 - log
        - scheduler : linear - 2025-03-25 15:20:38,296 - log
        - max_step : None - 2025-03-25 15:20:38,296 - log
        - max_epoch : 5 - 2025-03-25 15:20:38,296 - log
        - warmup_step : 500 - 2025-03-25 15:20:38,296 - log
        - i_steps : 0 - 2025-03-25 15:20:38,296 - log
        - i_lrs : 0.00025 - 2025-03-25 15:20:38,296 - log
        - random_seed : 2025 - 2025-03-25 15:20:38,296 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 15:20:38,296 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 15:20:38,296 - log
        - train_batch_size : 2 - 2025-03-25 15:20:38,296 - log
        - valid_batch_size : 1 - 2025-03-25 15:20:38,296 - log
        - grad_acc : 2 - 2025-03-25 15:20:38,296 - log
        - clip : 0.0 - 2025-03-25 15:20:38,296 - log
        - seq_len : 64 - 2025-03-25 15:20:38,296 - log
        - model_card : gpt2.sm - 2025-03-25 15:20:38,296 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 15:20:38,296 - log
        - fp16 : False - 2025-03-25 15:20:38,296 - log
        - log_interval : 100 - 2025-03-25 15:20:38,296 - log
        - eval_interval : 2000 - 2025-03-25 15:20:38,296 - log
        - save_interval : 1000 - 2025-03-25 15:20:38,296 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 15:20:38,296 - log
        - lora_dim : 4 - 2025-03-25 15:20:38,296 - log
        - lora_alpha : 32 - 2025-03-25 15:20:38,296 - log
        - obj : clm - 2025-03-25 15:20:38,296 - log
        - lora_dropout : 0.1 - 2025-03-25 15:20:38,296 - log
        - label_smooth : 0.1 - 2025-03-25 15:20:38,296 - log
        - roll_interval : -1 - 2025-03-25 15:20:38,296 - log
        - roll_lr : 1e-05 - 2025-03-25 15:20:38,296 - log
        - roll_step : 100 - 2025-03-25 15:20:38,296 - log
        - eval_epoch : 1 - 2025-03-25 15:20:38,296 - log
        - device : cuda - 2025-03-25 15:20:38,296 - log
==================================================================================================== - 2025-03-25 15:20:38,296 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 15:20:38,296 - log
loading model pretrained weight. - 2025-03-25 15:20:38,403 - log
set max_step: 1050 - 2025-03-25 15:20:39,522 - log
start to train the model................ 1 - 2025-03-25 15:20:39,523 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.41 | loss 15.19 | avg loss 16.53 | ppl 15075361.68 - 2025-03-25 15:20:43,063 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 31.99 | loss 16.86 | avg loss 16.56 | ppl 15480574.54 - 2025-03-25 15:20:46,262 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 15:20:46,582 - log
start to train the model................ 2 - 2025-03-25 15:20:48,020 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 28.87 | loss 17.93 | avg loss 16.76 | ppl 19042297.15 - 2025-03-25 15:20:50,907 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.12 | loss 15.73 | avg loss 16.61 | ppl 16292631.48 - 2025-03-25 15:20:54,118 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 15:20:54,761 - log
start to train the model................ 3 - 2025-03-25 15:20:56,153 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 25.68 | loss 15.37 | avg loss 16.79 | ppl 19553536.77 - 2025-03-25 15:20:58,721 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.18 | loss 16.82 | avg loss 16.25 | ppl 11400158.22 - 2025-03-25 15:21:01,939 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 15:21:02,904 - log
start to train the model................ 4 - 2025-03-25 15:21:04,346 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.56 | loss 16.50 | avg loss 16.74 | ppl 18554550.60 - 2025-03-25 15:21:06,603 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.26 | loss 16.37 | avg loss 15.80 | ppl 7302279.58 - 2025-03-25 15:21:09,829 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 15:21:11,125 - log
start to train the model................ 5 - 2025-03-25 15:21:12,467 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.38 | loss 13.71 | avg loss 15.86 | ppl 7744788.87 - 2025-03-25 15:21:14,405 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.34 | loss 14.56 | avg loss 15.50 | ppl 5374613.73 - 2025-03-25 15:21:17,639 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 15:21:17,639 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 15:21:19,264 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 15:21:20,652 - log
End of training - 2025-03-25 15:21:20,652 - log
==================================================================================================== - 2025-03-25 15:22:38,207 - log
        - lr : 0.0002 - 2025-03-25 15:22:38,207 - log
        - weight_decay : 0.01 - 2025-03-25 15:22:38,207 - log
        - correct_bias : False - 2025-03-25 15:22:38,208 - log
        - adam_epislon : 1e-06 - 2025-03-25 15:22:38,208 - log
        - no_decay_bias : False - 2025-03-25 15:22:38,208 - log
        - adam_beta1 : 0.9 - 2025-03-25 15:22:38,208 - log
        - adam_beta2 : 0.999 - 2025-03-25 15:22:38,208 - log
        - scheduler : linear - 2025-03-25 15:22:38,208 - log
        - max_step : None - 2025-03-25 15:22:38,208 - log
        - max_epoch : 5 - 2025-03-25 15:22:38,208 - log
        - warmup_step : 500 - 2025-03-25 15:22:38,208 - log
        - i_steps : 0 - 2025-03-25 15:22:38,208 - log
        - i_lrs : 0.00025 - 2025-03-25 15:22:38,208 - log
        - random_seed : 2025 - 2025-03-25 15:22:38,208 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 15:22:38,208 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 15:22:38,208 - log
        - train_batch_size : 2 - 2025-03-25 15:22:38,208 - log
        - valid_batch_size : 1 - 2025-03-25 15:22:38,208 - log
        - grad_acc : 2 - 2025-03-25 15:22:38,208 - log
        - clip : 0.0 - 2025-03-25 15:22:38,208 - log
        - seq_len : 64 - 2025-03-25 15:22:38,208 - log
        - model_card : gpt2.sm - 2025-03-25 15:22:38,208 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 15:22:38,208 - log
        - fp16 : False - 2025-03-25 15:22:38,208 - log
        - log_interval : 100 - 2025-03-25 15:22:38,208 - log
        - eval_interval : 2000 - 2025-03-25 15:22:38,208 - log
        - save_interval : 1000 - 2025-03-25 15:22:38,208 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 15:22:38,208 - log
        - lora_dim : 4 - 2025-03-25 15:22:38,208 - log
        - lora_alpha : 32 - 2025-03-25 15:22:38,208 - log
        - obj : clm - 2025-03-25 15:22:38,208 - log
        - lora_dropout : 0.1 - 2025-03-25 15:22:38,208 - log
        - label_smooth : 0.1 - 2025-03-25 15:22:38,208 - log
        - roll_interval : -1 - 2025-03-25 15:22:38,208 - log
        - roll_lr : 1e-05 - 2025-03-25 15:22:38,208 - log
        - roll_step : 100 - 2025-03-25 15:22:38,208 - log
        - eval_epoch : 1 - 2025-03-25 15:22:38,208 - log
        - device : cuda - 2025-03-25 15:22:38,208 - log
==================================================================================================== - 2025-03-25 15:22:38,208 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 15:22:38,208 - log
loading model pretrained weight. - 2025-03-25 15:22:38,314 - log
set max_step: 1050 - 2025-03-25 15:22:39,224 - log
start to train the model................ 1 - 2025-03-25 15:22:39,224 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.71 | loss 15.19 | avg loss 16.53 | ppl 15075349.75 - 2025-03-25 15:22:42,795 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.59 | loss 16.86 | avg loss 16.56 | ppl 15480688.96 - 2025-03-25 15:22:46,055 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 15:22:46,380 - log
start to train the model................ 2 - 2025-03-25 15:22:47,860 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.19 | loss 17.93 | avg loss 16.76 | ppl 19042342.96 - 2025-03-25 15:22:50,779 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.76 | loss 15.73 | avg loss 16.61 | ppl 16292697.98 - 2025-03-25 15:22:54,055 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 15:22:54,706 - log
start to train the model................ 3 - 2025-03-25 15:22:56,207 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.17 | loss 15.37 | avg loss 16.79 | ppl 19552570.84 - 2025-03-25 15:22:58,824 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.83 | loss 16.83 | avg loss 16.25 | ppl 11388928.49 - 2025-03-25 15:23:02,108 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 15:23:03,099 - log
start to train the model................ 4 - 2025-03-25 15:23:04,606 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.98 | loss 16.52 | avg loss 16.73 | ppl 18431595.33 - 2025-03-25 15:23:06,904 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 33.02 | loss 16.37 | avg loss 15.80 | ppl 7301376.12 - 2025-03-25 15:23:10,206 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 15:23:11,526 - log
start to train the model................ 5 - 2025-03-25 15:23:12,903 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.65 | loss 13.76 | avg loss 15.86 | ppl 7722072.86 - 2025-03-25 15:23:14,869 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.92 | loss 14.58 | avg loss 15.49 | ppl 5355001.16 - 2025-03-25 15:23:18,161 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 15:23:18,161 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 15:23:19,817 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 15:23:21,172 - log
End of training - 2025-03-25 15:23:21,172 - log
==================================================================================================== - 2025-03-25 15:30:08,978 - log
        - random_seed : 2025 - 2025-03-25 15:30:08,979 - log
        - lr : 0.0002 - 2025-03-25 15:30:08,979 - log
        - weight_decay : 0.01 - 2025-03-25 15:30:08,979 - log
        - correct_bias : False - 2025-03-25 15:30:08,979 - log
        - adam_epislon : 1e-06 - 2025-03-25 15:30:08,979 - log
        - no_decay_bias : False - 2025-03-25 15:30:08,979 - log
        - adam_beta1 : 0.9 - 2025-03-25 15:30:08,979 - log
        - adam_beta2 : 0.999 - 2025-03-25 15:30:08,979 - log
        - scheduler : linear - 2025-03-25 15:30:08,979 - log
        - max_step : None - 2025-03-25 15:30:08,979 - log
        - max_epoch : 5 - 2025-03-25 15:30:08,979 - log
        - warmup_step : 500 - 2025-03-25 15:30:08,979 - log
        - i_steps : 0 - 2025-03-25 15:30:08,979 - log
        - i_lrs : 0.00025 - 2025-03-25 15:30:08,979 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 15:30:08,979 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 15:30:08,979 - log
        - train_batch_size : 2 - 2025-03-25 15:30:08,979 - log
        - valid_batch_size : 1 - 2025-03-25 15:30:08,979 - log
        - grad_acc : 2 - 2025-03-25 15:30:08,979 - log
        - clip : 0.0 - 2025-03-25 15:30:08,979 - log
        - seq_len : 64 - 2025-03-25 15:30:08,979 - log
        - model_card : gpt2.sm - 2025-03-25 15:30:08,979 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 15:30:08,979 - log
        - fp16 : False - 2025-03-25 15:30:08,979 - log
        - log_interval : 100 - 2025-03-25 15:30:08,979 - log
        - eval_interval : 2000 - 2025-03-25 15:30:08,979 - log
        - save_interval : 1000 - 2025-03-25 15:30:08,979 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 15:30:08,979 - log
        - lora_dim : 4 - 2025-03-25 15:30:08,979 - log
        - lora_alpha : 32 - 2025-03-25 15:30:08,979 - log
        - obj : clm - 2025-03-25 15:30:08,979 - log
        - lora_dropout : 0.1 - 2025-03-25 15:30:08,979 - log
        - label_smooth : 0.1 - 2025-03-25 15:30:08,979 - log
        - roll_interval : -1 - 2025-03-25 15:30:08,979 - log
        - roll_lr : 1e-05 - 2025-03-25 15:30:08,979 - log
        - roll_step : 100 - 2025-03-25 15:30:08,979 - log
        - eval_epoch : 1 - 2025-03-25 15:30:08,979 - log
        - device : cuda - 2025-03-25 15:30:08,979 - log
==================================================================================================== - 2025-03-25 15:30:08,979 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 15:30:08,979 - log
loading model pretrained weight. - 2025-03-25 15:30:09,086 - log
set max_step: 1050 - 2025-03-25 15:30:10,012 - log
start to train the model................ 1 - 2025-03-25 15:30:10,012 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.21 | loss 15.19 | avg loss 16.53 | ppl 15075365.42 - 2025-03-25 15:30:13,533 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.44 | loss 16.86 | avg loss 16.56 | ppl 15480627.54 - 2025-03-25 15:30:16,777 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 15:30:17,101 - log
start to train the model................ 2 - 2025-03-25 15:30:18,516 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.19 | loss 17.93 | avg loss 16.76 | ppl 19042201.71 - 2025-03-25 15:30:21,435 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.53 | loss 15.73 | avg loss 16.61 | ppl 16292717.40 - 2025-03-25 15:30:24,688 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 15:30:25,337 - log
start to train the model................ 3 - 2025-03-25 15:30:26,753 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.06 | loss 15.37 | avg loss 16.79 | ppl 19553173.38 - 2025-03-25 15:30:29,359 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.58 | loss 16.83 | avg loss 16.25 | ppl 11387282.68 - 2025-03-25 15:30:32,618 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 15:30:33,597 - log
start to train the model................ 4 - 2025-03-25 15:30:35,037 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.86 | loss 16.51 | avg loss 16.73 | ppl 18431944.38 - 2025-03-25 15:30:37,323 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.71 | loss 16.38 | avg loss 15.81 | ppl 7321596.61 - 2025-03-25 15:30:40,594 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 15:30:41,906 - log
start to train the model................ 5 - 2025-03-25 15:30:43,274 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.54 | loss 13.77 | avg loss 15.87 | ppl 7767981.37 - 2025-03-25 15:30:45,228 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.86 | loss 14.60 | avg loss 15.49 | ppl 5344675.02 - 2025-03-25 15:30:48,514 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 15:30:48,514 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 15:30:50,161 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 15:30:51,587 - log
End of training - 2025-03-25 15:30:51,587 - log
==================================================================================================== - 2025-03-25 15:37:25,606 - log
        - random_seed : 2025 - 2025-03-25 15:37:25,606 - log
        - lr : 0.0002 - 2025-03-25 15:37:25,606 - log
        - weight_decay : 0.01 - 2025-03-25 15:37:25,606 - log
        - correct_bias : False - 2025-03-25 15:37:25,606 - log
        - adam_epislon : 1e-06 - 2025-03-25 15:37:25,606 - log
        - no_decay_bias : False - 2025-03-25 15:37:25,606 - log
        - adam_beta1 : 0.9 - 2025-03-25 15:37:25,606 - log
        - adam_beta2 : 0.999 - 2025-03-25 15:37:25,606 - log
        - scheduler : linear - 2025-03-25 15:37:25,606 - log
        - max_step : None - 2025-03-25 15:37:25,606 - log
        - max_epoch : 5 - 2025-03-25 15:37:25,606 - log
        - warmup_step : 500 - 2025-03-25 15:37:25,606 - log
        - i_steps : 0 - 2025-03-25 15:37:25,607 - log
        - i_lrs : 0.00025 - 2025-03-25 15:37:25,607 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 15:37:25,607 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 15:37:25,607 - log
        - train_batch_size : 2 - 2025-03-25 15:37:25,607 - log
        - valid_batch_size : 1 - 2025-03-25 15:37:25,607 - log
        - grad_acc : 2 - 2025-03-25 15:37:25,607 - log
        - clip : 0.0 - 2025-03-25 15:37:25,607 - log
        - seq_len : 64 - 2025-03-25 15:37:25,607 - log
        - model_card : gpt2.sm - 2025-03-25 15:37:25,607 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 15:37:25,607 - log
        - fp16 : False - 2025-03-25 15:37:25,607 - log
        - log_interval : 100 - 2025-03-25 15:37:25,607 - log
        - eval_interval : 2000 - 2025-03-25 15:37:25,607 - log
        - save_interval : 1000 - 2025-03-25 15:37:25,607 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 15:37:25,607 - log
        - lora_dim : 4 - 2025-03-25 15:37:25,607 - log
        - lora_alpha : 32 - 2025-03-25 15:37:25,607 - log
        - obj : clm - 2025-03-25 15:37:25,607 - log
        - lora_dropout : 0.1 - 2025-03-25 15:37:25,607 - log
        - label_smooth : 0.1 - 2025-03-25 15:37:25,607 - log
        - roll_interval : -1 - 2025-03-25 15:37:25,607 - log
        - roll_lr : 1e-05 - 2025-03-25 15:37:25,607 - log
        - roll_step : 100 - 2025-03-25 15:37:25,607 - log
        - eval_epoch : 1 - 2025-03-25 15:37:25,607 - log
        - device : cuda - 2025-03-25 15:37:25,607 - log
==================================================================================================== - 2025-03-25 15:37:25,607 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 15:37:25,607 - log
loading model pretrained weight. - 2025-03-25 15:37:25,714 - log
set max_step: 1050 - 2025-03-25 15:37:26,829 - log
start to train the model................ 1 - 2025-03-25 15:37:26,829 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.84 | loss 15.19 | avg loss 16.53 | ppl 15075316.82 - 2025-03-25 15:37:30,313 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.13 | loss 16.86 | avg loss 16.56 | ppl 15480585.76 - 2025-03-25 15:37:33,525 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 15:37:33,847 - log
start to train the model................ 2 - 2025-03-25 15:37:35,351 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.03 | loss 17.93 | avg loss 16.76 | ppl 19042279.19 - 2025-03-25 15:37:38,254 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.31 | loss 15.73 | avg loss 16.61 | ppl 16292739.47 - 2025-03-25 15:37:41,486 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 15:37:42,134 - log
start to train the model................ 3 - 2025-03-25 15:37:43,616 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 25.87 | loss 15.37 | avg loss 16.79 | ppl 19552888.54 - 2025-03-25 15:37:46,203 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.68 | loss 16.83 | avg loss 16.25 | ppl 11395451.60 - 2025-03-25 15:37:49,471 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 15:37:50,454 - log
start to train the model................ 4 - 2025-03-25 15:37:51,847 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.80 | loss 16.49 | avg loss 16.73 | ppl 18482724.42 - 2025-03-25 15:37:54,127 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.58 | loss 16.37 | avg loss 15.80 | ppl 7274235.44 - 2025-03-25 15:37:57,385 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 15:37:58,688 - log
start to train the model................ 5 - 2025-03-25 15:38:00,197 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.60 | loss 13.66 | avg loss 15.85 | ppl 7655575.69 - 2025-03-25 15:38:02,156 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.61 | loss 14.57 | avg loss 15.50 | ppl 5375661.51 - 2025-03-25 15:38:05,418 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 15:38:05,418 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 15:38:07,055 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 15:38:08,461 - log
End of training - 2025-03-25 15:38:08,461 - log
==================================================================================================== - 2025-03-25 15:43:17,890 - log
        - random_seed : 2025 - 2025-03-25 15:43:17,891 - log
        - lr : 0.0002 - 2025-03-25 15:43:17,891 - log
        - weight_decay : 0.01 - 2025-03-25 15:43:17,891 - log
        - correct_bias : False - 2025-03-25 15:43:17,891 - log
        - adam_epislon : 1e-06 - 2025-03-25 15:43:17,891 - log
        - no_decay_bias : False - 2025-03-25 15:43:17,891 - log
        - adam_beta1 : 0.9 - 2025-03-25 15:43:17,891 - log
        - adam_beta2 : 0.999 - 2025-03-25 15:43:17,891 - log
        - scheduler : linear - 2025-03-25 15:43:17,891 - log
        - max_step : None - 2025-03-25 15:43:17,891 - log
        - max_epoch : 5 - 2025-03-25 15:43:17,891 - log
        - warmup_step : 500 - 2025-03-25 15:43:17,891 - log
        - i_steps : 0 - 2025-03-25 15:43:17,891 - log
        - i_lrs : 0.00025 - 2025-03-25 15:43:17,891 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 15:43:17,891 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 15:43:17,891 - log
        - train_batch_size : 2 - 2025-03-25 15:43:17,891 - log
        - valid_batch_size : 1 - 2025-03-25 15:43:17,891 - log
        - grad_acc : 2 - 2025-03-25 15:43:17,891 - log
        - clip : 0.0 - 2025-03-25 15:43:17,891 - log
        - seq_len : 64 - 2025-03-25 15:43:17,891 - log
        - model_card : gpt2.sm - 2025-03-25 15:43:17,891 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 15:43:17,891 - log
        - fp16 : False - 2025-03-25 15:43:17,891 - log
        - log_interval : 100 - 2025-03-25 15:43:17,891 - log
        - eval_interval : 2000 - 2025-03-25 15:43:17,891 - log
        - save_interval : 1000 - 2025-03-25 15:43:17,891 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 15:43:17,891 - log
        - lora_dim : 4 - 2025-03-25 15:43:17,891 - log
        - lora_alpha : 32 - 2025-03-25 15:43:17,891 - log
        - obj : clm - 2025-03-25 15:43:17,891 - log
        - lora_dropout : 0.1 - 2025-03-25 15:43:17,891 - log
        - label_smooth : 0.1 - 2025-03-25 15:43:17,891 - log
        - roll_interval : -1 - 2025-03-25 15:43:17,891 - log
        - roll_lr : 1e-05 - 2025-03-25 15:43:17,891 - log
        - roll_step : 100 - 2025-03-25 15:43:17,891 - log
        - eval_epoch : 1 - 2025-03-25 15:43:17,891 - log
        - device : cuda - 2025-03-25 15:43:17,891 - log
==================================================================================================== - 2025-03-25 15:43:17,891 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 15:43:17,891 - log
loading model pretrained weight. - 2025-03-25 15:43:18,003 - log
set max_step: 1050 - 2025-03-25 15:43:18,979 - log
start to train the model................ 1 - 2025-03-25 15:43:18,979 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.17 | loss 15.19 | avg loss 16.53 | ppl 15075408.84 - 2025-03-25 15:43:22,496 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.42 | loss 16.86 | avg loss 16.56 | ppl 15480647.18 - 2025-03-25 15:43:25,738 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 15:43:26,059 - log
start to train the model................ 2 - 2025-03-25 15:43:27,470 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.03 | loss 17.93 | avg loss 16.76 | ppl 19042240.45 - 2025-03-25 15:43:30,374 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.41 | loss 15.73 | avg loss 16.61 | ppl 16292653.54 - 2025-03-25 15:43:33,615 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 15:43:34,263 - log
start to train the model................ 3 - 2025-03-25 15:43:35,689 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 25.92 | loss 15.37 | avg loss 16.79 | ppl 19552950.78 - 2025-03-25 15:43:38,281 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.67 | loss 16.83 | avg loss 16.25 | ppl 11395431.39 - 2025-03-25 15:43:41,548 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 15:43:42,531 - log
start to train the model................ 4 - 2025-03-25 15:43:44,067 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.92 | loss 16.49 | avg loss 16.73 | ppl 18481585.03 - 2025-03-25 15:43:46,359 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.85 | loss 16.36 | avg loss 15.80 | ppl 7270827.28 - 2025-03-25 15:43:49,644 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 15:43:50,963 - log
start to train the model................ 5 - 2025-03-25 15:43:52,409 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.70 | loss 13.63 | avg loss 15.85 | ppl 7667807.57 - 2025-03-25 15:43:54,380 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 33.08 | loss 14.56 | avg loss 15.50 | ppl 5371624.83 - 2025-03-25 15:43:57,688 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 15:43:57,688 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 15:43:59,339 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 15:44:00,850 - log
End of training - 2025-03-25 15:44:00,850 - log
==================================================================================================== - 2025-03-25 15:48:32,538 - log
        - random_seed : 2025 - 2025-03-25 15:48:32,538 - log
        - lr : 0.0002 - 2025-03-25 15:48:32,538 - log
        - weight_decay : 0.01 - 2025-03-25 15:48:32,538 - log
        - correct_bias : False - 2025-03-25 15:48:32,538 - log
        - adam_epislon : 1e-06 - 2025-03-25 15:48:32,538 - log
        - no_decay_bias : False - 2025-03-25 15:48:32,538 - log
        - adam_beta1 : 0.9 - 2025-03-25 15:48:32,538 - log
        - adam_beta2 : 0.999 - 2025-03-25 15:48:32,538 - log
        - scheduler : linear - 2025-03-25 15:48:32,538 - log
        - max_step : None - 2025-03-25 15:48:32,538 - log
        - max_epoch : 5 - 2025-03-25 15:48:32,538 - log
        - warmup_step : 500 - 2025-03-25 15:48:32,538 - log
        - i_steps : 0 - 2025-03-25 15:48:32,538 - log
        - i_lrs : 0.00025 - 2025-03-25 15:48:32,538 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 15:48:32,538 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 15:48:32,538 - log
        - train_batch_size : 2 - 2025-03-25 15:48:32,538 - log
        - valid_batch_size : 1 - 2025-03-25 15:48:32,538 - log
        - grad_acc : 2 - 2025-03-25 15:48:32,538 - log
        - clip : 0.0 - 2025-03-25 15:48:32,538 - log
        - seq_len : 64 - 2025-03-25 15:48:32,538 - log
        - model_card : gpt2.sm - 2025-03-25 15:48:32,538 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 15:48:32,538 - log
        - fp16 : False - 2025-03-25 15:48:32,538 - log
        - log_interval : 100 - 2025-03-25 15:48:32,538 - log
        - eval_interval : 2000 - 2025-03-25 15:48:32,538 - log
        - save_interval : 1000 - 2025-03-25 15:48:32,538 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 15:48:32,538 - log
        - lora_dim : 4 - 2025-03-25 15:48:32,538 - log
        - lora_alpha : 32 - 2025-03-25 15:48:32,539 - log
        - obj : clm - 2025-03-25 15:48:32,539 - log
        - lora_dropout : 0.1 - 2025-03-25 15:48:32,539 - log
        - label_smooth : 0.1 - 2025-03-25 15:48:32,539 - log
        - roll_interval : -1 - 2025-03-25 15:48:32,539 - log
        - roll_lr : 1e-05 - 2025-03-25 15:48:32,539 - log
        - roll_step : 100 - 2025-03-25 15:48:32,539 - log
        - eval_epoch : 1 - 2025-03-25 15:48:32,539 - log
        - device : cuda - 2025-03-25 15:48:32,539 - log
==================================================================================================== - 2025-03-25 15:48:32,539 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 15:48:32,539 - log
loading model pretrained weight. - 2025-03-25 15:48:32,649 - log
set max_step: 1050 - 2025-03-25 15:48:33,734 - log
start to train the model................ 1 - 2025-03-25 15:48:33,735 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.19 | loss 15.19 | avg loss 16.53 | ppl 15075319.56 - 2025-03-25 15:48:37,253 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.31 | loss 16.86 | avg loss 16.56 | ppl 15480581.04 - 2025-03-25 15:48:40,484 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 15:48:40,812 - log
start to train the model................ 2 - 2025-03-25 15:48:42,222 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.52 | loss 17.93 | avg loss 16.76 | ppl 19042199.09 - 2025-03-25 15:48:45,174 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.49 | loss 15.73 | avg loss 16.61 | ppl 16292698.29 - 2025-03-25 15:48:48,424 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 15:48:49,076 - log
start to train the model................ 3 - 2025-03-25 15:48:50,552 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.65 | loss 15.37 | avg loss 16.79 | ppl 19552645.43 - 2025-03-25 15:48:53,217 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.62 | loss 16.86 | avg loss 16.25 | ppl 11386294.27 - 2025-03-25 15:48:56,479 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 15:48:57,462 - log
start to train the model................ 4 - 2025-03-25 15:48:58,934 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.85 | loss 16.53 | avg loss 16.74 | ppl 18658215.57 - 2025-03-25 15:49:01,219 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.73 | loss 16.39 | avg loss 15.80 | ppl 7297873.95 - 2025-03-25 15:49:04,492 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 15:49:05,799 - log
start to train the model................ 5 - 2025-03-25 15:49:07,200 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.59 | loss 13.72 | avg loss 15.87 | ppl 7815752.02 - 2025-03-25 15:49:09,160 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.89 | loss 14.58 | avg loss 15.51 | ppl 5423460.51 - 2025-03-25 15:49:12,449 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 15:49:12,449 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 15:49:14,093 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 15:49:15,574 - log
End of training - 2025-03-25 15:49:15,574 - log
==================================================================================================== - 2025-03-25 15:59:01,804 - log
        - random_seed : 2025 - 2025-03-25 15:59:01,804 - log
        - lr : 0.0002 - 2025-03-25 15:59:01,804 - log
        - weight_decay : 0.01 - 2025-03-25 15:59:01,804 - log
        - correct_bias : False - 2025-03-25 15:59:01,804 - log
        - adam_epislon : 1e-06 - 2025-03-25 15:59:01,804 - log
        - no_decay_bias : False - 2025-03-25 15:59:01,804 - log
        - adam_beta1 : 0.9 - 2025-03-25 15:59:01,804 - log
        - adam_beta2 : 0.999 - 2025-03-25 15:59:01,804 - log
        - scheduler : linear - 2025-03-25 15:59:01,804 - log
        - max_step : None - 2025-03-25 15:59:01,804 - log
        - max_epoch : 5 - 2025-03-25 15:59:01,804 - log
        - warmup_step : 500 - 2025-03-25 15:59:01,804 - log
        - i_steps : 0 - 2025-03-25 15:59:01,804 - log
        - i_lrs : 0.00025 - 2025-03-25 15:59:01,804 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 15:59:01,804 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 15:59:01,804 - log
        - train_batch_size : 2 - 2025-03-25 15:59:01,804 - log
        - valid_batch_size : 1 - 2025-03-25 15:59:01,804 - log
        - grad_acc : 2 - 2025-03-25 15:59:01,804 - log
        - clip : 0.0 - 2025-03-25 15:59:01,804 - log
        - seq_len : 64 - 2025-03-25 15:59:01,804 - log
        - model_card : gpt2.sm - 2025-03-25 15:59:01,804 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 15:59:01,804 - log
        - fp16 : False - 2025-03-25 15:59:01,804 - log
        - log_interval : 100 - 2025-03-25 15:59:01,804 - log
        - eval_interval : 2000 - 2025-03-25 15:59:01,804 - log
        - save_interval : 1000 - 2025-03-25 15:59:01,804 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 15:59:01,805 - log
        - lora_dim : 4 - 2025-03-25 15:59:01,805 - log
        - lora_alpha : 32 - 2025-03-25 15:59:01,805 - log
        - obj : clm - 2025-03-25 15:59:01,805 - log
        - lora_dropout : 0.1 - 2025-03-25 15:59:01,805 - log
        - label_smooth : 0.1 - 2025-03-25 15:59:01,805 - log
        - roll_interval : -1 - 2025-03-25 15:59:01,805 - log
        - roll_lr : 1e-05 - 2025-03-25 15:59:01,805 - log
        - roll_step : 100 - 2025-03-25 15:59:01,805 - log
        - eval_epoch : 1 - 2025-03-25 15:59:01,805 - log
        - device : cuda - 2025-03-25 15:59:01,805 - log
==================================================================================================== - 2025-03-25 15:59:01,805 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 15:59:01,805 - log
loading model pretrained weight. - 2025-03-25 15:59:01,914 - log
set max_step: 1050 - 2025-03-25 15:59:02,817 - log
start to train the model................ 1 - 2025-03-25 15:59:02,817 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.59 | loss 15.19 | avg loss 16.53 | ppl 15075354.20 - 2025-03-25 15:59:06,276 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.13 | loss 16.86 | avg loss 16.56 | ppl 15480697.22 - 2025-03-25 15:59:09,489 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 15:59:09,812 - log
start to train the model................ 2 - 2025-03-25 15:59:11,207 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 28.97 | loss 17.93 | avg loss 16.76 | ppl 19042184.76 - 2025-03-25 15:59:14,105 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.35 | loss 15.73 | avg loss 16.61 | ppl 16292670.32 - 2025-03-25 15:59:17,340 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 15:59:17,984 - log
start to train the model................ 3 - 2025-03-25 15:59:19,425 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 25.72 | loss 15.37 | avg loss 16.79 | ppl 19552498.12 - 2025-03-25 15:59:21,997 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.22 | loss 16.85 | avg loss 16.25 | ppl 11378031.10 - 2025-03-25 15:59:25,220 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 15:59:26,186 - log
start to train the model................ 4 - 2025-03-25 15:59:27,634 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.55 | loss 16.54 | avg loss 16.73 | ppl 18470613.20 - 2025-03-25 15:59:29,889 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.49 | loss 16.37 | avg loss 15.81 | ppl 7327145.61 - 2025-03-25 15:59:33,138 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 15:59:34,437 - log
start to train the model................ 5 - 2025-03-25 15:59:35,885 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.48 | loss 13.82 | avg loss 15.86 | ppl 7757823.21 - 2025-03-25 15:59:37,833 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.60 | loss 14.66 | avg loss 15.49 | ppl 5353291.99 - 2025-03-25 15:59:41,093 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 15:59:41,093 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 15:59:42,732 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 15:59:44,175 - log
End of training - 2025-03-25 15:59:44,175 - log
==================================================================================================== - 2025-03-25 17:05:44,273 - log
        - random_seed : 2025 - 2025-03-25 17:05:44,274 - log
        - lr : 0.0002 - 2025-03-25 17:05:44,274 - log
        - weight_decay : 0.01 - 2025-03-25 17:05:44,274 - log
        - correct_bias : False - 2025-03-25 17:05:44,274 - log
        - adam_epislon : 1e-06 - 2025-03-25 17:05:44,274 - log
        - no_decay_bias : False - 2025-03-25 17:05:44,274 - log
        - adam_beta1 : 0.9 - 2025-03-25 17:05:44,274 - log
        - adam_beta2 : 0.999 - 2025-03-25 17:05:44,274 - log
        - scheduler : linear - 2025-03-25 17:05:44,274 - log
        - max_step : None - 2025-03-25 17:05:44,274 - log
        - max_epoch : 5 - 2025-03-25 17:05:44,274 - log
        - warmup_step : 500 - 2025-03-25 17:05:44,274 - log
        - i_steps : 0 - 2025-03-25 17:05:44,274 - log
        - i_lrs : 0.00025 - 2025-03-25 17:05:44,274 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 17:05:44,274 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 17:05:44,274 - log
        - train_batch_size : 2 - 2025-03-25 17:05:44,274 - log
        - valid_batch_size : 1 - 2025-03-25 17:05:44,274 - log
        - grad_acc : 2 - 2025-03-25 17:05:44,274 - log
        - clip : 0.0 - 2025-03-25 17:05:44,274 - log
        - seq_len : 64 - 2025-03-25 17:05:44,274 - log
        - model_card : gpt2.sm - 2025-03-25 17:05:44,274 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 17:05:44,274 - log
        - fp16 : False - 2025-03-25 17:05:44,274 - log
        - log_interval : 100 - 2025-03-25 17:05:44,274 - log
        - eval_interval : 2000 - 2025-03-25 17:05:44,274 - log
        - save_interval : 1000 - 2025-03-25 17:05:44,274 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 17:05:44,274 - log
        - lora_dim : 4 - 2025-03-25 17:05:44,274 - log
        - lora_alpha : 32 - 2025-03-25 17:05:44,274 - log
        - obj : clm - 2025-03-25 17:05:44,274 - log
        - lora_dropout : 0.1 - 2025-03-25 17:05:44,274 - log
        - label_smooth : 0.1 - 2025-03-25 17:05:44,274 - log
        - roll_interval : -1 - 2025-03-25 17:05:44,274 - log
        - roll_lr : 1e-05 - 2025-03-25 17:05:44,274 - log
        - roll_step : 100 - 2025-03-25 17:05:44,274 - log
        - eval_epoch : 1 - 2025-03-25 17:05:44,274 - log
        - device : cuda - 2025-03-25 17:05:44,274 - log
==================================================================================================== - 2025-03-25 17:05:44,274 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 17:05:44,274 - log
loading model pretrained weight. - 2025-03-25 17:05:44,392 - log
set max_step: 1050 - 2025-03-25 17:05:45,498 - log
start to train the model................ 1 - 2025-03-25 17:05:45,498 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.98 | loss 15.19 | avg loss 16.53 | ppl 15075380.66 - 2025-03-25 17:05:49,096 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.70 | loss 16.86 | avg loss 16.56 | ppl 15480545.90 - 2025-03-25 17:05:52,366 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 17:05:52,692 - log
start to train the model................ 2 - 2025-03-25 17:05:54,113 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.32 | loss 17.93 | avg loss 16.76 | ppl 19042263.25 - 2025-03-25 17:05:57,045 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 33.04 | loss 15.73 | avg loss 16.61 | ppl 16292657.58 - 2025-03-25 17:06:00,350 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 17:06:01,086 - log
start to train the model................ 3 - 2025-03-25 17:06:02,547 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.08 | loss 15.37 | avg loss 16.79 | ppl 19553111.61 - 2025-03-25 17:06:05,155 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.61 | loss 16.85 | avg loss 16.25 | ppl 11380953.20 - 2025-03-25 17:06:08,416 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 17:06:09,396 - log
start to train the model................ 4 - 2025-03-25 17:06:10,790 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.86 | loss 16.60 | avg loss 16.74 | ppl 18563938.91 - 2025-03-25 17:06:13,076 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.83 | loss 16.37 | avg loss 15.81 | ppl 7383239.40 - 2025-03-25 17:06:16,360 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 17:06:17,676 - log
start to train the model................ 5 - 2025-03-25 17:06:19,125 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 20.13 | loss 13.74 | avg loss 15.87 | ppl 7786015.62 - 2025-03-25 17:06:21,138 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 33.08 | loss 14.61 | avg loss 15.50 | ppl 5369004.30 - 2025-03-25 17:06:24,446 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 17:06:24,446 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 17:06:26,097 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 17:06:27,476 - log
End of training - 2025-03-25 17:06:27,476 - log
==================================================================================================== - 2025-03-25 17:10:57,654 - log
        - random_seed : 2025 - 2025-03-25 17:10:57,655 - log
        - lr : 0.0002 - 2025-03-25 17:10:57,655 - log
        - weight_decay : 0.01 - 2025-03-25 17:10:57,655 - log
        - correct_bias : False - 2025-03-25 17:10:57,655 - log
        - adam_epislon : 1e-06 - 2025-03-25 17:10:57,655 - log
        - no_decay_bias : False - 2025-03-25 17:10:57,655 - log
        - adam_beta1 : 0.9 - 2025-03-25 17:10:57,655 - log
        - adam_beta2 : 0.999 - 2025-03-25 17:10:57,655 - log
        - scheduler : linear - 2025-03-25 17:10:57,655 - log
        - max_step : None - 2025-03-25 17:10:57,655 - log
        - max_epoch : 5 - 2025-03-25 17:10:57,655 - log
        - warmup_step : 500 - 2025-03-25 17:10:57,655 - log
        - i_steps : 0 - 2025-03-25 17:10:57,655 - log
        - i_lrs : 0.00025 - 2025-03-25 17:10:57,655 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 17:10:57,655 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 17:10:57,655 - log
        - train_batch_size : 2 - 2025-03-25 17:10:57,655 - log
        - valid_batch_size : 1 - 2025-03-25 17:10:57,655 - log
        - grad_acc : 2 - 2025-03-25 17:10:57,655 - log
        - clip : 0.0 - 2025-03-25 17:10:57,655 - log
        - seq_len : 64 - 2025-03-25 17:10:57,655 - log
        - model_card : gpt2.sm - 2025-03-25 17:10:57,655 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 17:10:57,655 - log
        - fp16 : False - 2025-03-25 17:10:57,655 - log
        - log_interval : 100 - 2025-03-25 17:10:57,655 - log
        - eval_interval : 2000 - 2025-03-25 17:10:57,655 - log
        - save_interval : 1000 - 2025-03-25 17:10:57,655 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 17:10:57,655 - log
        - lora_dim : 4 - 2025-03-25 17:10:57,655 - log
        - lora_alpha : 32 - 2025-03-25 17:10:57,655 - log
        - obj : clm - 2025-03-25 17:10:57,655 - log
        - lora_dropout : 0.1 - 2025-03-25 17:10:57,655 - log
        - label_smooth : 0.1 - 2025-03-25 17:10:57,655 - log
        - roll_interval : -1 - 2025-03-25 17:10:57,655 - log
        - roll_lr : 1e-05 - 2025-03-25 17:10:57,655 - log
        - roll_step : 100 - 2025-03-25 17:10:57,655 - log
        - eval_epoch : 1 - 2025-03-25 17:10:57,655 - log
        - device : cuda - 2025-03-25 17:10:57,655 - log
==================================================================================================== - 2025-03-25 17:10:57,655 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 17:10:57,655 - log
loading model pretrained weight. - 2025-03-25 17:10:57,769 - log
set max_step: 1050 - 2025-03-25 17:10:59,415 - log
start to train the model................ 1 - 2025-03-25 17:10:59,415 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.86 | loss 15.19 | avg loss 16.53 | ppl 15075314.52 - 2025-03-25 17:11:03,001 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 33.10 | loss 16.86 | avg loss 16.56 | ppl 15480591.08 - 2025-03-25 17:11:06,311 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 17:11:06,640 - log
start to train the model................ 2 - 2025-03-25 17:11:07,848 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.46 | loss 17.93 | avg loss 16.76 | ppl 19042232.99 - 2025-03-25 17:11:10,795 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.92 | loss 15.73 | avg loss 16.61 | ppl 16292684.15 - 2025-03-25 17:11:14,087 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 17:11:14,744 - log
start to train the model................ 3 - 2025-03-25 17:11:15,936 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.42 | loss 15.37 | avg loss 16.79 | ppl 19553418.13 - 2025-03-25 17:11:18,578 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 33.68 | loss 16.86 | avg loss 16.25 | ppl 11382918.00 - 2025-03-25 17:11:21,946 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 17:11:22,938 - log
start to train the model................ 4 - 2025-03-25 17:11:24,205 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 23.99 | loss 16.55 | avg loss 16.74 | ppl 18630736.94 - 2025-03-25 17:11:26,604 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 33.45 | loss 16.36 | avg loss 15.81 | ppl 7333448.44 - 2025-03-25 17:11:29,950 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 17:11:31,279 - log
start to train the model................ 5 - 2025-03-25 17:11:32,491 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 20.17 | loss 13.71 | avg loss 15.87 | ppl 7772786.90 - 2025-03-25 17:11:34,508 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 33.22 | loss 14.53 | avg loss 15.51 | ppl 5433641.42 - 2025-03-25 17:11:37,830 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 17:11:37,830 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 17:11:39,501 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 17:11:40,855 - log
End of training - 2025-03-25 17:11:40,866 - log
==================================================================================================== - 2025-03-25 17:19:36,042 - log
        - random_seed : 2025 - 2025-03-25 17:19:36,042 - log
        - lr : 0.0002 - 2025-03-25 17:19:36,042 - log
        - weight_decay : 0.01 - 2025-03-25 17:19:36,042 - log
        - correct_bias : False - 2025-03-25 17:19:36,042 - log
        - adam_epislon : 1e-06 - 2025-03-25 17:19:36,042 - log
        - no_decay_bias : False - 2025-03-25 17:19:36,042 - log
        - adam_beta1 : 0.9 - 2025-03-25 17:19:36,042 - log
        - adam_beta2 : 0.999 - 2025-03-25 17:19:36,042 - log
        - scheduler : linear - 2025-03-25 17:19:36,042 - log
        - max_step : None - 2025-03-25 17:19:36,042 - log
        - max_epoch : 5 - 2025-03-25 17:19:36,042 - log
        - warmup_step : 500 - 2025-03-25 17:19:36,042 - log
        - i_steps : 0 - 2025-03-25 17:19:36,042 - log
        - i_lrs : 0.00025 - 2025-03-25 17:19:36,042 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 17:19:36,042 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 17:19:36,042 - log
        - train_batch_size : 2 - 2025-03-25 17:19:36,042 - log
        - valid_batch_size : 1 - 2025-03-25 17:19:36,042 - log
        - grad_acc : 2 - 2025-03-25 17:19:36,042 - log
        - clip : 0.0 - 2025-03-25 17:19:36,042 - log
        - seq_len : 64 - 2025-03-25 17:19:36,042 - log
        - model_card : gpt2.sm - 2025-03-25 17:19:36,042 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 17:19:36,042 - log
        - fp16 : False - 2025-03-25 17:19:36,042 - log
        - log_interval : 100 - 2025-03-25 17:19:36,042 - log
        - eval_interval : 2000 - 2025-03-25 17:19:36,042 - log
        - save_interval : 1000 - 2025-03-25 17:19:36,042 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 17:19:36,042 - log
        - lora_dim : 4 - 2025-03-25 17:19:36,042 - log
        - lora_alpha : 32 - 2025-03-25 17:19:36,042 - log
        - obj : clm - 2025-03-25 17:19:36,042 - log
        - lora_dropout : 0.1 - 2025-03-25 17:19:36,042 - log
        - label_smooth : 0.1 - 2025-03-25 17:19:36,042 - log
        - roll_interval : -1 - 2025-03-25 17:19:36,043 - log
        - roll_lr : 1e-05 - 2025-03-25 17:19:36,043 - log
        - roll_step : 100 - 2025-03-25 17:19:36,043 - log
        - eval_epoch : 1 - 2025-03-25 17:19:36,043 - log
        - device : cuda - 2025-03-25 17:19:36,043 - log
==================================================================================================== - 2025-03-25 17:19:36,043 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 17:19:36,043 - log
loading model pretrained weight. - 2025-03-25 17:19:36,152 - log
set max_step: 1050 - 2025-03-25 17:19:37,740 - log
start to train the model................ 1 - 2025-03-25 17:19:37,740 - log
==================================================================================================== - 2025-03-25 17:44:49,919 - log
        - random_seed : 2025 - 2025-03-25 17:44:49,919 - log
        - lr : 0.0002 - 2025-03-25 17:44:49,919 - log
        - weight_decay : 0.01 - 2025-03-25 17:44:49,919 - log
        - correct_bias : False - 2025-03-25 17:44:49,919 - log
        - adam_epislon : 1e-06 - 2025-03-25 17:44:49,919 - log
        - no_decay_bias : False - 2025-03-25 17:44:49,919 - log
        - adam_beta1 : 0.9 - 2025-03-25 17:44:49,919 - log
        - adam_beta2 : 0.999 - 2025-03-25 17:44:49,919 - log
        - scheduler : linear - 2025-03-25 17:44:49,919 - log
        - max_step : None - 2025-03-25 17:44:49,919 - log
        - max_epoch : 5 - 2025-03-25 17:44:49,919 - log
        - warmup_step : 500 - 2025-03-25 17:44:49,919 - log
        - i_steps : 0 - 2025-03-25 17:44:49,919 - log
        - i_lrs : 0.00025 - 2025-03-25 17:44:49,919 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 17:44:49,919 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 17:44:49,919 - log
        - train_batch_size : 2 - 2025-03-25 17:44:49,919 - log
        - valid_batch_size : 1 - 2025-03-25 17:44:49,919 - log
        - grad_acc : 2 - 2025-03-25 17:44:49,919 - log
        - clip : 0.0 - 2025-03-25 17:44:49,919 - log
        - seq_len : 64 - 2025-03-25 17:44:49,919 - log
        - model_card : gpt2.sm - 2025-03-25 17:44:49,920 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 17:44:49,920 - log
        - fp16 : False - 2025-03-25 17:44:49,920 - log
        - log_interval : 100 - 2025-03-25 17:44:49,920 - log
        - eval_interval : 2000 - 2025-03-25 17:44:49,920 - log
        - save_interval : 1000 - 2025-03-25 17:44:49,920 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 17:44:49,920 - log
        - lora_dim : 4 - 2025-03-25 17:44:49,920 - log
        - lora_alpha : 32 - 2025-03-25 17:44:49,920 - log
        - obj : clm - 2025-03-25 17:44:49,920 - log
        - lora_dropout : 0.1 - 2025-03-25 17:44:49,920 - log
        - label_smooth : 0.1 - 2025-03-25 17:44:49,920 - log
        - roll_interval : -1 - 2025-03-25 17:44:49,920 - log
        - roll_lr : 1e-05 - 2025-03-25 17:44:49,920 - log
        - roll_step : 100 - 2025-03-25 17:44:49,920 - log
        - eval_epoch : 1 - 2025-03-25 17:44:49,920 - log
        - device : cuda - 2025-03-25 17:44:49,920 - log
==================================================================================================== - 2025-03-25 17:44:49,920 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 17:44:49,920 - log
loading model pretrained weight. - 2025-03-25 17:44:50,026 - log
set max_step: 1050 - 2025-03-25 17:44:50,795 - log
start to train the model................ 1 - 2025-03-25 17:44:50,795 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.49 | loss 15.19 | avg loss 16.53 | ppl 15075368.44 - 2025-03-25 17:44:54,245 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.06 | loss 16.86 | avg loss 16.56 | ppl 15480758.79 - 2025-03-25 17:44:57,451 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 17:44:57,784 - log
start to train the model................ 2 - 2025-03-25 17:44:59,167 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 28.98 | loss 17.93 | avg loss 16.76 | ppl 19042274.15 - 2025-03-25 17:45:02,066 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.34 | loss 15.73 | avg loss 16.61 | ppl 16292665.50 - 2025-03-25 17:45:05,300 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 17:45:05,965 - log
start to train the model................ 3 - 2025-03-25 17:45:07,331 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.55 | loss 15.37 | avg loss 16.79 | ppl 19553729.78 - 2025-03-25 17:45:09,987 - log
==================================================================================================== - 2025-03-25 17:45:15,589 - log
        - random_seed : 2025 - 2025-03-25 17:45:15,589 - log
        - lr : 0.0002 - 2025-03-25 17:45:15,589 - log
        - weight_decay : 0.01 - 2025-03-25 17:45:15,589 - log
        - correct_bias : False - 2025-03-25 17:45:15,589 - log
        - adam_epislon : 1e-06 - 2025-03-25 17:45:15,589 - log
        - no_decay_bias : False - 2025-03-25 17:45:15,589 - log
        - adam_beta1 : 0.9 - 2025-03-25 17:45:15,589 - log
        - adam_beta2 : 0.999 - 2025-03-25 17:45:15,589 - log
        - scheduler : linear - 2025-03-25 17:45:15,589 - log
        - max_step : None - 2025-03-25 17:45:15,589 - log
        - max_epoch : 5 - 2025-03-25 17:45:15,589 - log
        - warmup_step : 500 - 2025-03-25 17:45:15,589 - log
        - i_steps : 0 - 2025-03-25 17:45:15,589 - log
        - i_lrs : 0.00025 - 2025-03-25 17:45:15,589 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 17:45:15,589 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 17:45:15,589 - log
        - train_batch_size : 2 - 2025-03-25 17:45:15,589 - log
        - valid_batch_size : 1 - 2025-03-25 17:45:15,589 - log
        - grad_acc : 2 - 2025-03-25 17:45:15,589 - log
        - clip : 0.0 - 2025-03-25 17:45:15,589 - log
        - seq_len : 64 - 2025-03-25 17:45:15,589 - log
        - model_card : gpt2.sm - 2025-03-25 17:45:15,589 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 17:45:15,589 - log
        - fp16 : False - 2025-03-25 17:45:15,589 - log
        - log_interval : 100 - 2025-03-25 17:45:15,589 - log
        - eval_interval : 2000 - 2025-03-25 17:45:15,589 - log
        - save_interval : 1000 - 2025-03-25 17:45:15,589 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 17:45:15,589 - log
        - lora_dim : 4 - 2025-03-25 17:45:15,589 - log
        - lora_alpha : 32 - 2025-03-25 17:45:15,589 - log
        - obj : clm - 2025-03-25 17:45:15,589 - log
        - lora_dropout : 0.1 - 2025-03-25 17:45:15,589 - log
        - label_smooth : 0.1 - 2025-03-25 17:45:15,589 - log
        - roll_interval : -1 - 2025-03-25 17:45:15,589 - log
        - roll_lr : 1e-05 - 2025-03-25 17:45:15,589 - log
        - roll_step : 100 - 2025-03-25 17:45:15,589 - log
        - eval_epoch : 1 - 2025-03-25 17:45:15,589 - log
        - device : cuda - 2025-03-25 17:45:15,590 - log
==================================================================================================== - 2025-03-25 17:45:15,590 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 17:45:15,590 - log
loading model pretrained weight. - 2025-03-25 17:45:15,696 - log
set max_step: 1050 - 2025-03-25 17:45:16,592 - log
start to train the model................ 1 - 2025-03-25 17:45:16,592 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.57 | loss 15.19 | avg loss 16.53 | ppl 15075346.30 - 2025-03-25 17:45:20,149 - log
==================================================================================================== - 2025-03-25 17:47:29,933 - log
        - random_seed : 2025 - 2025-03-25 17:47:29,933 - log
        - lr : 0.0002 - 2025-03-25 17:47:29,933 - log
        - weight_decay : 0.01 - 2025-03-25 17:47:29,933 - log
        - correct_bias : False - 2025-03-25 17:47:29,933 - log
        - adam_epislon : 1e-06 - 2025-03-25 17:47:29,933 - log
        - no_decay_bias : False - 2025-03-25 17:47:29,933 - log
        - adam_beta1 : 0.9 - 2025-03-25 17:47:29,933 - log
        - adam_beta2 : 0.999 - 2025-03-25 17:47:29,933 - log
        - scheduler : linear - 2025-03-25 17:47:29,933 - log
        - max_step : None - 2025-03-25 17:47:29,933 - log
        - max_epoch : 5 - 2025-03-25 17:47:29,933 - log
        - warmup_step : 500 - 2025-03-25 17:47:29,933 - log
        - i_steps : 0 - 2025-03-25 17:47:29,933 - log
        - i_lrs : 0.00025 - 2025-03-25 17:47:29,933 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 17:47:29,933 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 17:47:29,933 - log
        - train_batch_size : 2 - 2025-03-25 17:47:29,933 - log
        - valid_batch_size : 1 - 2025-03-25 17:47:29,933 - log
        - grad_acc : 2 - 2025-03-25 17:47:29,933 - log
        - clip : 0.0 - 2025-03-25 17:47:29,933 - log
        - seq_len : 64 - 2025-03-25 17:47:29,933 - log
        - model_card : gpt2.sm - 2025-03-25 17:47:29,933 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 17:47:29,933 - log
        - fp16 : False - 2025-03-25 17:47:29,933 - log
        - log_interval : 100 - 2025-03-25 17:47:29,933 - log
        - eval_interval : 2000 - 2025-03-25 17:47:29,933 - log
        - save_interval : 1000 - 2025-03-25 17:47:29,933 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 17:47:29,933 - log
        - lora_dim : 4 - 2025-03-25 17:47:29,933 - log
        - lora_alpha : 32 - 2025-03-25 17:47:29,933 - log
        - obj : clm - 2025-03-25 17:47:29,933 - log
        - lora_dropout : 0.1 - 2025-03-25 17:47:29,933 - log
        - label_smooth : 0.1 - 2025-03-25 17:47:29,933 - log
        - roll_interval : -1 - 2025-03-25 17:47:29,933 - log
        - roll_lr : 1e-05 - 2025-03-25 17:47:29,933 - log
        - roll_step : 100 - 2025-03-25 17:47:29,933 - log
        - eval_epoch : 1 - 2025-03-25 17:47:29,933 - log
        - device : cuda - 2025-03-25 17:47:29,934 - log
==================================================================================================== - 2025-03-25 17:47:29,934 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 17:47:29,934 - log
loading model pretrained weight. - 2025-03-25 17:47:30,088 - log
set max_step: 1050 - 2025-03-25 17:47:32,108 - log
start to train the model................ 1 - 2025-03-25 17:47:32,113 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 56.22 | loss 15.19 | avg loss 16.53 | ppl 15075336.66 - 2025-03-25 17:47:37,735 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.33 | loss 16.86 | avg loss 16.56 | ppl 15480654.26 - 2025-03-25 17:47:40,968 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 17:47:41,293 - log
start to train the model................ 2 - 2025-03-25 17:47:42,723 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.17 | loss 17.93 | avg loss 16.76 | ppl 19042300.99 - 2025-03-25 17:47:45,640 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.65 | loss 15.73 | avg loss 16.61 | ppl 16292590.30 - 2025-03-25 17:47:48,905 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 17:47:49,569 - log
start to train the model................ 3 - 2025-03-25 17:47:51,028 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.32 | loss 15.37 | avg loss 16.79 | ppl 19553072.92 - 2025-03-25 17:47:53,660 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.70 | loss 16.83 | avg loss 16.25 | ppl 11396909.58 - 2025-03-25 17:47:56,930 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 17:47:57,906 - log
start to train the model................ 4 - 2025-03-25 17:47:59,277 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.86 | loss 16.48 | avg loss 16.73 | ppl 18471618.53 - 2025-03-25 17:48:01,563 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 33.10 | loss 16.37 | avg loss 15.80 | ppl 7277895.14 - 2025-03-25 17:48:04,874 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 17:48:06,208 - log
start to train the model................ 5 - 2025-03-25 17:48:07,610 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.82 | loss 13.59 | avg loss 15.86 | ppl 7756052.84 - 2025-03-25 17:48:09,592 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 33.11 | loss 14.57 | avg loss 15.50 | ppl 5390962.30 - 2025-03-25 17:48:12,903 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 17:48:12,903 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 17:48:14,600 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 17:48:16,020 - log
End of training - 2025-03-25 17:48:16,020 - log
==================================================================================================== - 2025-03-25 17:52:20,259 - log
        - random_seed : 2025 - 2025-03-25 17:52:20,259 - log
        - lr : 0.0002 - 2025-03-25 17:52:20,259 - log
        - weight_decay : 0.01 - 2025-03-25 17:52:20,259 - log
        - correct_bias : False - 2025-03-25 17:52:20,259 - log
        - adam_epislon : 1e-06 - 2025-03-25 17:52:20,259 - log
        - no_decay_bias : False - 2025-03-25 17:52:20,259 - log
        - adam_beta1 : 0.9 - 2025-03-25 17:52:20,259 - log
        - adam_beta2 : 0.999 - 2025-03-25 17:52:20,259 - log
        - scheduler : linear - 2025-03-25 17:52:20,259 - log
        - max_step : None - 2025-03-25 17:52:20,259 - log
        - max_epoch : 5 - 2025-03-25 17:52:20,259 - log
        - warmup_step : 500 - 2025-03-25 17:52:20,259 - log
        - i_steps : 0 - 2025-03-25 17:52:20,259 - log
        - i_lrs : 0.00025 - 2025-03-25 17:52:20,259 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 17:52:20,259 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 17:52:20,259 - log
        - train_batch_size : 2 - 2025-03-25 17:52:20,259 - log
        - valid_batch_size : 1 - 2025-03-25 17:52:20,259 - log
        - grad_acc : 2 - 2025-03-25 17:52:20,259 - log
        - clip : 0.0 - 2025-03-25 17:52:20,259 - log
        - seq_len : 64 - 2025-03-25 17:52:20,259 - log
        - model_card : gpt2.sm - 2025-03-25 17:52:20,259 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 17:52:20,259 - log
        - fp16 : False - 2025-03-25 17:52:20,259 - log
        - log_interval : 100 - 2025-03-25 17:52:20,259 - log
        - eval_interval : 2000 - 2025-03-25 17:52:20,259 - log
        - save_interval : 1000 - 2025-03-25 17:52:20,259 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 17:52:20,259 - log
        - lora_dim : 4 - 2025-03-25 17:52:20,259 - log
        - lora_alpha : 32 - 2025-03-25 17:52:20,259 - log
        - obj : clm - 2025-03-25 17:52:20,259 - log
        - lora_dropout : 0.1 - 2025-03-25 17:52:20,259 - log
        - label_smooth : 0.1 - 2025-03-25 17:52:20,259 - log
        - roll_interval : -1 - 2025-03-25 17:52:20,259 - log
        - roll_lr : 1e-05 - 2025-03-25 17:52:20,259 - log
        - roll_step : 100 - 2025-03-25 17:52:20,259 - log
        - eval_epoch : 1 - 2025-03-25 17:52:20,259 - log
        - device : cuda - 2025-03-25 17:52:20,259 - log
==================================================================================================== - 2025-03-25 17:52:20,259 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 17:52:20,259 - log
loading model pretrained weight. - 2025-03-25 17:52:20,368 - log
set max_step: 1050 - 2025-03-25 17:52:21,252 - log
start to train the model................ 1 - 2025-03-25 17:52:21,253 - log
==================================================================================================== - 2025-03-25 17:52:39,922 - log
        - random_seed : 2025 - 2025-03-25 17:52:39,925 - log
        - lr : 0.0002 - 2025-03-25 17:52:39,925 - log
        - weight_decay : 0.01 - 2025-03-25 17:52:39,925 - log
        - correct_bias : False - 2025-03-25 17:52:39,925 - log
        - adam_epislon : 1e-06 - 2025-03-25 17:52:39,925 - log
        - no_decay_bias : False - 2025-03-25 17:52:39,925 - log
        - adam_beta1 : 0.9 - 2025-03-25 17:52:39,925 - log
        - adam_beta2 : 0.999 - 2025-03-25 17:52:39,925 - log
        - scheduler : linear - 2025-03-25 17:52:39,925 - log
        - max_step : None - 2025-03-25 17:52:39,925 - log
        - max_epoch : 5 - 2025-03-25 17:52:39,925 - log
        - warmup_step : 500 - 2025-03-25 17:52:39,925 - log
        - i_steps : 0 - 2025-03-25 17:52:39,925 - log
        - i_lrs : 0.00025 - 2025-03-25 17:52:39,925 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 17:52:39,925 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 17:52:39,925 - log
        - train_batch_size : 2 - 2025-03-25 17:52:39,925 - log
        - valid_batch_size : 1 - 2025-03-25 17:52:39,925 - log
        - grad_acc : 2 - 2025-03-25 17:52:39,925 - log
        - clip : 0.0 - 2025-03-25 17:52:39,925 - log
        - seq_len : 64 - 2025-03-25 17:52:39,925 - log
        - model_card : gpt2.sm - 2025-03-25 17:52:39,925 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 17:52:39,925 - log
        - fp16 : False - 2025-03-25 17:52:39,925 - log
        - log_interval : 100 - 2025-03-25 17:52:39,925 - log
        - eval_interval : 2000 - 2025-03-25 17:52:39,925 - log
        - save_interval : 1000 - 2025-03-25 17:52:39,925 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 17:52:39,925 - log
        - lora_dim : 4 - 2025-03-25 17:52:39,925 - log
        - lora_alpha : 32 - 2025-03-25 17:52:39,925 - log
        - obj : clm - 2025-03-25 17:52:39,925 - log
        - lora_dropout : 0.1 - 2025-03-25 17:52:39,926 - log
        - label_smooth : 0.1 - 2025-03-25 17:52:39,926 - log
        - roll_interval : -1 - 2025-03-25 17:52:39,926 - log
        - roll_lr : 1e-05 - 2025-03-25 17:52:39,926 - log
        - roll_step : 100 - 2025-03-25 17:52:39,926 - log
        - eval_epoch : 1 - 2025-03-25 17:52:39,926 - log
        - device : cuda - 2025-03-25 17:52:39,926 - log
==================================================================================================== - 2025-03-25 17:52:39,926 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 17:52:39,926 - log
loading model pretrained weight. - 2025-03-25 17:52:40,033 - log
set max_step: 1050 - 2025-03-25 17:52:41,340 - log
start to train the model................ 1 - 2025-03-25 17:52:41,340 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.11 | loss 15.19 | avg loss 16.53 | ppl 15075339.97 - 2025-03-25 17:52:44,851 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.34 | loss 16.86 | avg loss 16.56 | ppl 15480727.64 - 2025-03-25 17:52:48,085 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 17:52:48,410 - log
start to train the model................ 2 - 2025-03-25 17:52:49,871 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.15 | loss 17.93 | avg loss 16.76 | ppl 19042285.45 - 2025-03-25 17:52:52,786 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.51 | loss 15.73 | avg loss 16.61 | ppl 16292627.90 - 2025-03-25 17:52:56,037 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 17:52:56,688 - log
start to train the model................ 3 - 2025-03-25 17:52:58,139 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.02 | loss 15.37 | avg loss 16.79 | ppl 19553056.13 - 2025-03-25 17:53:00,741 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.91 | loss 16.83 | avg loss 16.25 | ppl 11394537.35 - 2025-03-25 17:53:04,032 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 17:53:05,018 - log
start to train the model................ 4 - 2025-03-25 17:53:06,463 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.85 | loss 16.49 | avg loss 16.73 | ppl 18477914.78 - 2025-03-25 17:53:08,748 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.76 | loss 16.36 | avg loss 15.80 | ppl 7271090.08 - 2025-03-25 17:53:12,024 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 17:53:13,359 - log
start to train the model................ 5 - 2025-03-25 17:53:14,818 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 20.03 | loss 13.68 | avg loss 15.85 | ppl 7671449.01 - 2025-03-25 17:53:16,821 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 33.19 | loss 14.57 | avg loss 15.50 | ppl 5408821.99 - 2025-03-25 17:53:20,140 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 17:53:20,140 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 17:53:21,827 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 17:53:23,555 - log
End of training - 2025-03-25 17:53:23,555 - log
==================================================================================================== - 2025-03-25 17:58:26,358 - log
        - random_seed : 2025 - 2025-03-25 17:58:26,358 - log
        - lr : 0.0002 - 2025-03-25 17:58:26,358 - log
        - weight_decay : 0.01 - 2025-03-25 17:58:26,358 - log
        - correct_bias : False - 2025-03-25 17:58:26,358 - log
        - adam_epislon : 1e-06 - 2025-03-25 17:58:26,358 - log
        - no_decay_bias : False - 2025-03-25 17:58:26,358 - log
        - adam_beta1 : 0.9 - 2025-03-25 17:58:26,358 - log
        - adam_beta2 : 0.999 - 2025-03-25 17:58:26,358 - log
        - scheduler : linear - 2025-03-25 17:58:26,359 - log
        - max_step : None - 2025-03-25 17:58:26,359 - log
        - max_epoch : 5 - 2025-03-25 17:58:26,359 - log
        - warmup_step : 500 - 2025-03-25 17:58:26,359 - log
        - i_steps : 0 - 2025-03-25 17:58:26,359 - log
        - i_lrs : 0.00025 - 2025-03-25 17:58:26,359 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 17:58:26,359 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 17:58:26,359 - log
        - train_batch_size : 2 - 2025-03-25 17:58:26,359 - log
        - valid_batch_size : 1 - 2025-03-25 17:58:26,359 - log
        - grad_acc : 2 - 2025-03-25 17:58:26,359 - log
        - clip : 0.0 - 2025-03-25 17:58:26,359 - log
        - seq_len : 64 - 2025-03-25 17:58:26,359 - log
        - model_card : gpt2.sm - 2025-03-25 17:58:26,359 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 17:58:26,359 - log
        - fp16 : False - 2025-03-25 17:58:26,359 - log
        - log_interval : 100 - 2025-03-25 17:58:26,359 - log
        - eval_interval : 2000 - 2025-03-25 17:58:26,359 - log
        - save_interval : 1000 - 2025-03-25 17:58:26,359 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 17:58:26,359 - log
        - lora_dim : 4 - 2025-03-25 17:58:26,359 - log
        - lora_alpha : 32 - 2025-03-25 17:58:26,359 - log
        - obj : clm - 2025-03-25 17:58:26,359 - log
        - lora_dropout : 0.1 - 2025-03-25 17:58:26,359 - log
        - label_smooth : 0.1 - 2025-03-25 17:58:26,359 - log
        - roll_interval : -1 - 2025-03-25 17:58:26,359 - log
        - roll_lr : 1e-05 - 2025-03-25 17:58:26,359 - log
        - roll_step : 100 - 2025-03-25 17:58:26,359 - log
        - eval_epoch : 1 - 2025-03-25 17:58:26,359 - log
        - device : cuda - 2025-03-25 17:58:26,359 - log
==================================================================================================== - 2025-03-25 17:58:26,359 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 17:58:26,359 - log
loading model pretrained weight. - 2025-03-25 17:58:26,469 - log
set max_step: 1050 - 2025-03-25 17:58:27,915 - log
start to train the model................ 1 - 2025-03-25 17:58:27,915 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 92.66 | loss 15.19 | avg loss 16.53 | ppl 15075500.27 - 2025-03-25 17:58:37,181 - log
==================================================================================================== - 2025-03-25 17:58:45,705 - log
        - random_seed : 2025 - 2025-03-25 17:58:45,706 - log
        - lr : 0.0002 - 2025-03-25 17:58:45,706 - log
        - weight_decay : 0.01 - 2025-03-25 17:58:45,706 - log
        - correct_bias : False - 2025-03-25 17:58:45,706 - log
        - adam_epislon : 1e-06 - 2025-03-25 17:58:45,706 - log
        - no_decay_bias : False - 2025-03-25 17:58:45,706 - log
        - adam_beta1 : 0.9 - 2025-03-25 17:58:45,706 - log
        - adam_beta2 : 0.999 - 2025-03-25 17:58:45,706 - log
        - scheduler : linear - 2025-03-25 17:58:45,706 - log
        - max_step : None - 2025-03-25 17:58:45,706 - log
        - max_epoch : 5 - 2025-03-25 17:58:45,706 - log
        - warmup_step : 500 - 2025-03-25 17:58:45,706 - log
        - i_steps : 0 - 2025-03-25 17:58:45,706 - log
        - i_lrs : 0.00025 - 2025-03-25 17:58:45,706 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 17:58:45,706 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 17:58:45,706 - log
        - train_batch_size : 2 - 2025-03-25 17:58:45,706 - log
        - valid_batch_size : 1 - 2025-03-25 17:58:45,706 - log
        - grad_acc : 2 - 2025-03-25 17:58:45,706 - log
        - clip : 0.0 - 2025-03-25 17:58:45,706 - log
        - seq_len : 64 - 2025-03-25 17:58:45,706 - log
        - model_card : gpt2.sm - 2025-03-25 17:58:45,706 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 17:58:45,706 - log
        - fp16 : False - 2025-03-25 17:58:45,706 - log
        - log_interval : 100 - 2025-03-25 17:58:45,706 - log
        - eval_interval : 2000 - 2025-03-25 17:58:45,706 - log
        - save_interval : 1000 - 2025-03-25 17:58:45,706 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 17:58:45,706 - log
        - lora_dim : 4 - 2025-03-25 17:58:45,706 - log
        - lora_alpha : 32 - 2025-03-25 17:58:45,706 - log
        - obj : clm - 2025-03-25 17:58:45,706 - log
        - lora_dropout : 0.1 - 2025-03-25 17:58:45,706 - log
        - label_smooth : 0.1 - 2025-03-25 17:58:45,706 - log
        - roll_interval : -1 - 2025-03-25 17:58:45,706 - log
        - roll_lr : 1e-05 - 2025-03-25 17:58:45,706 - log
        - roll_step : 100 - 2025-03-25 17:58:45,706 - log
        - eval_epoch : 1 - 2025-03-25 17:58:45,706 - log
        - device : cuda - 2025-03-25 17:58:45,706 - log
==================================================================================================== - 2025-03-25 17:58:45,706 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 17:58:45,706 - log
loading model pretrained weight. - 2025-03-25 17:58:45,817 - log
set max_step: 1050 - 2025-03-25 17:58:46,635 - log
start to train the model................ 1 - 2025-03-25 17:58:46,635 - log
==================================================================================================== - 2025-03-25 17:59:08,692 - log
        - random_seed : 2025 - 2025-03-25 17:59:08,692 - log
        - lr : 0.0002 - 2025-03-25 17:59:08,692 - log
        - weight_decay : 0.01 - 2025-03-25 17:59:08,692 - log
        - correct_bias : False - 2025-03-25 17:59:08,692 - log
        - adam_epislon : 1e-06 - 2025-03-25 17:59:08,692 - log
        - no_decay_bias : False - 2025-03-25 17:59:08,692 - log
        - adam_beta1 : 0.9 - 2025-03-25 17:59:08,692 - log
        - adam_beta2 : 0.999 - 2025-03-25 17:59:08,692 - log
        - scheduler : linear - 2025-03-25 17:59:08,692 - log
        - max_step : None - 2025-03-25 17:59:08,692 - log
        - max_epoch : 5 - 2025-03-25 17:59:08,692 - log
        - warmup_step : 500 - 2025-03-25 17:59:08,692 - log
        - i_steps : 0 - 2025-03-25 17:59:08,692 - log
        - i_lrs : 0.00025 - 2025-03-25 17:59:08,692 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 17:59:08,692 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 17:59:08,692 - log
        - train_batch_size : 2 - 2025-03-25 17:59:08,692 - log
        - valid_batch_size : 1 - 2025-03-25 17:59:08,692 - log
        - grad_acc : 2 - 2025-03-25 17:59:08,692 - log
        - clip : 0.0 - 2025-03-25 17:59:08,692 - log
        - seq_len : 64 - 2025-03-25 17:59:08,692 - log
        - model_card : gpt2.sm - 2025-03-25 17:59:08,692 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 17:59:08,692 - log
        - fp16 : False - 2025-03-25 17:59:08,692 - log
        - log_interval : 100 - 2025-03-25 17:59:08,692 - log
        - eval_interval : 2000 - 2025-03-25 17:59:08,692 - log
        - save_interval : 1000 - 2025-03-25 17:59:08,692 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 17:59:08,692 - log
        - lora_dim : 4 - 2025-03-25 17:59:08,692 - log
        - lora_alpha : 32 - 2025-03-25 17:59:08,692 - log
        - obj : clm - 2025-03-25 17:59:08,692 - log
        - lora_dropout : 0.1 - 2025-03-25 17:59:08,692 - log
        - label_smooth : 0.1 - 2025-03-25 17:59:08,692 - log
        - roll_interval : -1 - 2025-03-25 17:59:08,692 - log
        - roll_lr : 1e-05 - 2025-03-25 17:59:08,692 - log
        - roll_step : 100 - 2025-03-25 17:59:08,692 - log
        - eval_epoch : 1 - 2025-03-25 17:59:08,692 - log
        - device : cuda - 2025-03-25 17:59:08,692 - log
==================================================================================================== - 2025-03-25 17:59:08,692 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 17:59:08,692 - log
loading model pretrained weight. - 2025-03-25 17:59:08,803 - log
set max_step: 1050 - 2025-03-25 17:59:09,617 - log
start to train the model................ 1 - 2025-03-25 17:59:09,617 - log
==================================================================================================== - 2025-03-25 18:01:02,779 - log
        - random_seed : 2025 - 2025-03-25 18:01:02,779 - log
        - lr : 0.0002 - 2025-03-25 18:01:02,779 - log
        - weight_decay : 0.01 - 2025-03-25 18:01:02,779 - log
        - correct_bias : False - 2025-03-25 18:01:02,779 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:01:02,779 - log
        - no_decay_bias : False - 2025-03-25 18:01:02,779 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:01:02,779 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:01:02,779 - log
        - scheduler : linear - 2025-03-25 18:01:02,779 - log
        - max_step : None - 2025-03-25 18:01:02,779 - log
        - max_epoch : 5 - 2025-03-25 18:01:02,779 - log
        - warmup_step : 500 - 2025-03-25 18:01:02,779 - log
        - i_steps : 0 - 2025-03-25 18:01:02,779 - log
        - i_lrs : 0.00025 - 2025-03-25 18:01:02,779 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:01:02,779 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:01:02,779 - log
        - train_batch_size : 2 - 2025-03-25 18:01:02,779 - log
        - valid_batch_size : 1 - 2025-03-25 18:01:02,779 - log
        - grad_acc : 2 - 2025-03-25 18:01:02,779 - log
        - clip : 0.0 - 2025-03-25 18:01:02,780 - log
        - seq_len : 64 - 2025-03-25 18:01:02,780 - log
        - model_card : gpt2.sm - 2025-03-25 18:01:02,780 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:01:02,780 - log
        - fp16 : False - 2025-03-25 18:01:02,780 - log
        - log_interval : 100 - 2025-03-25 18:01:02,780 - log
        - eval_interval : 2000 - 2025-03-25 18:01:02,780 - log
        - save_interval : 1000 - 2025-03-25 18:01:02,780 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:01:02,780 - log
        - lora_dim : 4 - 2025-03-25 18:01:02,780 - log
        - lora_alpha : 32 - 2025-03-25 18:01:02,780 - log
        - obj : clm - 2025-03-25 18:01:02,780 - log
        - lora_dropout : 0.1 - 2025-03-25 18:01:02,780 - log
        - label_smooth : 0.1 - 2025-03-25 18:01:02,780 - log
        - roll_interval : -1 - 2025-03-25 18:01:02,780 - log
        - roll_lr : 1e-05 - 2025-03-25 18:01:02,780 - log
        - roll_step : 100 - 2025-03-25 18:01:02,780 - log
        - eval_epoch : 1 - 2025-03-25 18:01:02,780 - log
        - device : cuda - 2025-03-25 18:01:02,780 - log
==================================================================================================== - 2025-03-25 18:01:02,780 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:01:02,780 - log
loading model pretrained weight. - 2025-03-25 18:01:02,887 - log
set max_step: 1050 - 2025-03-25 18:01:03,683 - log
start to train the model................ 1 - 2025-03-25 18:01:03,684 - log
==================================================================================================== - 2025-03-25 18:04:24,461 - log
        - random_seed : 2025 - 2025-03-25 18:04:24,461 - log
        - lr : 0.0002 - 2025-03-25 18:04:24,461 - log
        - weight_decay : 0.01 - 2025-03-25 18:04:24,461 - log
        - correct_bias : False - 2025-03-25 18:04:24,461 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:04:24,461 - log
        - no_decay_bias : False - 2025-03-25 18:04:24,461 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:04:24,461 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:04:24,461 - log
        - scheduler : linear - 2025-03-25 18:04:24,461 - log
        - max_step : None - 2025-03-25 18:04:24,461 - log
        - max_epoch : 5 - 2025-03-25 18:04:24,461 - log
        - warmup_step : 500 - 2025-03-25 18:04:24,461 - log
        - i_steps : 0 - 2025-03-25 18:04:24,461 - log
        - i_lrs : 0.00025 - 2025-03-25 18:04:24,461 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:04:24,461 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:04:24,461 - log
        - train_batch_size : 2 - 2025-03-25 18:04:24,461 - log
        - valid_batch_size : 1 - 2025-03-25 18:04:24,461 - log
        - grad_acc : 2 - 2025-03-25 18:04:24,461 - log
        - clip : 0.0 - 2025-03-25 18:04:24,461 - log
        - seq_len : 64 - 2025-03-25 18:04:24,461 - log
        - model_card : gpt2.sm - 2025-03-25 18:04:24,461 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:04:24,461 - log
        - fp16 : False - 2025-03-25 18:04:24,461 - log
        - log_interval : 100 - 2025-03-25 18:04:24,461 - log
        - eval_interval : 2000 - 2025-03-25 18:04:24,461 - log
        - save_interval : 1000 - 2025-03-25 18:04:24,461 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:04:24,461 - log
        - lora_dim : 4 - 2025-03-25 18:04:24,461 - log
        - lora_alpha : 32 - 2025-03-25 18:04:24,461 - log
        - obj : clm - 2025-03-25 18:04:24,461 - log
        - lora_dropout : 0.1 - 2025-03-25 18:04:24,461 - log
        - label_smooth : 0.1 - 2025-03-25 18:04:24,461 - log
        - roll_interval : -1 - 2025-03-25 18:04:24,462 - log
        - roll_lr : 1e-05 - 2025-03-25 18:04:24,462 - log
        - roll_step : 100 - 2025-03-25 18:04:24,462 - log
        - eval_epoch : 1 - 2025-03-25 18:04:24,462 - log
        - device : cuda - 2025-03-25 18:04:24,462 - log
==================================================================================================== - 2025-03-25 18:04:24,462 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:04:24,462 - log
loading model pretrained weight. - 2025-03-25 18:04:24,570 - log
==================================================================================================== - 2025-03-25 18:04:38,175 - log
        - random_seed : 2025 - 2025-03-25 18:04:38,175 - log
        - lr : 0.0002 - 2025-03-25 18:04:38,175 - log
        - weight_decay : 0.01 - 2025-03-25 18:04:38,175 - log
        - correct_bias : False - 2025-03-25 18:04:38,175 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:04:38,176 - log
        - no_decay_bias : False - 2025-03-25 18:04:38,176 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:04:38,176 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:04:38,176 - log
        - scheduler : linear - 2025-03-25 18:04:38,176 - log
        - max_step : None - 2025-03-25 18:04:38,176 - log
        - max_epoch : 5 - 2025-03-25 18:04:38,176 - log
        - warmup_step : 500 - 2025-03-25 18:04:38,176 - log
        - i_steps : 0 - 2025-03-25 18:04:38,176 - log
        - i_lrs : 0.00025 - 2025-03-25 18:04:38,176 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:04:38,176 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:04:38,176 - log
        - train_batch_size : 2 - 2025-03-25 18:04:38,176 - log
        - valid_batch_size : 1 - 2025-03-25 18:04:38,176 - log
        - grad_acc : 2 - 2025-03-25 18:04:38,176 - log
        - clip : 0.0 - 2025-03-25 18:04:38,176 - log
        - seq_len : 64 - 2025-03-25 18:04:38,176 - log
        - model_card : gpt2.sm - 2025-03-25 18:04:38,176 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:04:38,176 - log
        - fp16 : False - 2025-03-25 18:04:38,176 - log
        - log_interval : 100 - 2025-03-25 18:04:38,176 - log
        - eval_interval : 2000 - 2025-03-25 18:04:38,176 - log
        - save_interval : 1000 - 2025-03-25 18:04:38,176 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:04:38,176 - log
        - lora_dim : 4 - 2025-03-25 18:04:38,176 - log
        - lora_alpha : 32 - 2025-03-25 18:04:38,176 - log
        - obj : clm - 2025-03-25 18:04:38,176 - log
        - lora_dropout : 0.1 - 2025-03-25 18:04:38,176 - log
        - label_smooth : 0.1 - 2025-03-25 18:04:38,176 - log
        - roll_interval : -1 - 2025-03-25 18:04:38,176 - log
        - roll_lr : 1e-05 - 2025-03-25 18:04:38,176 - log
        - roll_step : 100 - 2025-03-25 18:04:38,176 - log
        - eval_epoch : 1 - 2025-03-25 18:04:38,176 - log
        - device : cuda - 2025-03-25 18:04:38,176 - log
==================================================================================================== - 2025-03-25 18:04:38,176 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:04:38,176 - log
loading model pretrained weight. - 2025-03-25 18:04:38,285 - log
set max_step: 1050 - 2025-03-25 18:04:39,085 - log
start to train the model................ 1 - 2025-03-25 18:04:39,085 - log
==================================================================================================== - 2025-03-25 18:05:13,637 - log
        - random_seed : 2025 - 2025-03-25 18:05:13,637 - log
        - lr : 0.0002 - 2025-03-25 18:05:13,637 - log
        - weight_decay : 0.01 - 2025-03-25 18:05:13,637 - log
        - correct_bias : False - 2025-03-25 18:05:13,637 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:05:13,637 - log
        - no_decay_bias : False - 2025-03-25 18:05:13,637 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:05:13,637 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:05:13,637 - log
        - scheduler : linear - 2025-03-25 18:05:13,637 - log
        - max_step : None - 2025-03-25 18:05:13,637 - log
        - max_epoch : 5 - 2025-03-25 18:05:13,637 - log
        - warmup_step : 500 - 2025-03-25 18:05:13,637 - log
        - i_steps : 0 - 2025-03-25 18:05:13,637 - log
        - i_lrs : 0.00025 - 2025-03-25 18:05:13,637 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:05:13,637 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:05:13,637 - log
        - train_batch_size : 2 - 2025-03-25 18:05:13,637 - log
        - valid_batch_size : 1 - 2025-03-25 18:05:13,637 - log
        - grad_acc : 2 - 2025-03-25 18:05:13,637 - log
        - clip : 0.0 - 2025-03-25 18:05:13,637 - log
        - seq_len : 64 - 2025-03-25 18:05:13,637 - log
        - model_card : gpt2.sm - 2025-03-25 18:05:13,637 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:05:13,637 - log
        - fp16 : False - 2025-03-25 18:05:13,637 - log
        - log_interval : 100 - 2025-03-25 18:05:13,637 - log
        - eval_interval : 2000 - 2025-03-25 18:05:13,637 - log
        - save_interval : 1000 - 2025-03-25 18:05:13,637 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:05:13,637 - log
        - lora_dim : 4 - 2025-03-25 18:05:13,637 - log
        - lora_alpha : 32 - 2025-03-25 18:05:13,637 - log
        - obj : clm - 2025-03-25 18:05:13,637 - log
        - lora_dropout : 0.1 - 2025-03-25 18:05:13,637 - log
        - label_smooth : 0.1 - 2025-03-25 18:05:13,637 - log
        - roll_interval : -1 - 2025-03-25 18:05:13,637 - log
        - roll_lr : 1e-05 - 2025-03-25 18:05:13,637 - log
        - roll_step : 100 - 2025-03-25 18:05:13,637 - log
        - eval_epoch : 1 - 2025-03-25 18:05:13,637 - log
        - device : cuda - 2025-03-25 18:05:13,637 - log
==================================================================================================== - 2025-03-25 18:05:13,637 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:05:13,637 - log
loading model pretrained weight. - 2025-03-25 18:05:13,744 - log
set max_step: 1050 - 2025-03-25 18:05:14,532 - log
start to train the model................ 1 - 2025-03-25 18:05:14,532 - log
==================================================================================================== - 2025-03-25 18:05:46,644 - log
        - random_seed : 2025 - 2025-03-25 18:05:46,644 - log
        - lr : 0.0002 - 2025-03-25 18:05:46,644 - log
        - weight_decay : 0.01 - 2025-03-25 18:05:46,644 - log
        - correct_bias : False - 2025-03-25 18:05:46,644 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:05:46,644 - log
        - no_decay_bias : False - 2025-03-25 18:05:46,644 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:05:46,644 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:05:46,644 - log
        - scheduler : linear - 2025-03-25 18:05:46,644 - log
        - max_step : None - 2025-03-25 18:05:46,644 - log
        - max_epoch : 5 - 2025-03-25 18:05:46,644 - log
        - warmup_step : 500 - 2025-03-25 18:05:46,644 - log
        - i_steps : 0 - 2025-03-25 18:05:46,644 - log
        - i_lrs : 0.00025 - 2025-03-25 18:05:46,644 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:05:46,644 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:05:46,644 - log
        - train_batch_size : 2 - 2025-03-25 18:05:46,644 - log
        - valid_batch_size : 1 - 2025-03-25 18:05:46,644 - log
        - grad_acc : 2 - 2025-03-25 18:05:46,644 - log
        - clip : 0.0 - 2025-03-25 18:05:46,644 - log
        - seq_len : 64 - 2025-03-25 18:05:46,644 - log
        - model_card : gpt2.sm - 2025-03-25 18:05:46,644 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:05:46,644 - log
        - fp16 : False - 2025-03-25 18:05:46,644 - log
        - log_interval : 100 - 2025-03-25 18:05:46,644 - log
        - eval_interval : 2000 - 2025-03-25 18:05:46,644 - log
        - save_interval : 1000 - 2025-03-25 18:05:46,644 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:05:46,644 - log
        - lora_dim : 4 - 2025-03-25 18:05:46,644 - log
        - lora_alpha : 32 - 2025-03-25 18:05:46,644 - log
        - obj : clm - 2025-03-25 18:05:46,644 - log
        - lora_dropout : 0.1 - 2025-03-25 18:05:46,644 - log
        - label_smooth : 0.1 - 2025-03-25 18:05:46,644 - log
        - roll_interval : -1 - 2025-03-25 18:05:46,644 - log
        - roll_lr : 1e-05 - 2025-03-25 18:05:46,644 - log
        - roll_step : 100 - 2025-03-25 18:05:46,644 - log
        - eval_epoch : 1 - 2025-03-25 18:05:46,644 - log
        - device : cuda - 2025-03-25 18:05:46,644 - log
==================================================================================================== - 2025-03-25 18:05:46,644 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:05:46,644 - log
loading model pretrained weight. - 2025-03-25 18:05:46,753 - log
set max_step: 1050 - 2025-03-25 18:05:48,031 - log
start to train the model................ 1 - 2025-03-25 18:05:48,031 - log
==================================================================================================== - 2025-03-25 18:06:31,345 - log
        - random_seed : 2025 - 2025-03-25 18:06:31,345 - log
        - lr : 0.0002 - 2025-03-25 18:06:31,345 - log
        - weight_decay : 0.01 - 2025-03-25 18:06:31,345 - log
        - correct_bias : False - 2025-03-25 18:06:31,345 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:06:31,346 - log
        - no_decay_bias : False - 2025-03-25 18:06:31,346 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:06:31,346 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:06:31,346 - log
        - scheduler : linear - 2025-03-25 18:06:31,346 - log
        - max_step : None - 2025-03-25 18:06:31,346 - log
        - max_epoch : 5 - 2025-03-25 18:06:31,346 - log
        - warmup_step : 500 - 2025-03-25 18:06:31,346 - log
        - i_steps : 0 - 2025-03-25 18:06:31,346 - log
        - i_lrs : 0.00025 - 2025-03-25 18:06:31,346 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:06:31,346 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:06:31,346 - log
        - train_batch_size : 2 - 2025-03-25 18:06:31,346 - log
        - valid_batch_size : 1 - 2025-03-25 18:06:31,346 - log
        - grad_acc : 2 - 2025-03-25 18:06:31,346 - log
        - clip : 0.0 - 2025-03-25 18:06:31,346 - log
        - seq_len : 64 - 2025-03-25 18:06:31,346 - log
        - model_card : gpt2.sm - 2025-03-25 18:06:31,346 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:06:31,346 - log
        - fp16 : False - 2025-03-25 18:06:31,346 - log
        - log_interval : 100 - 2025-03-25 18:06:31,346 - log
        - eval_interval : 2000 - 2025-03-25 18:06:31,346 - log
        - save_interval : 1000 - 2025-03-25 18:06:31,346 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:06:31,346 - log
        - lora_dim : 4 - 2025-03-25 18:06:31,346 - log
        - lora_alpha : 32 - 2025-03-25 18:06:31,346 - log
        - obj : clm - 2025-03-25 18:06:31,346 - log
        - lora_dropout : 0.1 - 2025-03-25 18:06:31,346 - log
        - label_smooth : 0.1 - 2025-03-25 18:06:31,346 - log
        - roll_interval : -1 - 2025-03-25 18:06:31,346 - log
        - roll_lr : 1e-05 - 2025-03-25 18:06:31,346 - log
        - roll_step : 100 - 2025-03-25 18:06:31,346 - log
        - eval_epoch : 1 - 2025-03-25 18:06:31,346 - log
        - device : cuda - 2025-03-25 18:06:31,346 - log
==================================================================================================== - 2025-03-25 18:06:31,346 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:06:31,346 - log
loading model pretrained weight. - 2025-03-25 18:06:31,453 - log
==================================================================================================== - 2025-03-25 18:07:43,963 - log
        - random_seed : 2025 - 2025-03-25 18:07:43,963 - log
        - lr : 0.0002 - 2025-03-25 18:07:43,963 - log
        - weight_decay : 0.01 - 2025-03-25 18:07:43,963 - log
        - correct_bias : False - 2025-03-25 18:07:43,963 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:07:43,963 - log
        - no_decay_bias : False - 2025-03-25 18:07:43,963 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:07:43,963 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:07:43,963 - log
        - scheduler : linear - 2025-03-25 18:07:43,963 - log
        - max_step : None - 2025-03-25 18:07:43,963 - log
        - max_epoch : 5 - 2025-03-25 18:07:43,963 - log
        - warmup_step : 500 - 2025-03-25 18:07:43,963 - log
        - i_steps : 0 - 2025-03-25 18:07:43,963 - log
        - i_lrs : 0.00025 - 2025-03-25 18:07:43,963 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:07:43,963 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:07:43,963 - log
        - train_batch_size : 2 - 2025-03-25 18:07:43,963 - log
        - valid_batch_size : 1 - 2025-03-25 18:07:43,963 - log
        - grad_acc : 2 - 2025-03-25 18:07:43,963 - log
        - clip : 0.0 - 2025-03-25 18:07:43,964 - log
        - seq_len : 64 - 2025-03-25 18:07:43,964 - log
        - model_card : gpt2.sm - 2025-03-25 18:07:43,964 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:07:43,964 - log
        - fp16 : False - 2025-03-25 18:07:43,964 - log
        - log_interval : 100 - 2025-03-25 18:07:43,964 - log
        - eval_interval : 2000 - 2025-03-25 18:07:43,964 - log
        - save_interval : 1000 - 2025-03-25 18:07:43,964 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:07:43,964 - log
        - lora_dim : 4 - 2025-03-25 18:07:43,964 - log
        - lora_alpha : 32 - 2025-03-25 18:07:43,964 - log
        - obj : clm - 2025-03-25 18:07:43,964 - log
        - lora_dropout : 0.1 - 2025-03-25 18:07:43,964 - log
        - label_smooth : 0.1 - 2025-03-25 18:07:43,964 - log
        - roll_interval : -1 - 2025-03-25 18:07:43,964 - log
        - roll_lr : 1e-05 - 2025-03-25 18:07:43,964 - log
        - roll_step : 100 - 2025-03-25 18:07:43,964 - log
        - eval_epoch : 1 - 2025-03-25 18:07:43,964 - log
        - device : cuda - 2025-03-25 18:07:43,964 - log
==================================================================================================== - 2025-03-25 18:07:43,964 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:07:43,964 - log
loading model pretrained weight. - 2025-03-25 18:07:44,072 - log
set max_step: 1050 - 2025-03-25 18:07:44,860 - log
start to train the model................ 1 - 2025-03-25 18:07:44,860 - log
==================================================================================================== - 2025-03-25 18:08:40,282 - log
        - random_seed : 2025 - 2025-03-25 18:08:40,283 - log
        - lr : 0.0002 - 2025-03-25 18:08:40,283 - log
        - weight_decay : 0.01 - 2025-03-25 18:08:40,283 - log
        - correct_bias : False - 2025-03-25 18:08:40,283 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:08:40,283 - log
        - no_decay_bias : False - 2025-03-25 18:08:40,283 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:08:40,283 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:08:40,283 - log
        - scheduler : linear - 2025-03-25 18:08:40,283 - log
        - max_step : None - 2025-03-25 18:08:40,283 - log
        - max_epoch : 5 - 2025-03-25 18:08:40,283 - log
        - warmup_step : 500 - 2025-03-25 18:08:40,283 - log
        - i_steps : 0 - 2025-03-25 18:08:40,283 - log
        - i_lrs : 0.00025 - 2025-03-25 18:08:40,283 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:08:40,283 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:08:40,283 - log
        - train_batch_size : 2 - 2025-03-25 18:08:40,283 - log
        - valid_batch_size : 1 - 2025-03-25 18:08:40,283 - log
        - grad_acc : 2 - 2025-03-25 18:08:40,283 - log
        - clip : 0.0 - 2025-03-25 18:08:40,283 - log
        - seq_len : 64 - 2025-03-25 18:08:40,283 - log
        - model_card : gpt2.sm - 2025-03-25 18:08:40,283 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:08:40,283 - log
        - fp16 : False - 2025-03-25 18:08:40,283 - log
        - log_interval : 100 - 2025-03-25 18:08:40,283 - log
        - eval_interval : 2000 - 2025-03-25 18:08:40,283 - log
        - save_interval : 1000 - 2025-03-25 18:08:40,283 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:08:40,283 - log
        - lora_dim : 4 - 2025-03-25 18:08:40,283 - log
        - lora_alpha : 32 - 2025-03-25 18:08:40,283 - log
        - obj : clm - 2025-03-25 18:08:40,283 - log
        - lora_dropout : 0.1 - 2025-03-25 18:08:40,283 - log
        - label_smooth : 0.1 - 2025-03-25 18:08:40,283 - log
        - roll_interval : -1 - 2025-03-25 18:08:40,283 - log
        - roll_lr : 1e-05 - 2025-03-25 18:08:40,283 - log
        - roll_step : 100 - 2025-03-25 18:08:40,283 - log
        - eval_epoch : 1 - 2025-03-25 18:08:40,283 - log
        - device : cuda - 2025-03-25 18:08:40,283 - log
==================================================================================================== - 2025-03-25 18:08:40,283 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:08:40,283 - log
loading model pretrained weight. - 2025-03-25 18:08:40,390 - log
set max_step: 1050 - 2025-03-25 18:08:41,715 - log
start to train the model................ 1 - 2025-03-25 18:08:41,716 - log
==================================================================================================== - 2025-03-25 18:13:59,660 - log
        - random_seed : 2025 - 2025-03-25 18:13:59,660 - log
        - lr : 0.0002 - 2025-03-25 18:13:59,660 - log
        - weight_decay : 0.01 - 2025-03-25 18:13:59,660 - log
        - correct_bias : False - 2025-03-25 18:13:59,660 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:13:59,660 - log
        - no_decay_bias : False - 2025-03-25 18:13:59,660 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:13:59,660 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:13:59,660 - log
        - scheduler : linear - 2025-03-25 18:13:59,660 - log
        - max_step : None - 2025-03-25 18:13:59,660 - log
        - max_epoch : 5 - 2025-03-25 18:13:59,660 - log
        - warmup_step : 500 - 2025-03-25 18:13:59,660 - log
        - i_steps : 0 - 2025-03-25 18:13:59,660 - log
        - i_lrs : 0.00025 - 2025-03-25 18:13:59,660 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:13:59,660 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:13:59,660 - log
        - train_batch_size : 2 - 2025-03-25 18:13:59,660 - log
        - valid_batch_size : 1 - 2025-03-25 18:13:59,660 - log
        - grad_acc : 2 - 2025-03-25 18:13:59,660 - log
        - clip : 0.0 - 2025-03-25 18:13:59,660 - log
        - seq_len : 64 - 2025-03-25 18:13:59,660 - log
        - model_card : gpt2.sm - 2025-03-25 18:13:59,660 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:13:59,660 - log
        - fp16 : False - 2025-03-25 18:13:59,660 - log
        - log_interval : 100 - 2025-03-25 18:13:59,660 - log
        - eval_interval : 2000 - 2025-03-25 18:13:59,660 - log
        - save_interval : 1000 - 2025-03-25 18:13:59,660 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:13:59,660 - log
        - lora_dim : 4 - 2025-03-25 18:13:59,660 - log
        - lora_alpha : 32 - 2025-03-25 18:13:59,660 - log
        - obj : clm - 2025-03-25 18:13:59,660 - log
        - lora_dropout : 0.1 - 2025-03-25 18:13:59,660 - log
        - label_smooth : 0.1 - 2025-03-25 18:13:59,660 - log
        - roll_interval : -1 - 2025-03-25 18:13:59,660 - log
        - roll_lr : 1e-05 - 2025-03-25 18:13:59,660 - log
        - roll_step : 100 - 2025-03-25 18:13:59,660 - log
        - eval_epoch : 1 - 2025-03-25 18:13:59,660 - log
        - device : cuda - 2025-03-25 18:13:59,660 - log
==================================================================================================== - 2025-03-25 18:13:59,660 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:13:59,660 - log
loading model pretrained weight. - 2025-03-25 18:13:59,769 - log
set max_step: 1050 - 2025-03-25 18:14:00,564 - log
start to train the model................ 1 - 2025-03-25 18:14:00,564 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.12 | loss 15.19 | avg loss 16.53 | ppl 15075351.76 - 2025-03-25 18:14:04,076 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.34 | loss 16.86 | avg loss 16.56 | ppl 15480599.49 - 2025-03-25 18:14:07,310 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 18:14:07,635 - log
start to train the model................ 2 - 2025-03-25 18:14:08,977 - log
==================================================================================================== - 2025-03-25 18:14:38,561 - log
        - random_seed : 2025 - 2025-03-25 18:14:38,562 - log
        - lr : 0.0002 - 2025-03-25 18:14:38,562 - log
        - weight_decay : 0.01 - 2025-03-25 18:14:38,562 - log
        - correct_bias : False - 2025-03-25 18:14:38,562 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:14:38,562 - log
        - no_decay_bias : False - 2025-03-25 18:14:38,562 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:14:38,562 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:14:38,562 - log
        - scheduler : linear - 2025-03-25 18:14:38,562 - log
        - max_step : None - 2025-03-25 18:14:38,562 - log
        - max_epoch : 5 - 2025-03-25 18:14:38,562 - log
        - warmup_step : 500 - 2025-03-25 18:14:38,562 - log
        - i_steps : 0 - 2025-03-25 18:14:38,562 - log
        - i_lrs : 0.00025 - 2025-03-25 18:14:38,562 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:14:38,562 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:14:38,562 - log
        - train_batch_size : 2 - 2025-03-25 18:14:38,562 - log
        - valid_batch_size : 1 - 2025-03-25 18:14:38,562 - log
        - grad_acc : 2 - 2025-03-25 18:14:38,562 - log
        - clip : 0.0 - 2025-03-25 18:14:38,562 - log
        - seq_len : 64 - 2025-03-25 18:14:38,562 - log
        - model_card : gpt2.sm - 2025-03-25 18:14:38,562 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:14:38,562 - log
        - fp16 : False - 2025-03-25 18:14:38,562 - log
        - log_interval : 100 - 2025-03-25 18:14:38,562 - log
        - eval_interval : 2000 - 2025-03-25 18:14:38,562 - log
        - save_interval : 1000 - 2025-03-25 18:14:38,562 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:14:38,562 - log
        - lora_dim : 4 - 2025-03-25 18:14:38,562 - log
        - lora_alpha : 32 - 2025-03-25 18:14:38,562 - log
        - obj : clm - 2025-03-25 18:14:38,562 - log
        - lora_dropout : 0.1 - 2025-03-25 18:14:38,562 - log
        - label_smooth : 0.1 - 2025-03-25 18:14:38,562 - log
        - roll_interval : -1 - 2025-03-25 18:14:38,562 - log
        - roll_lr : 1e-05 - 2025-03-25 18:14:38,562 - log
        - roll_step : 100 - 2025-03-25 18:14:38,562 - log
        - eval_epoch : 1 - 2025-03-25 18:14:38,562 - log
        - device : cuda - 2025-03-25 18:14:38,562 - log
==================================================================================================== - 2025-03-25 18:14:38,562 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:14:38,562 - log
loading model pretrained weight. - 2025-03-25 18:14:38,671 - log
set max_step: 1050 - 2025-03-25 18:14:39,537 - log
start to train the model................ 1 - 2025-03-25 18:14:39,537 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.59 | loss 15.19 | avg loss 16.53 | ppl 15075327.03 - 2025-03-25 18:14:43,096 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.56 | loss 16.86 | avg loss 16.56 | ppl 15480575.72 - 2025-03-25 18:14:46,352 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 18:14:46,675 - log
start to train the model................ 2 - 2025-03-25 18:14:48,140 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.04 | loss 17.93 | avg loss 16.76 | ppl 19042250.54 - 2025-03-25 18:14:51,044 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.35 | loss 15.73 | avg loss 16.61 | ppl 16292607.39 - 2025-03-25 18:14:54,279 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 18:14:54,923 - log
start to train the model................ 3 - 2025-03-25 18:14:56,273 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 25.95 | loss 15.37 | avg loss 16.79 | ppl 19552637.51 - 2025-03-25 18:14:58,868 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.45 | loss 16.84 | avg loss 16.25 | ppl 11373769.54 - 2025-03-25 18:15:02,113 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 18:15:03,088 - log
start to train the model................ 4 - 2025-03-25 18:15:04,546 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 23.06 | loss 16.51 | avg loss 16.74 | ppl 18533324.96 - 2025-03-25 18:15:06,852 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.93 | loss 16.36 | avg loss 15.80 | ppl 7299817.45 - 2025-03-25 18:15:10,146 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 18:15:11,473 - log
start to train the model................ 5 - 2025-03-25 18:15:12,894 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 20.13 | loss 13.73 | avg loss 15.86 | ppl 7734266.42 - 2025-03-25 18:15:14,908 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 33.02 | loss 14.61 | avg loss 15.50 | ppl 5412280.05 - 2025-03-25 18:15:18,209 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 18:15:18,210 - log
==================================================================================================== - 2025-03-25 18:15:28,866 - log
        - random_seed : 2025 - 2025-03-25 18:15:28,866 - log
        - lr : 0.0002 - 2025-03-25 18:15:28,866 - log
        - weight_decay : 0.01 - 2025-03-25 18:15:28,866 - log
        - correct_bias : False - 2025-03-25 18:15:28,866 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:15:28,866 - log
        - no_decay_bias : False - 2025-03-25 18:15:28,866 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:15:28,866 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:15:28,866 - log
        - scheduler : linear - 2025-03-25 18:15:28,866 - log
        - max_step : None - 2025-03-25 18:15:28,866 - log
        - max_epoch : 5 - 2025-03-25 18:15:28,866 - log
        - warmup_step : 500 - 2025-03-25 18:15:28,866 - log
        - i_steps : 0 - 2025-03-25 18:15:28,866 - log
        - i_lrs : 0.00025 - 2025-03-25 18:15:28,866 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:15:28,866 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:15:28,866 - log
        - train_batch_size : 2 - 2025-03-25 18:15:28,866 - log
        - valid_batch_size : 1 - 2025-03-25 18:15:28,866 - log
        - grad_acc : 2 - 2025-03-25 18:15:28,866 - log
        - clip : 0.0 - 2025-03-25 18:15:28,866 - log
        - seq_len : 64 - 2025-03-25 18:15:28,866 - log
        - model_card : gpt2.sm - 2025-03-25 18:15:28,866 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:15:28,866 - log
        - fp16 : False - 2025-03-25 18:15:28,866 - log
        - log_interval : 100 - 2025-03-25 18:15:28,866 - log
        - eval_interval : 2000 - 2025-03-25 18:15:28,866 - log
        - save_interval : 1000 - 2025-03-25 18:15:28,866 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:15:28,866 - log
        - lora_dim : 4 - 2025-03-25 18:15:28,866 - log
        - lora_alpha : 32 - 2025-03-25 18:15:28,866 - log
        - obj : clm - 2025-03-25 18:15:28,866 - log
        - lora_dropout : 0.1 - 2025-03-25 18:15:28,866 - log
        - label_smooth : 0.1 - 2025-03-25 18:15:28,866 - log
        - roll_interval : -1 - 2025-03-25 18:15:28,866 - log
        - roll_lr : 1e-05 - 2025-03-25 18:15:28,866 - log
        - roll_step : 100 - 2025-03-25 18:15:28,866 - log
        - eval_epoch : 1 - 2025-03-25 18:15:28,866 - log
        - device : cuda - 2025-03-25 18:15:28,866 - log
==================================================================================================== - 2025-03-25 18:15:28,866 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:15:28,867 - log
loading model pretrained weight. - 2025-03-25 18:15:28,979 - log
set max_step: 1050 - 2025-03-25 18:15:29,803 - log
start to train the model................ 1 - 2025-03-25 18:15:29,803 - log
==================================================================================================== - 2025-03-25 18:16:45,072 - log
        - random_seed : 2025 - 2025-03-25 18:16:45,072 - log
        - lr : 0.0002 - 2025-03-25 18:16:45,072 - log
        - weight_decay : 0.01 - 2025-03-25 18:16:45,072 - log
        - correct_bias : False - 2025-03-25 18:16:45,072 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:16:45,072 - log
        - no_decay_bias : False - 2025-03-25 18:16:45,072 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:16:45,072 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:16:45,072 - log
        - scheduler : linear - 2025-03-25 18:16:45,072 - log
        - max_step : None - 2025-03-25 18:16:45,072 - log
        - max_epoch : 5 - 2025-03-25 18:16:45,072 - log
        - warmup_step : 500 - 2025-03-25 18:16:45,072 - log
        - i_steps : 0 - 2025-03-25 18:16:45,072 - log
        - i_lrs : 0.00025 - 2025-03-25 18:16:45,072 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:16:45,072 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:16:45,072 - log
        - train_batch_size : 2 - 2025-03-25 18:16:45,072 - log
        - valid_batch_size : 1 - 2025-03-25 18:16:45,072 - log
        - grad_acc : 2 - 2025-03-25 18:16:45,072 - log
        - clip : 0.0 - 2025-03-25 18:16:45,072 - log
        - seq_len : 64 - 2025-03-25 18:16:45,072 - log
        - model_card : gpt2.sm - 2025-03-25 18:16:45,072 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:16:45,072 - log
        - fp16 : False - 2025-03-25 18:16:45,072 - log
        - log_interval : 100 - 2025-03-25 18:16:45,072 - log
        - eval_interval : 2000 - 2025-03-25 18:16:45,072 - log
        - save_interval : 1000 - 2025-03-25 18:16:45,072 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:16:45,072 - log
        - lora_dim : 4 - 2025-03-25 18:16:45,072 - log
        - lora_alpha : 32 - 2025-03-25 18:16:45,072 - log
        - obj : clm - 2025-03-25 18:16:45,072 - log
        - lora_dropout : 0.1 - 2025-03-25 18:16:45,072 - log
        - label_smooth : 0.1 - 2025-03-25 18:16:45,072 - log
        - roll_interval : -1 - 2025-03-25 18:16:45,072 - log
        - roll_lr : 1e-05 - 2025-03-25 18:16:45,072 - log
        - roll_step : 100 - 2025-03-25 18:16:45,072 - log
        - eval_epoch : 1 - 2025-03-25 18:16:45,072 - log
        - device : cuda - 2025-03-25 18:16:45,072 - log
==================================================================================================== - 2025-03-25 18:16:45,072 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:16:45,072 - log
loading model pretrained weight. - 2025-03-25 18:16:45,187 - log
set max_step: 1050 - 2025-03-25 18:16:46,033 - log
start to train the model................ 1 - 2025-03-25 18:16:46,033 - log
==================================================================================================== - 2025-03-25 18:17:46,902 - log
        - random_seed : 2025 - 2025-03-25 18:17:46,902 - log
        - lr : 0.0002 - 2025-03-25 18:17:46,902 - log
        - weight_decay : 0.01 - 2025-03-25 18:17:46,902 - log
        - correct_bias : False - 2025-03-25 18:17:46,902 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:17:46,902 - log
        - no_decay_bias : False - 2025-03-25 18:17:46,902 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:17:46,902 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:17:46,902 - log
        - scheduler : linear - 2025-03-25 18:17:46,902 - log
        - max_step : None - 2025-03-25 18:17:46,902 - log
        - max_epoch : 5 - 2025-03-25 18:17:46,902 - log
        - warmup_step : 500 - 2025-03-25 18:17:46,902 - log
        - i_steps : 0 - 2025-03-25 18:17:46,902 - log
        - i_lrs : 0.00025 - 2025-03-25 18:17:46,902 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:17:46,902 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:17:46,902 - log
        - train_batch_size : 2 - 2025-03-25 18:17:46,902 - log
        - valid_batch_size : 1 - 2025-03-25 18:17:46,902 - log
        - grad_acc : 2 - 2025-03-25 18:17:46,902 - log
        - clip : 0.0 - 2025-03-25 18:17:46,902 - log
        - seq_len : 64 - 2025-03-25 18:17:46,902 - log
        - model_card : gpt2.sm - 2025-03-25 18:17:46,902 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:17:46,902 - log
        - fp16 : False - 2025-03-25 18:17:46,902 - log
        - log_interval : 100 - 2025-03-25 18:17:46,902 - log
        - eval_interval : 2000 - 2025-03-25 18:17:46,902 - log
        - save_interval : 1000 - 2025-03-25 18:17:46,902 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:17:46,902 - log
        - lora_dim : 4 - 2025-03-25 18:17:46,902 - log
        - lora_alpha : 32 - 2025-03-25 18:17:46,902 - log
        - obj : clm - 2025-03-25 18:17:46,902 - log
        - lora_dropout : 0.1 - 2025-03-25 18:17:46,903 - log
        - label_smooth : 0.1 - 2025-03-25 18:17:46,903 - log
        - roll_interval : -1 - 2025-03-25 18:17:46,903 - log
        - roll_lr : 1e-05 - 2025-03-25 18:17:46,903 - log
        - roll_step : 100 - 2025-03-25 18:17:46,903 - log
        - eval_epoch : 1 - 2025-03-25 18:17:46,903 - log
        - device : cuda - 2025-03-25 18:17:46,903 - log
==================================================================================================== - 2025-03-25 18:17:46,903 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:17:46,903 - log
loading model pretrained weight. - 2025-03-25 18:17:47,013 - log
set max_step: 1050 - 2025-03-25 18:17:47,810 - log
start to train the model................ 1 - 2025-03-25 18:17:47,810 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.40 | loss 15.19 | avg loss 16.53 | ppl 15075409.12 - 2025-03-25 18:17:51,351 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.57 | loss 16.86 | avg loss 16.56 | ppl 15480722.03 - 2025-03-25 18:17:54,607 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 18:17:54,934 - log
start to train the model................ 2 - 2025-03-25 18:17:56,318 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.39 | loss 17.93 | avg loss 16.76 | ppl 19042308.25 - 2025-03-25 18:17:59,257 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.60 | loss 15.73 | avg loss 16.61 | ppl 16292618.27 - 2025-03-25 18:18:02,517 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 18:18:03,169 - log
start to train the model................ 3 - 2025-03-25 18:18:04,602 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.11 | loss 15.37 | avg loss 16.79 | ppl 19553105.32 - 2025-03-25 18:18:07,213 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.72 | loss 16.84 | avg loss 16.25 | ppl 11378770.18 - 2025-03-25 18:18:10,485 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 18:18:11,467 - log
start to train the model................ 4 - 2025-03-25 18:18:12,867 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.90 | loss 16.51 | avg loss 16.73 | ppl 18429159.97 - 2025-03-25 18:18:15,157 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 33.12 | loss 16.36 | avg loss 15.81 | ppl 7312700.83 - 2025-03-25 18:18:18,469 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 18:18:19,824 - log
start to train the model................ 5 - 2025-03-25 18:18:21,282 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.92 | loss 13.82 | avg loss 15.88 | ppl 7862593.87 - 2025-03-25 18:18:23,274 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.90 | loss 14.68 | avg loss 15.50 | ppl 5378006.48 - 2025-03-25 18:18:26,564 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 18:18:26,564 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 18:18:28,223 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:18:29,578 - log
End of training - 2025-03-25 18:18:29,578 - log
==================================================================================================== - 2025-03-25 18:19:58,097 - log
        - random_seed : 2025 - 2025-03-25 18:19:58,097 - log
        - lr : 0.0002 - 2025-03-25 18:19:58,097 - log
        - weight_decay : 0.01 - 2025-03-25 18:19:58,097 - log
        - correct_bias : False - 2025-03-25 18:19:58,097 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:19:58,097 - log
        - no_decay_bias : False - 2025-03-25 18:19:58,097 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:19:58,097 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:19:58,097 - log
        - scheduler : linear - 2025-03-25 18:19:58,097 - log
        - max_step : None - 2025-03-25 18:19:58,097 - log
        - max_epoch : 5 - 2025-03-25 18:19:58,097 - log
        - warmup_step : 500 - 2025-03-25 18:19:58,097 - log
        - i_steps : 0 - 2025-03-25 18:19:58,097 - log
        - i_lrs : 0.00025 - 2025-03-25 18:19:58,097 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:19:58,097 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:19:58,097 - log
        - train_batch_size : 2 - 2025-03-25 18:19:58,097 - log
        - valid_batch_size : 1 - 2025-03-25 18:19:58,097 - log
        - grad_acc : 2 - 2025-03-25 18:19:58,097 - log
        - clip : 0.0 - 2025-03-25 18:19:58,097 - log
        - seq_len : 64 - 2025-03-25 18:19:58,097 - log
        - model_card : gpt2.sm - 2025-03-25 18:19:58,097 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:19:58,097 - log
        - fp16 : False - 2025-03-25 18:19:58,097 - log
        - log_interval : 100 - 2025-03-25 18:19:58,097 - log
        - eval_interval : 2000 - 2025-03-25 18:19:58,097 - log
        - save_interval : 1000 - 2025-03-25 18:19:58,097 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:19:58,097 - log
        - lora_dim : 4 - 2025-03-25 18:19:58,097 - log
        - lora_alpha : 32 - 2025-03-25 18:19:58,097 - log
        - obj : clm - 2025-03-25 18:19:58,097 - log
        - lora_dropout : 0.1 - 2025-03-25 18:19:58,097 - log
        - label_smooth : 0.1 - 2025-03-25 18:19:58,097 - log
        - roll_interval : -1 - 2025-03-25 18:19:58,097 - log
        - roll_lr : 1e-05 - 2025-03-25 18:19:58,097 - log
        - roll_step : 100 - 2025-03-25 18:19:58,097 - log
        - eval_epoch : 1 - 2025-03-25 18:19:58,097 - log
        - device : cuda - 2025-03-25 18:19:58,097 - log
==================================================================================================== - 2025-03-25 18:19:58,097 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:19:58,097 - log
loading model pretrained weight. - 2025-03-25 18:19:58,201 - log
set max_step: 1050 - 2025-03-25 18:19:59,211 - log
start to train the model................ 1 - 2025-03-25 18:19:59,211 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.34 | loss 15.19 | avg loss 16.53 | ppl 15075372.18 - 2025-03-25 18:20:02,745 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.39 | loss 16.86 | avg loss 16.56 | ppl 15480666.81 - 2025-03-25 18:20:05,984 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 18:20:06,311 - log
start to train the model................ 2 - 2025-03-25 18:20:07,761 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.28 | loss 17.93 | avg loss 16.76 | ppl 19042168.62 - 2025-03-25 18:20:10,689 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.78 | loss 15.73 | avg loss 16.61 | ppl 16292630.86 - 2025-03-25 18:20:13,967 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 18:20:14,625 - log
start to train the model................ 3 - 2025-03-25 18:20:16,116 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.35 | loss 15.37 | avg loss 16.79 | ppl 19553374.07 - 2025-03-25 18:20:18,751 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 33.09 | loss 16.84 | avg loss 16.25 | ppl 11378703.34 - 2025-03-25 18:20:22,061 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 18:20:23,050 - log
start to train the model................ 4 - 2025-03-25 18:20:24,582 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 23.22 | loss 16.54 | avg loss 16.73 | ppl 18471244.07 - 2025-03-25 18:20:26,905 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 33.29 | loss 16.38 | avg loss 15.81 | ppl 7338099.13 - 2025-03-25 18:20:30,233 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 18:20:31,566 - log
start to train the model................ 5 - 2025-03-25 18:20:33,086 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.94 | loss 13.76 | avg loss 15.86 | ppl 7735049.66 - 2025-03-25 18:20:35,081 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 33.36 | loss 14.58 | avg loss 15.49 | ppl 5342913.10 - 2025-03-25 18:20:38,417 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 18:20:38,417 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 18:20:40,097 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:20:41,570 - log
End of training - 2025-03-25 18:20:41,570 - log
==================================================================================================== - 2025-03-25 18:24:17,777 - log
        - random_seed : 2025 - 2025-03-25 18:24:17,777 - log
        - lr : 0.0002 - 2025-03-25 18:24:17,777 - log
        - weight_decay : 0.01 - 2025-03-25 18:24:17,777 - log
        - correct_bias : False - 2025-03-25 18:24:17,777 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:24:17,777 - log
        - no_decay_bias : False - 2025-03-25 18:24:17,777 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:24:17,777 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:24:17,777 - log
        - scheduler : linear - 2025-03-25 18:24:17,777 - log
        - max_step : None - 2025-03-25 18:24:17,777 - log
        - max_epoch : 5 - 2025-03-25 18:24:17,777 - log
        - warmup_step : 500 - 2025-03-25 18:24:17,777 - log
        - i_steps : 0 - 2025-03-25 18:24:17,777 - log
        - i_lrs : 0.00025 - 2025-03-25 18:24:17,777 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:24:17,777 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:24:17,777 - log
        - train_batch_size : 2 - 2025-03-25 18:24:17,777 - log
        - valid_batch_size : 1 - 2025-03-25 18:24:17,777 - log
        - grad_acc : 2 - 2025-03-25 18:24:17,777 - log
        - clip : 0.0 - 2025-03-25 18:24:17,777 - log
        - seq_len : 64 - 2025-03-25 18:24:17,777 - log
        - model_card : gpt2.sm - 2025-03-25 18:24:17,777 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:24:17,778 - log
        - fp16 : False - 2025-03-25 18:24:17,778 - log
        - log_interval : 100 - 2025-03-25 18:24:17,778 - log
        - eval_interval : 2000 - 2025-03-25 18:24:17,778 - log
        - save_interval : 1000 - 2025-03-25 18:24:17,778 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:24:17,778 - log
        - lora_dim : 4 - 2025-03-25 18:24:17,778 - log
        - lora_alpha : 32 - 2025-03-25 18:24:17,778 - log
        - obj : clm - 2025-03-25 18:24:17,778 - log
        - lora_dropout : 0.1 - 2025-03-25 18:24:17,778 - log
        - label_smooth : 0.1 - 2025-03-25 18:24:17,778 - log
        - roll_interval : -1 - 2025-03-25 18:24:17,778 - log
        - roll_lr : 1e-05 - 2025-03-25 18:24:17,778 - log
        - roll_step : 100 - 2025-03-25 18:24:17,778 - log
        - eval_epoch : 1 - 2025-03-25 18:24:17,778 - log
        - device : cuda - 2025-03-25 18:24:17,778 - log
==================================================================================================== - 2025-03-25 18:24:17,778 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:24:17,778 - log
loading model pretrained weight. - 2025-03-25 18:24:17,886 - log
set max_step: 1050 - 2025-03-25 18:24:18,718 - log
start to train the model................ 1 - 2025-03-25 18:24:18,718 - log
==================================================================================================== - 2025-03-25 18:25:47,463 - log
        - random_seed : 2025 - 2025-03-25 18:25:47,463 - log
        - lr : 0.0002 - 2025-03-25 18:25:47,463 - log
        - weight_decay : 0.01 - 2025-03-25 18:25:47,464 - log
        - correct_bias : False - 2025-03-25 18:25:47,464 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:25:47,464 - log
        - no_decay_bias : False - 2025-03-25 18:25:47,464 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:25:47,464 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:25:47,464 - log
        - scheduler : linear - 2025-03-25 18:25:47,464 - log
        - max_step : None - 2025-03-25 18:25:47,464 - log
        - max_epoch : 5 - 2025-03-25 18:25:47,464 - log
        - warmup_step : 500 - 2025-03-25 18:25:47,464 - log
        - i_steps : 0 - 2025-03-25 18:25:47,464 - log
        - i_lrs : 0.00025 - 2025-03-25 18:25:47,464 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:25:47,464 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:25:47,464 - log
        - train_batch_size : 2 - 2025-03-25 18:25:47,464 - log
        - valid_batch_size : 1 - 2025-03-25 18:25:47,464 - log
        - grad_acc : 2 - 2025-03-25 18:25:47,464 - log
        - clip : 0.0 - 2025-03-25 18:25:47,464 - log
        - seq_len : 64 - 2025-03-25 18:25:47,464 - log
        - model_card : gpt2.sm - 2025-03-25 18:25:47,464 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:25:47,464 - log
        - fp16 : False - 2025-03-25 18:25:47,464 - log
        - log_interval : 100 - 2025-03-25 18:25:47,464 - log
        - eval_interval : 2000 - 2025-03-25 18:25:47,464 - log
        - save_interval : 1000 - 2025-03-25 18:25:47,464 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:25:47,464 - log
        - lora_dim : 4 - 2025-03-25 18:25:47,464 - log
        - lora_alpha : 32 - 2025-03-25 18:25:47,464 - log
        - obj : clm - 2025-03-25 18:25:47,464 - log
        - lora_dropout : 0.1 - 2025-03-25 18:25:47,464 - log
        - label_smooth : 0.1 - 2025-03-25 18:25:47,464 - log
        - roll_interval : -1 - 2025-03-25 18:25:47,464 - log
        - roll_lr : 1e-05 - 2025-03-25 18:25:47,464 - log
        - roll_step : 100 - 2025-03-25 18:25:47,464 - log
        - eval_epoch : 1 - 2025-03-25 18:25:47,464 - log
        - device : cuda - 2025-03-25 18:25:47,464 - log
==================================================================================================== - 2025-03-25 18:25:47,464 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:25:47,464 - log
loading model pretrained weight. - 2025-03-25 18:25:47,574 - log
set max_step: 1050 - 2025-03-25 18:25:48,416 - log
start to train the model................ 1 - 2025-03-25 18:25:48,416 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.89 | loss 15.19 | avg loss 16.53 | ppl 15075379.79 - 2025-03-25 18:25:51,905 - log
==================================================================================================== - 2025-03-25 18:26:06,356 - log
        - random_seed : 2025 - 2025-03-25 18:26:06,356 - log
        - lr : 0.0002 - 2025-03-25 18:26:06,356 - log
        - weight_decay : 0.01 - 2025-03-25 18:26:06,356 - log
        - correct_bias : False - 2025-03-25 18:26:06,356 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:26:06,356 - log
        - no_decay_bias : False - 2025-03-25 18:26:06,356 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:26:06,356 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:26:06,356 - log
        - scheduler : linear - 2025-03-25 18:26:06,356 - log
        - max_step : None - 2025-03-25 18:26:06,356 - log
        - max_epoch : 5 - 2025-03-25 18:26:06,356 - log
        - warmup_step : 500 - 2025-03-25 18:26:06,356 - log
        - i_steps : 0 - 2025-03-25 18:26:06,356 - log
        - i_lrs : 0.00025 - 2025-03-25 18:26:06,356 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:26:06,356 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:26:06,356 - log
        - train_batch_size : 2 - 2025-03-25 18:26:06,356 - log
        - valid_batch_size : 1 - 2025-03-25 18:26:06,356 - log
        - grad_acc : 2 - 2025-03-25 18:26:06,356 - log
        - clip : 0.0 - 2025-03-25 18:26:06,356 - log
        - seq_len : 64 - 2025-03-25 18:26:06,356 - log
        - model_card : gpt2.sm - 2025-03-25 18:26:06,356 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:26:06,356 - log
        - fp16 : False - 2025-03-25 18:26:06,356 - log
        - log_interval : 100 - 2025-03-25 18:26:06,356 - log
        - eval_interval : 2000 - 2025-03-25 18:26:06,356 - log
        - save_interval : 1000 - 2025-03-25 18:26:06,356 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:26:06,356 - log
        - lora_dim : 4 - 2025-03-25 18:26:06,356 - log
        - lora_alpha : 32 - 2025-03-25 18:26:06,356 - log
        - obj : clm - 2025-03-25 18:26:06,356 - log
        - lora_dropout : 0.1 - 2025-03-25 18:26:06,356 - log
        - label_smooth : 0.1 - 2025-03-25 18:26:06,356 - log
        - roll_interval : -1 - 2025-03-25 18:26:06,356 - log
        - roll_lr : 1e-05 - 2025-03-25 18:26:06,356 - log
        - roll_step : 100 - 2025-03-25 18:26:06,356 - log
        - eval_epoch : 1 - 2025-03-25 18:26:06,356 - log
        - device : cuda - 2025-03-25 18:26:06,356 - log
==================================================================================================== - 2025-03-25 18:26:06,356 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:26:06,356 - log
loading model pretrained weight. - 2025-03-25 18:26:06,467 - log
set max_step: 1050 - 2025-03-25 18:26:07,301 - log
start to train the model................ 1 - 2025-03-25 18:26:07,301 - log
==================================================================================================== - 2025-03-25 18:29:01,220 - log
        - random_seed : 2025 - 2025-03-25 18:29:01,220 - log
        - lr : 0.0002 - 2025-03-25 18:29:01,220 - log
        - weight_decay : 0.01 - 2025-03-25 18:29:01,220 - log
        - correct_bias : False - 2025-03-25 18:29:01,220 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:29:01,220 - log
        - no_decay_bias : False - 2025-03-25 18:29:01,220 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:29:01,220 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:29:01,220 - log
        - scheduler : linear - 2025-03-25 18:29:01,220 - log
        - max_step : None - 2025-03-25 18:29:01,220 - log
        - max_epoch : 5 - 2025-03-25 18:29:01,220 - log
        - warmup_step : 500 - 2025-03-25 18:29:01,221 - log
        - i_steps : 0 - 2025-03-25 18:29:01,221 - log
        - i_lrs : 0.00025 - 2025-03-25 18:29:01,221 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:29:01,221 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:29:01,221 - log
        - train_batch_size : 2 - 2025-03-25 18:29:01,221 - log
        - valid_batch_size : 1 - 2025-03-25 18:29:01,221 - log
        - grad_acc : 2 - 2025-03-25 18:29:01,221 - log
        - clip : 0.0 - 2025-03-25 18:29:01,221 - log
        - seq_len : 64 - 2025-03-25 18:29:01,221 - log
        - model_card : gpt2.sm - 2025-03-25 18:29:01,221 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:29:01,221 - log
        - fp16 : False - 2025-03-25 18:29:01,221 - log
        - log_interval : 100 - 2025-03-25 18:29:01,221 - log
        - eval_interval : 2000 - 2025-03-25 18:29:01,221 - log
        - save_interval : 1000 - 2025-03-25 18:29:01,221 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:29:01,221 - log
        - lora_dim : 4 - 2025-03-25 18:29:01,221 - log
        - lora_alpha : 32 - 2025-03-25 18:29:01,221 - log
        - obj : clm - 2025-03-25 18:29:01,221 - log
        - lora_dropout : 0.1 - 2025-03-25 18:29:01,221 - log
        - label_smooth : 0.1 - 2025-03-25 18:29:01,221 - log
        - roll_interval : -1 - 2025-03-25 18:29:01,221 - log
        - roll_lr : 1e-05 - 2025-03-25 18:29:01,221 - log
        - roll_step : 100 - 2025-03-25 18:29:01,221 - log
        - eval_epoch : 1 - 2025-03-25 18:29:01,221 - log
        - device : cuda - 2025-03-25 18:29:01,221 - log
==================================================================================================== - 2025-03-25 18:29:01,221 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:29:01,221 - log
loading model pretrained weight. - 2025-03-25 18:29:01,354 - log
set max_step: 1050 - 2025-03-25 18:29:02,590 - log
start to train the model................ 1 - 2025-03-25 18:29:02,590 - log
==================================================================================================== - 2025-03-25 18:29:42,815 - log
        - random_seed : 2025 - 2025-03-25 18:29:42,816 - log
        - lr : 0.0002 - 2025-03-25 18:29:42,816 - log
        - weight_decay : 0.01 - 2025-03-25 18:29:42,816 - log
        - correct_bias : False - 2025-03-25 18:29:42,816 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:29:42,816 - log
        - no_decay_bias : False - 2025-03-25 18:29:42,816 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:29:42,816 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:29:42,816 - log
        - scheduler : linear - 2025-03-25 18:29:42,816 - log
        - max_step : None - 2025-03-25 18:29:42,816 - log
        - max_epoch : 5 - 2025-03-25 18:29:42,816 - log
        - warmup_step : 500 - 2025-03-25 18:29:42,816 - log
        - i_steps : 0 - 2025-03-25 18:29:42,816 - log
        - i_lrs : 0.00025 - 2025-03-25 18:29:42,816 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:29:42,816 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:29:42,816 - log
        - train_batch_size : 2 - 2025-03-25 18:29:42,816 - log
        - valid_batch_size : 1 - 2025-03-25 18:29:42,816 - log
        - grad_acc : 2 - 2025-03-25 18:29:42,816 - log
        - clip : 0.0 - 2025-03-25 18:29:42,816 - log
        - seq_len : 64 - 2025-03-25 18:29:42,816 - log
        - model_card : gpt2.sm - 2025-03-25 18:29:42,816 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:29:42,816 - log
        - fp16 : False - 2025-03-25 18:29:42,816 - log
        - log_interval : 100 - 2025-03-25 18:29:42,816 - log
        - eval_interval : 2000 - 2025-03-25 18:29:42,816 - log
        - save_interval : 1000 - 2025-03-25 18:29:42,816 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:29:42,816 - log
        - lora_dim : 4 - 2025-03-25 18:29:42,816 - log
        - lora_alpha : 32 - 2025-03-25 18:29:42,816 - log
        - obj : clm - 2025-03-25 18:29:42,816 - log
        - lora_dropout : 0.1 - 2025-03-25 18:29:42,816 - log
        - label_smooth : 0.1 - 2025-03-25 18:29:42,816 - log
        - roll_interval : -1 - 2025-03-25 18:29:42,816 - log
        - roll_lr : 1e-05 - 2025-03-25 18:29:42,816 - log
        - roll_step : 100 - 2025-03-25 18:29:42,816 - log
        - eval_epoch : 1 - 2025-03-25 18:29:42,816 - log
        - device : cuda - 2025-03-25 18:29:42,816 - log
==================================================================================================== - 2025-03-25 18:29:42,816 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:29:42,816 - log
loading model pretrained weight. - 2025-03-25 18:29:42,925 - log
set max_step: 1050 - 2025-03-25 18:29:43,776 - log
start to train the model................ 1 - 2025-03-25 18:29:43,777 - log
==================================================================================================== - 2025-03-25 18:33:12,220 - log
        - random_seed : 2025 - 2025-03-25 18:33:12,220 - log
        - lr : 0.0002 - 2025-03-25 18:33:12,220 - log
        - weight_decay : 0.01 - 2025-03-25 18:33:12,220 - log
        - correct_bias : False - 2025-03-25 18:33:12,220 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:33:12,220 - log
        - no_decay_bias : False - 2025-03-25 18:33:12,220 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:33:12,220 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:33:12,220 - log
        - scheduler : linear - 2025-03-25 18:33:12,220 - log
        - max_step : None - 2025-03-25 18:33:12,220 - log
        - max_epoch : 5 - 2025-03-25 18:33:12,220 - log
        - warmup_step : 500 - 2025-03-25 18:33:12,220 - log
        - i_steps : 0 - 2025-03-25 18:33:12,220 - log
        - i_lrs : 0.00025 - 2025-03-25 18:33:12,220 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:33:12,220 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:33:12,220 - log
        - train_batch_size : 2 - 2025-03-25 18:33:12,220 - log
        - valid_batch_size : 1 - 2025-03-25 18:33:12,220 - log
        - grad_acc : 2 - 2025-03-25 18:33:12,220 - log
        - clip : 0.0 - 2025-03-25 18:33:12,220 - log
        - seq_len : 64 - 2025-03-25 18:33:12,220 - log
        - model_card : gpt2.sm - 2025-03-25 18:33:12,220 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:33:12,220 - log
        - fp16 : False - 2025-03-25 18:33:12,220 - log
        - log_interval : 100 - 2025-03-25 18:33:12,220 - log
        - eval_interval : 2000 - 2025-03-25 18:33:12,220 - log
        - save_interval : 1000 - 2025-03-25 18:33:12,220 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:33:12,220 - log
        - lora_dim : 4 - 2025-03-25 18:33:12,220 - log
        - lora_alpha : 32 - 2025-03-25 18:33:12,220 - log
        - obj : clm - 2025-03-25 18:33:12,220 - log
        - lora_dropout : 0.1 - 2025-03-25 18:33:12,220 - log
        - label_smooth : 0.1 - 2025-03-25 18:33:12,220 - log
        - roll_interval : -1 - 2025-03-25 18:33:12,220 - log
        - roll_lr : 1e-05 - 2025-03-25 18:33:12,220 - log
        - roll_step : 100 - 2025-03-25 18:33:12,220 - log
        - eval_epoch : 1 - 2025-03-25 18:33:12,220 - log
        - device : cuda - 2025-03-25 18:33:12,220 - log
==================================================================================================== - 2025-03-25 18:33:12,220 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:33:12,220 - log
loading model pretrained weight. - 2025-03-25 18:33:12,331 - log
set max_step: 1050 - 2025-03-25 18:33:13,132 - log
start to train the model................ 1 - 2025-03-25 18:33:13,133 - log
==================================================================================================== - 2025-03-25 18:34:05,624 - log
        - random_seed : 2025 - 2025-03-25 18:34:05,624 - log
        - lr : 0.0002 - 2025-03-25 18:34:05,624 - log
        - weight_decay : 0.01 - 2025-03-25 18:34:05,624 - log
        - correct_bias : False - 2025-03-25 18:34:05,624 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:34:05,624 - log
        - no_decay_bias : False - 2025-03-25 18:34:05,624 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:34:05,624 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:34:05,624 - log
        - scheduler : linear - 2025-03-25 18:34:05,624 - log
        - max_step : None - 2025-03-25 18:34:05,624 - log
        - max_epoch : 5 - 2025-03-25 18:34:05,624 - log
        - warmup_step : 500 - 2025-03-25 18:34:05,624 - log
        - i_steps : 0 - 2025-03-25 18:34:05,624 - log
        - i_lrs : 0.00025 - 2025-03-25 18:34:05,624 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:34:05,624 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:34:05,624 - log
        - train_batch_size : 2 - 2025-03-25 18:34:05,624 - log
        - valid_batch_size : 1 - 2025-03-25 18:34:05,624 - log
        - grad_acc : 2 - 2025-03-25 18:34:05,624 - log
        - clip : 0.0 - 2025-03-25 18:34:05,624 - log
        - seq_len : 64 - 2025-03-25 18:34:05,624 - log
        - model_card : gpt2.sm - 2025-03-25 18:34:05,624 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:34:05,624 - log
        - fp16 : False - 2025-03-25 18:34:05,624 - log
        - log_interval : 100 - 2025-03-25 18:34:05,624 - log
        - eval_interval : 2000 - 2025-03-25 18:34:05,624 - log
        - save_interval : 1000 - 2025-03-25 18:34:05,624 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:34:05,624 - log
        - lora_dim : 4 - 2025-03-25 18:34:05,624 - log
        - lora_alpha : 32 - 2025-03-25 18:34:05,624 - log
        - obj : clm - 2025-03-25 18:34:05,624 - log
        - lora_dropout : 0.1 - 2025-03-25 18:34:05,624 - log
        - label_smooth : 0.1 - 2025-03-25 18:34:05,624 - log
        - roll_interval : -1 - 2025-03-25 18:34:05,624 - log
        - roll_lr : 1e-05 - 2025-03-25 18:34:05,624 - log
        - roll_step : 100 - 2025-03-25 18:34:05,624 - log
        - eval_epoch : 1 - 2025-03-25 18:34:05,624 - log
        - device : cuda - 2025-03-25 18:34:05,624 - log
==================================================================================================== - 2025-03-25 18:34:05,624 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:34:05,624 - log
loading model pretrained weight. - 2025-03-25 18:34:05,734 - log
set max_step: 1050 - 2025-03-25 18:34:06,562 - log
start to train the model................ 1 - 2025-03-25 18:34:06,562 - log
==================================================================================================== - 2025-03-25 18:36:24,185 - log
        - random_seed : 2025 - 2025-03-25 18:36:24,185 - log
        - lr : 0.0002 - 2025-03-25 18:36:24,185 - log
        - weight_decay : 0.01 - 2025-03-25 18:36:24,185 - log
        - correct_bias : False - 2025-03-25 18:36:24,185 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:36:24,185 - log
        - no_decay_bias : False - 2025-03-25 18:36:24,185 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:36:24,185 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:36:24,185 - log
        - scheduler : linear - 2025-03-25 18:36:24,185 - log
        - max_step : None - 2025-03-25 18:36:24,185 - log
        - max_epoch : 5 - 2025-03-25 18:36:24,185 - log
        - warmup_step : 500 - 2025-03-25 18:36:24,185 - log
        - i_steps : 0 - 2025-03-25 18:36:24,185 - log
        - i_lrs : 0.00025 - 2025-03-25 18:36:24,185 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:36:24,185 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:36:24,185 - log
        - train_batch_size : 2 - 2025-03-25 18:36:24,185 - log
        - valid_batch_size : 1 - 2025-03-25 18:36:24,185 - log
        - grad_acc : 2 - 2025-03-25 18:36:24,185 - log
        - clip : 0.0 - 2025-03-25 18:36:24,185 - log
        - seq_len : 64 - 2025-03-25 18:36:24,185 - log
        - model_card : gpt2.sm - 2025-03-25 18:36:24,185 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:36:24,185 - log
        - fp16 : False - 2025-03-25 18:36:24,185 - log
        - log_interval : 100 - 2025-03-25 18:36:24,185 - log
        - eval_interval : 2000 - 2025-03-25 18:36:24,185 - log
        - save_interval : 1000 - 2025-03-25 18:36:24,185 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:36:24,185 - log
        - lora_dim : 4 - 2025-03-25 18:36:24,185 - log
        - lora_alpha : 32 - 2025-03-25 18:36:24,185 - log
        - obj : clm - 2025-03-25 18:36:24,185 - log
        - lora_dropout : 0.1 - 2025-03-25 18:36:24,185 - log
        - label_smooth : 0.1 - 2025-03-25 18:36:24,185 - log
        - roll_interval : -1 - 2025-03-25 18:36:24,185 - log
        - roll_lr : 1e-05 - 2025-03-25 18:36:24,185 - log
        - roll_step : 100 - 2025-03-25 18:36:24,185 - log
        - eval_epoch : 1 - 2025-03-25 18:36:24,185 - log
        - device : cuda - 2025-03-25 18:36:24,186 - log
==================================================================================================== - 2025-03-25 18:36:24,186 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:36:24,186 - log
loading model pretrained weight. - 2025-03-25 18:36:24,292 - log
set max_step: 1050 - 2025-03-25 18:36:25,095 - log
start to train the model................ 1 - 2025-03-25 18:36:25,096 - log
==================================================================================================== - 2025-03-25 18:38:38,135 - log
        - random_seed : 2025 - 2025-03-25 18:38:38,135 - log
        - lr : 0.0002 - 2025-03-25 18:38:38,135 - log
        - weight_decay : 0.01 - 2025-03-25 18:38:38,135 - log
        - correct_bias : False - 2025-03-25 18:38:38,135 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:38:38,135 - log
        - no_decay_bias : False - 2025-03-25 18:38:38,135 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:38:38,135 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:38:38,135 - log
        - scheduler : linear - 2025-03-25 18:38:38,135 - log
        - max_step : None - 2025-03-25 18:38:38,135 - log
        - max_epoch : 5 - 2025-03-25 18:38:38,135 - log
        - warmup_step : 500 - 2025-03-25 18:38:38,135 - log
        - i_steps : 0 - 2025-03-25 18:38:38,135 - log
        - i_lrs : 0.00025 - 2025-03-25 18:38:38,135 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:38:38,136 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:38:38,136 - log
        - train_batch_size : 2 - 2025-03-25 18:38:38,136 - log
        - valid_batch_size : 1 - 2025-03-25 18:38:38,136 - log
        - grad_acc : 2 - 2025-03-25 18:38:38,136 - log
        - clip : 0.0 - 2025-03-25 18:38:38,136 - log
        - seq_len : 64 - 2025-03-25 18:38:38,136 - log
        - model_card : gpt2.sm - 2025-03-25 18:38:38,136 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:38:38,136 - log
        - fp16 : False - 2025-03-25 18:38:38,136 - log
        - log_interval : 100 - 2025-03-25 18:38:38,136 - log
        - eval_interval : 2000 - 2025-03-25 18:38:38,136 - log
        - save_interval : 1000 - 2025-03-25 18:38:38,136 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:38:38,136 - log
        - lora_dim : 4 - 2025-03-25 18:38:38,136 - log
        - lora_alpha : 32 - 2025-03-25 18:38:38,136 - log
        - obj : clm - 2025-03-25 18:38:38,136 - log
        - lora_dropout : 0.1 - 2025-03-25 18:38:38,136 - log
        - label_smooth : 0.1 - 2025-03-25 18:38:38,136 - log
        - roll_interval : -1 - 2025-03-25 18:38:38,136 - log
        - roll_lr : 1e-05 - 2025-03-25 18:38:38,136 - log
        - roll_step : 100 - 2025-03-25 18:38:38,136 - log
        - eval_epoch : 1 - 2025-03-25 18:38:38,136 - log
        - device : cuda - 2025-03-25 18:38:38,136 - log
==================================================================================================== - 2025-03-25 18:38:38,136 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:38:38,136 - log
loading model pretrained weight. - 2025-03-25 18:38:38,243 - log
set max_step: 1050 - 2025-03-25 18:38:39,041 - log
start to train the model................ 1 - 2025-03-25 18:38:39,041 - log
==================================================================================================== - 2025-03-25 18:40:39,196 - log
        - random_seed : 2025 - 2025-03-25 18:40:39,196 - log
        - lr : 0.0002 - 2025-03-25 18:40:39,196 - log
        - weight_decay : 0.01 - 2025-03-25 18:40:39,197 - log
        - correct_bias : False - 2025-03-25 18:40:39,197 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:40:39,197 - log
        - no_decay_bias : False - 2025-03-25 18:40:39,197 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:40:39,197 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:40:39,197 - log
        - scheduler : linear - 2025-03-25 18:40:39,197 - log
        - max_step : None - 2025-03-25 18:40:39,197 - log
        - max_epoch : 5 - 2025-03-25 18:40:39,197 - log
        - warmup_step : 500 - 2025-03-25 18:40:39,197 - log
        - i_steps : 0 - 2025-03-25 18:40:39,197 - log
        - i_lrs : 0.00025 - 2025-03-25 18:40:39,197 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:40:39,197 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:40:39,197 - log
        - train_batch_size : 2 - 2025-03-25 18:40:39,197 - log
        - valid_batch_size : 1 - 2025-03-25 18:40:39,197 - log
        - grad_acc : 2 - 2025-03-25 18:40:39,197 - log
        - clip : 0.0 - 2025-03-25 18:40:39,197 - log
        - seq_len : 64 - 2025-03-25 18:40:39,197 - log
        - model_card : gpt2.sm - 2025-03-25 18:40:39,197 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:40:39,197 - log
        - fp16 : False - 2025-03-25 18:40:39,197 - log
        - log_interval : 100 - 2025-03-25 18:40:39,197 - log
        - eval_interval : 2000 - 2025-03-25 18:40:39,197 - log
        - save_interval : 1000 - 2025-03-25 18:40:39,197 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:40:39,197 - log
        - lora_dim : 4 - 2025-03-25 18:40:39,197 - log
        - lora_alpha : 32 - 2025-03-25 18:40:39,197 - log
        - obj : clm - 2025-03-25 18:40:39,197 - log
        - lora_dropout : 0.1 - 2025-03-25 18:40:39,197 - log
        - label_smooth : 0.1 - 2025-03-25 18:40:39,197 - log
        - roll_interval : -1 - 2025-03-25 18:40:39,197 - log
        - roll_lr : 1e-05 - 2025-03-25 18:40:39,197 - log
        - roll_step : 100 - 2025-03-25 18:40:39,197 - log
        - eval_epoch : 1 - 2025-03-25 18:40:39,197 - log
        - device : cuda - 2025-03-25 18:40:39,197 - log
==================================================================================================== - 2025-03-25 18:40:39,197 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:40:39,197 - log
loading model pretrained weight. - 2025-03-25 18:40:39,304 - log
set max_step: 1050 - 2025-03-25 18:40:40,138 - log
start to train the model................ 1 - 2025-03-25 18:40:40,138 - log
==================================================================================================== - 2025-03-25 18:42:03,598 - log
        - random_seed : 2025 - 2025-03-25 18:42:03,599 - log
        - lr : 0.0002 - 2025-03-25 18:42:03,599 - log
        - weight_decay : 0.01 - 2025-03-25 18:42:03,599 - log
        - correct_bias : False - 2025-03-25 18:42:03,599 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:42:03,599 - log
        - no_decay_bias : False - 2025-03-25 18:42:03,599 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:42:03,599 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:42:03,599 - log
        - scheduler : linear - 2025-03-25 18:42:03,599 - log
        - max_step : None - 2025-03-25 18:42:03,599 - log
        - max_epoch : 5 - 2025-03-25 18:42:03,599 - log
        - warmup_step : 500 - 2025-03-25 18:42:03,599 - log
        - i_steps : 0 - 2025-03-25 18:42:03,599 - log
        - i_lrs : 0.00025 - 2025-03-25 18:42:03,599 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:42:03,599 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:42:03,599 - log
        - train_batch_size : 2 - 2025-03-25 18:42:03,599 - log
        - valid_batch_size : 1 - 2025-03-25 18:42:03,599 - log
        - grad_acc : 2 - 2025-03-25 18:42:03,599 - log
        - clip : 0.0 - 2025-03-25 18:42:03,599 - log
        - seq_len : 64 - 2025-03-25 18:42:03,599 - log
        - model_card : gpt2.sm - 2025-03-25 18:42:03,599 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:42:03,599 - log
        - fp16 : False - 2025-03-25 18:42:03,599 - log
        - log_interval : 100 - 2025-03-25 18:42:03,599 - log
        - eval_interval : 2000 - 2025-03-25 18:42:03,599 - log
        - save_interval : 1000 - 2025-03-25 18:42:03,599 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:42:03,599 - log
        - lora_dim : 4 - 2025-03-25 18:42:03,599 - log
        - lora_alpha : 32 - 2025-03-25 18:42:03,599 - log
        - obj : clm - 2025-03-25 18:42:03,599 - log
        - lora_dropout : 0.1 - 2025-03-25 18:42:03,599 - log
        - label_smooth : 0.1 - 2025-03-25 18:42:03,599 - log
        - roll_interval : -1 - 2025-03-25 18:42:03,599 - log
        - roll_lr : 1e-05 - 2025-03-25 18:42:03,599 - log
        - roll_step : 100 - 2025-03-25 18:42:03,599 - log
        - eval_epoch : 1 - 2025-03-25 18:42:03,599 - log
        - device : cuda - 2025-03-25 18:42:03,599 - log
==================================================================================================== - 2025-03-25 18:42:03,599 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:42:03,599 - log
loading model pretrained weight. - 2025-03-25 18:42:03,706 - log
set max_step: 1050 - 2025-03-25 18:42:04,529 - log
start to train the model................ 1 - 2025-03-25 18:42:04,529 - log
==================================================================================================== - 2025-03-25 18:44:45,524 - log
        - random_seed : 2025 - 2025-03-25 18:44:45,524 - log
        - lr : 0.0002 - 2025-03-25 18:44:45,524 - log
        - weight_decay : 0.01 - 2025-03-25 18:44:45,524 - log
        - correct_bias : False - 2025-03-25 18:44:45,524 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:44:45,524 - log
        - no_decay_bias : False - 2025-03-25 18:44:45,524 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:44:45,524 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:44:45,524 - log
        - scheduler : linear - 2025-03-25 18:44:45,524 - log
        - max_step : None - 2025-03-25 18:44:45,524 - log
        - max_epoch : 5 - 2025-03-25 18:44:45,524 - log
        - warmup_step : 500 - 2025-03-25 18:44:45,524 - log
        - i_steps : 0 - 2025-03-25 18:44:45,524 - log
        - i_lrs : 0.00025 - 2025-03-25 18:44:45,524 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:44:45,524 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:44:45,524 - log
        - train_batch_size : 2 - 2025-03-25 18:44:45,524 - log
        - valid_batch_size : 1 - 2025-03-25 18:44:45,524 - log
        - grad_acc : 2 - 2025-03-25 18:44:45,524 - log
        - clip : 0.0 - 2025-03-25 18:44:45,524 - log
        - seq_len : 64 - 2025-03-25 18:44:45,524 - log
        - model_card : gpt2.sm - 2025-03-25 18:44:45,524 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:44:45,524 - log
        - fp16 : False - 2025-03-25 18:44:45,524 - log
        - log_interval : 100 - 2025-03-25 18:44:45,524 - log
        - eval_interval : 2000 - 2025-03-25 18:44:45,524 - log
        - save_interval : 1000 - 2025-03-25 18:44:45,524 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:44:45,524 - log
        - lora_dim : 4 - 2025-03-25 18:44:45,524 - log
        - lora_alpha : 32 - 2025-03-25 18:44:45,524 - log
        - obj : clm - 2025-03-25 18:44:45,524 - log
        - lora_dropout : 0.1 - 2025-03-25 18:44:45,524 - log
        - label_smooth : 0.1 - 2025-03-25 18:44:45,524 - log
        - roll_interval : -1 - 2025-03-25 18:44:45,524 - log
        - roll_lr : 1e-05 - 2025-03-25 18:44:45,524 - log
        - roll_step : 100 - 2025-03-25 18:44:45,524 - log
        - eval_epoch : 1 - 2025-03-25 18:44:45,524 - log
        - device : cuda - 2025-03-25 18:44:45,524 - log
==================================================================================================== - 2025-03-25 18:44:45,524 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:44:45,524 - log
loading model pretrained weight. - 2025-03-25 18:44:45,633 - log
set max_step: 1050 - 2025-03-25 18:44:46,475 - log
start to train the model................ 1 - 2025-03-25 18:44:46,475 - log
==================================================================================================== - 2025-03-25 18:50:27,104 - log
        - random_seed : 2025 - 2025-03-25 18:50:27,105 - log
        - lr : 0.0002 - 2025-03-25 18:50:27,105 - log
        - weight_decay : 0.01 - 2025-03-25 18:50:27,105 - log
        - correct_bias : False - 2025-03-25 18:50:27,105 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:50:27,105 - log
        - no_decay_bias : False - 2025-03-25 18:50:27,105 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:50:27,105 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:50:27,105 - log
        - scheduler : linear - 2025-03-25 18:50:27,105 - log
        - max_step : None - 2025-03-25 18:50:27,105 - log
        - max_epoch : 5 - 2025-03-25 18:50:27,105 - log
        - warmup_step : 500 - 2025-03-25 18:50:27,105 - log
        - i_steps : 0 - 2025-03-25 18:50:27,105 - log
        - i_lrs : 0.00025 - 2025-03-25 18:50:27,105 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:50:27,105 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:50:27,105 - log
        - train_batch_size : 2 - 2025-03-25 18:50:27,105 - log
        - valid_batch_size : 1 - 2025-03-25 18:50:27,105 - log
        - grad_acc : 2 - 2025-03-25 18:50:27,105 - log
        - clip : 0.0 - 2025-03-25 18:50:27,105 - log
        - seq_len : 64 - 2025-03-25 18:50:27,105 - log
        - model_card : gpt2.sm - 2025-03-25 18:50:27,105 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:50:27,105 - log
        - fp16 : False - 2025-03-25 18:50:27,105 - log
        - log_interval : 100 - 2025-03-25 18:50:27,105 - log
        - eval_interval : 2000 - 2025-03-25 18:50:27,105 - log
        - save_interval : 1000 - 2025-03-25 18:50:27,105 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:50:27,105 - log
        - lora_dim : 4 - 2025-03-25 18:50:27,105 - log
        - lora_alpha : 32 - 2025-03-25 18:50:27,105 - log
        - obj : clm - 2025-03-25 18:50:27,105 - log
        - lora_dropout : 0.1 - 2025-03-25 18:50:27,105 - log
        - label_smooth : 0.1 - 2025-03-25 18:50:27,105 - log
        - roll_interval : -1 - 2025-03-25 18:50:27,105 - log
        - roll_lr : 1e-05 - 2025-03-25 18:50:27,105 - log
        - roll_step : 100 - 2025-03-25 18:50:27,105 - log
        - eval_epoch : 1 - 2025-03-25 18:50:27,105 - log
        - device : cuda - 2025-03-25 18:50:27,105 - log
==================================================================================================== - 2025-03-25 18:50:27,105 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:50:27,105 - log
loading model pretrained weight. - 2025-03-25 18:50:27,214 - log
set max_step: 1050 - 2025-03-25 18:50:28,039 - log
start to train the model................ 1 - 2025-03-25 18:50:28,039 - log
==================================================================================================== - 2025-03-25 19:11:50,588 - log
        - random_seed : 2025 - 2025-03-25 19:11:50,588 - log
        - lr : 0.0002 - 2025-03-25 19:11:50,588 - log
        - weight_decay : 0.01 - 2025-03-25 19:11:50,588 - log
        - correct_bias : False - 2025-03-25 19:11:50,588 - log
        - adam_epislon : 1e-06 - 2025-03-25 19:11:50,588 - log
        - no_decay_bias : False - 2025-03-25 19:11:50,588 - log
        - adam_beta1 : 0.9 - 2025-03-25 19:11:50,588 - log
        - adam_beta2 : 0.999 - 2025-03-25 19:11:50,588 - log
        - scheduler : linear - 2025-03-25 19:11:50,588 - log
        - max_step : None - 2025-03-25 19:11:50,588 - log
        - max_epoch : 5 - 2025-03-25 19:11:50,588 - log
        - warmup_step : 500 - 2025-03-25 19:11:50,588 - log
        - i_steps : 0 - 2025-03-25 19:11:50,588 - log
        - i_lrs : 0.00025 - 2025-03-25 19:11:50,588 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 19:11:50,588 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 19:11:50,588 - log
        - train_batch_size : 2 - 2025-03-25 19:11:50,588 - log
        - valid_batch_size : 1 - 2025-03-25 19:11:50,588 - log
        - grad_acc : 2 - 2025-03-25 19:11:50,588 - log
        - clip : 0.0 - 2025-03-25 19:11:50,588 - log
        - seq_len : 64 - 2025-03-25 19:11:50,588 - log
        - model_card : gpt2.sm - 2025-03-25 19:11:50,588 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 19:11:50,588 - log
        - fp16 : False - 2025-03-25 19:11:50,588 - log
        - log_interval : 100 - 2025-03-25 19:11:50,588 - log
        - eval_interval : 2000 - 2025-03-25 19:11:50,588 - log
        - save_interval : 1000 - 2025-03-25 19:11:50,588 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 19:11:50,588 - log
        - lora_dim : 4 - 2025-03-25 19:11:50,588 - log
        - lora_alpha : 32 - 2025-03-25 19:11:50,588 - log
        - obj : clm - 2025-03-25 19:11:50,588 - log
        - lora_dropout : 0.1 - 2025-03-25 19:11:50,588 - log
        - label_smooth : 0.1 - 2025-03-25 19:11:50,588 - log
        - roll_interval : -1 - 2025-03-25 19:11:50,588 - log
        - roll_lr : 1e-05 - 2025-03-25 19:11:50,588 - log
        - roll_step : 100 - 2025-03-25 19:11:50,588 - log
        - eval_epoch : 1 - 2025-03-25 19:11:50,588 - log
        - device : cuda - 2025-03-25 19:11:50,588 - log
==================================================================================================== - 2025-03-25 19:11:50,588 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 19:11:50,588 - log
loading model pretrained weight. - 2025-03-25 19:11:50,697 - log
set max_step: 1050 - 2025-03-25 19:11:51,528 - log
start to train the model................ 1 - 2025-03-25 19:11:51,528 - log
==================================================================================================== - 2025-03-25 19:36:21,097 - log
        - random_seed : 2025 - 2025-03-25 19:36:21,097 - log
        - lr : 0.0002 - 2025-03-25 19:36:21,097 - log
        - weight_decay : 0.01 - 2025-03-25 19:36:21,097 - log
        - correct_bias : False - 2025-03-25 19:36:21,097 - log
        - adam_epislon : 1e-06 - 2025-03-25 19:36:21,097 - log
        - no_decay_bias : False - 2025-03-25 19:36:21,097 - log
        - adam_beta1 : 0.9 - 2025-03-25 19:36:21,097 - log
        - adam_beta2 : 0.999 - 2025-03-25 19:36:21,097 - log
        - scheduler : linear - 2025-03-25 19:36:21,097 - log
        - max_step : None - 2025-03-25 19:36:21,097 - log
        - max_epoch : 5 - 2025-03-25 19:36:21,097 - log
        - warmup_step : 500 - 2025-03-25 19:36:21,097 - log
        - i_steps : 0 - 2025-03-25 19:36:21,097 - log
        - i_lrs : 0.00025 - 2025-03-25 19:36:21,097 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 19:36:21,097 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 19:36:21,097 - log
        - train_batch_size : 2 - 2025-03-25 19:36:21,097 - log
        - valid_batch_size : 1 - 2025-03-25 19:36:21,098 - log
        - grad_acc : 2 - 2025-03-25 19:36:21,098 - log
        - clip : 0.0 - 2025-03-25 19:36:21,098 - log
        - seq_len : 64 - 2025-03-25 19:36:21,098 - log
        - model_card : gpt2.sm - 2025-03-25 19:36:21,098 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 19:36:21,098 - log
        - fp16 : False - 2025-03-25 19:36:21,098 - log
        - log_interval : 100 - 2025-03-25 19:36:21,098 - log
        - eval_interval : 2000 - 2025-03-25 19:36:21,098 - log
        - save_interval : 1000 - 2025-03-25 19:36:21,098 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 19:36:21,098 - log
        - lora_dim : 4 - 2025-03-25 19:36:21,098 - log
        - lora_alpha : 32 - 2025-03-25 19:36:21,098 - log
        - obj : clm - 2025-03-25 19:36:21,098 - log
        - lora_dropout : 0.1 - 2025-03-25 19:36:21,098 - log
        - label_smooth : 0.1 - 2025-03-25 19:36:21,098 - log
        - roll_interval : -1 - 2025-03-25 19:36:21,098 - log
        - roll_lr : 1e-05 - 2025-03-25 19:36:21,098 - log
        - roll_step : 100 - 2025-03-25 19:36:21,098 - log
        - eval_epoch : 1 - 2025-03-25 19:36:21,098 - log
        - device : cuda - 2025-03-25 19:36:21,098 - log
==================================================================================================== - 2025-03-25 19:36:21,098 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 19:36:21,098 - log
loading model pretrained weight. - 2025-03-25 19:36:21,208 - log
set max_step: 1050 - 2025-03-25 19:36:22,217 - log
start to train the model................ 1 - 2025-03-25 19:36:22,217 - log
==================================================================================================== - 2025-03-25 19:36:25,849 - log
        - random_seed : 2025 - 2025-03-25 19:36:25,849 - log
        - lr : 0.0002 - 2025-03-25 19:36:25,849 - log
        - weight_decay : 0.01 - 2025-03-25 19:36:25,849 - log
        - correct_bias : False - 2025-03-25 19:36:25,849 - log
        - adam_epislon : 1e-06 - 2025-03-25 19:36:25,849 - log
        - no_decay_bias : False - 2025-03-25 19:36:25,849 - log
        - adam_beta1 : 0.9 - 2025-03-25 19:36:25,849 - log
        - adam_beta2 : 0.999 - 2025-03-25 19:36:25,850 - log
        - scheduler : linear - 2025-03-25 19:36:25,850 - log
        - max_step : None - 2025-03-25 19:36:25,850 - log
        - max_epoch : 5 - 2025-03-25 19:36:25,850 - log
        - warmup_step : 500 - 2025-03-25 19:36:25,850 - log
        - i_steps : 0 - 2025-03-25 19:36:25,850 - log
        - i_lrs : 0.00025 - 2025-03-25 19:36:25,850 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 19:36:25,850 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 19:36:25,850 - log
        - train_batch_size : 2 - 2025-03-25 19:36:25,850 - log
        - valid_batch_size : 1 - 2025-03-25 19:36:25,850 - log
        - grad_acc : 2 - 2025-03-25 19:36:25,850 - log
        - clip : 0.0 - 2025-03-25 19:36:25,850 - log
        - seq_len : 64 - 2025-03-25 19:36:25,850 - log
        - model_card : gpt2.sm - 2025-03-25 19:36:25,850 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 19:36:25,850 - log
        - fp16 : False - 2025-03-25 19:36:25,850 - log
        - log_interval : 100 - 2025-03-25 19:36:25,850 - log
        - eval_interval : 2000 - 2025-03-25 19:36:25,850 - log
        - save_interval : 1000 - 2025-03-25 19:36:25,850 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 19:36:25,850 - log
        - lora_dim : 4 - 2025-03-25 19:36:25,850 - log
        - lora_alpha : 32 - 2025-03-25 19:36:25,850 - log
        - obj : clm - 2025-03-25 19:36:25,850 - log
        - lora_dropout : 0.1 - 2025-03-25 19:36:25,850 - log
        - label_smooth : 0.1 - 2025-03-25 19:36:25,850 - log
        - roll_interval : -1 - 2025-03-25 19:36:25,850 - log
        - roll_lr : 1e-05 - 2025-03-25 19:36:25,850 - log
        - roll_step : 100 - 2025-03-25 19:36:25,850 - log
        - eval_epoch : 1 - 2025-03-25 19:36:25,850 - log
        - device : cuda - 2025-03-25 19:36:25,850 - log
==================================================================================================== - 2025-03-25 19:36:25,850 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 19:36:25,850 - log
loading model pretrained weight. - 2025-03-25 19:36:25,959 - log
set max_step: 1050 - 2025-03-25 19:36:26,903 - log
start to train the model................ 1 - 2025-03-25 19:36:26,903 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.16 | loss 15.19 | avg loss 16.53 | ppl 15075340.83 - 2025-03-25 19:36:30,420 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.54 | loss 16.86 | avg loss 16.56 | ppl 15480710.36 - 2025-03-25 19:36:33,673 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 19:36:34,000 - log
start to train the model................ 2 - 2025-03-25 19:36:35,358 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.16 | loss 17.93 | avg loss 16.76 | ppl 19042240.25 - 2025-03-25 19:36:38,274 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.73 | loss 15.73 | avg loss 16.61 | ppl 16292583.93 - 2025-03-25 19:36:41,547 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 19:36:42,204 - log
start to train the model................ 3 - 2025-03-25 19:36:43,610 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.31 | loss 15.37 | avg loss 16.79 | ppl 19553872.20 - 2025-03-25 19:36:46,241 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.88 | loss 16.84 | avg loss 16.25 | ppl 11378235.32 - 2025-03-25 19:36:49,529 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 19:36:50,514 - log
start to train the model................ 4 - 2025-03-25 19:36:51,912 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 23.21 | loss 16.52 | avg loss 16.73 | ppl 18492685.07 - 2025-03-25 19:36:54,233 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 33.80 | loss 16.36 | avg loss 15.80 | ppl 7302543.45 - 2025-03-25 19:36:57,614 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 19:36:58,944 - log
start to train the model................ 5 - 2025-03-25 19:37:00,403 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.89 | loss 13.77 | avg loss 15.86 | ppl 7721180.48 - 2025-03-25 19:37:02,393 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 33.48 | loss 14.59 | avg loss 15.50 | ppl 5385888.45 - 2025-03-25 19:37:05,741 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 19:37:05,741 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 19:37:07,403 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 19:37:08,769 - log
End of training - 2025-03-25 19:37:08,769 - log
==================================================================================================== - 2025-03-25 19:42:18,705 - log
        - random_seed : 2025 - 2025-03-25 19:42:18,705 - log
        - lr : 0.0002 - 2025-03-25 19:42:18,705 - log
        - weight_decay : 0.01 - 2025-03-25 19:42:18,705 - log
        - correct_bias : False - 2025-03-25 19:42:18,705 - log
        - adam_epislon : 1e-06 - 2025-03-25 19:42:18,705 - log
        - no_decay_bias : False - 2025-03-25 19:42:18,705 - log
        - adam_beta1 : 0.9 - 2025-03-25 19:42:18,705 - log
        - adam_beta2 : 0.999 - 2025-03-25 19:42:18,705 - log
        - scheduler : linear - 2025-03-25 19:42:18,705 - log
        - max_step : None - 2025-03-25 19:42:18,705 - log
        - max_epoch : 5 - 2025-03-25 19:42:18,705 - log
        - warmup_step : 500 - 2025-03-25 19:42:18,705 - log
        - i_steps : 0 - 2025-03-25 19:42:18,705 - log
        - i_lrs : 0.00025 - 2025-03-25 19:42:18,705 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 19:42:18,705 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 19:42:18,705 - log
        - train_batch_size : 2 - 2025-03-25 19:42:18,705 - log
        - valid_batch_size : 1 - 2025-03-25 19:42:18,705 - log
        - grad_acc : 2 - 2025-03-25 19:42:18,705 - log
        - clip : 0.0 - 2025-03-25 19:42:18,705 - log
        - seq_len : 64 - 2025-03-25 19:42:18,705 - log
        - model_card : gpt2.sm - 2025-03-25 19:42:18,705 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 19:42:18,705 - log
        - fp16 : False - 2025-03-25 19:42:18,705 - log
        - log_interval : 100 - 2025-03-25 19:42:18,705 - log
        - eval_interval : 2000 - 2025-03-25 19:42:18,705 - log
        - save_interval : 1000 - 2025-03-25 19:42:18,705 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 19:42:18,705 - log
        - lora_dim : 4 - 2025-03-25 19:42:18,705 - log
        - lora_alpha : 32 - 2025-03-25 19:42:18,705 - log
        - obj : clm - 2025-03-25 19:42:18,705 - log
        - lora_dropout : 0.1 - 2025-03-25 19:42:18,705 - log
        - label_smooth : 0.1 - 2025-03-25 19:42:18,705 - log
        - roll_interval : -1 - 2025-03-25 19:42:18,705 - log
        - roll_lr : 1e-05 - 2025-03-25 19:42:18,705 - log
        - roll_step : 100 - 2025-03-25 19:42:18,705 - log
        - eval_epoch : 1 - 2025-03-25 19:42:18,705 - log
        - device : cuda - 2025-03-25 19:42:18,705 - log
==================================================================================================== - 2025-03-25 19:42:18,705 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 19:42:18,705 - log
loading model pretrained weight. - 2025-03-25 19:42:18,814 - log
set max_step: 1050 - 2025-03-25 19:42:19,752 - log
start to train the model................ 1 - 2025-03-25 19:42:19,753 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.11 | loss 15.19 | avg loss 16.53 | ppl 15075306.62 - 2025-03-25 19:42:23,264 - log
==================================================================================================== - 2025-03-25 19:44:19,028 - log
        - random_seed : 2025 - 2025-03-25 19:44:19,028 - log
        - lr : 0.0002 - 2025-03-25 19:44:19,028 - log
        - weight_decay : 0.01 - 2025-03-25 19:44:19,028 - log
        - correct_bias : False - 2025-03-25 19:44:19,028 - log
        - adam_epislon : 1e-06 - 2025-03-25 19:44:19,028 - log
        - no_decay_bias : False - 2025-03-25 19:44:19,028 - log
        - adam_beta1 : 0.9 - 2025-03-25 19:44:19,028 - log
        - adam_beta2 : 0.999 - 2025-03-25 19:44:19,028 - log
        - scheduler : linear - 2025-03-25 19:44:19,029 - log
        - max_step : None - 2025-03-25 19:44:19,029 - log
        - max_epoch : 5 - 2025-03-25 19:44:19,029 - log
        - warmup_step : 500 - 2025-03-25 19:44:19,029 - log
        - i_steps : 0 - 2025-03-25 19:44:19,029 - log
        - i_lrs : 0.00025 - 2025-03-25 19:44:19,029 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 19:44:19,029 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 19:44:19,029 - log
        - train_batch_size : 2 - 2025-03-25 19:44:19,029 - log
        - valid_batch_size : 1 - 2025-03-25 19:44:19,029 - log
        - grad_acc : 2 - 2025-03-25 19:44:19,029 - log
        - clip : 0.0 - 2025-03-25 19:44:19,029 - log
        - seq_len : 64 - 2025-03-25 19:44:19,029 - log
        - model_card : gpt2.sm - 2025-03-25 19:44:19,029 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 19:44:19,029 - log
        - fp16 : False - 2025-03-25 19:44:19,029 - log
        - log_interval : 100 - 2025-03-25 19:44:19,029 - log
        - eval_interval : 2000 - 2025-03-25 19:44:19,029 - log
        - save_interval : 1000 - 2025-03-25 19:44:19,029 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 19:44:19,029 - log
        - lora_dim : 4 - 2025-03-25 19:44:19,029 - log
        - lora_alpha : 32 - 2025-03-25 19:44:19,029 - log
        - obj : clm - 2025-03-25 19:44:19,029 - log
        - lora_dropout : 0.1 - 2025-03-25 19:44:19,029 - log
        - label_smooth : 0.1 - 2025-03-25 19:44:19,029 - log
        - roll_interval : -1 - 2025-03-25 19:44:19,029 - log
        - roll_lr : 1e-05 - 2025-03-25 19:44:19,029 - log
        - roll_step : 100 - 2025-03-25 19:44:19,029 - log
        - eval_epoch : 1 - 2025-03-25 19:44:19,029 - log
        - device : cuda - 2025-03-25 19:44:19,029 - log
==================================================================================================== - 2025-03-25 19:44:19,029 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 19:44:19,029 - log
loading model pretrained weight. - 2025-03-25 19:44:19,139 - log
set max_step: 1050 - 2025-03-25 19:44:20,005 - log
start to train the model................ 1 - 2025-03-25 19:44:20,005 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.39 | loss 15.19 | avg loss 16.53 | ppl 15075447.65 - 2025-03-25 19:44:23,545 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.51 | loss 16.86 | avg loss 16.56 | ppl 15480604.81 - 2025-03-25 19:44:26,796 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 19:44:27,120 - log
start to train the model................ 2 - 2025-03-25 19:44:28,555 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.23 | loss 17.93 | avg loss 16.76 | ppl 19042312.69 - 2025-03-25 19:44:31,478 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.57 | loss 15.73 | avg loss 16.61 | ppl 16292609.26 - 2025-03-25 19:44:34,735 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 19:44:35,385 - log
start to train the model................ 3 - 2025-03-25 19:44:36,745 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.12 | loss 15.37 | avg loss 16.79 | ppl 19553444.00 - 2025-03-25 19:44:39,357 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.70 | loss 16.82 | avg loss 16.24 | ppl 11347680.57 - 2025-03-25 19:44:42,627 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 19:44:43,625 - log
start to train the model................ 4 - 2025-03-25 19:44:45,007 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.93 | loss 16.58 | avg loss 16.74 | ppl 18707776.68 - 2025-03-25 19:44:47,300 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 33.32 | loss 16.37 | avg loss 15.81 | ppl 7336888.13 - 2025-03-25 19:44:50,633 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 19:44:51,957 - log
start to train the model................ 5 - 2025-03-25 19:44:53,347 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.87 | loss 13.77 | avg loss 15.87 | ppl 7828723.73 - 2025-03-25 19:44:55,334 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 33.02 | loss 14.60 | avg loss 15.49 | ppl 5341790.14 - 2025-03-25 19:44:58,637 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 19:44:58,637 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 19:45:00,296 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 19:45:01,789 - log
End of training - 2025-03-25 19:45:01,789 - log
==================================================================================================== - 2025-03-25 19:50:26,008 - log
        - random_seed : 2025 - 2025-03-25 19:50:26,008 - log
        - lr : 0.0002 - 2025-03-25 19:50:26,008 - log
        - weight_decay : 0.01 - 2025-03-25 19:50:26,008 - log
        - correct_bias : False - 2025-03-25 19:50:26,008 - log
        - adam_epislon : 1e-06 - 2025-03-25 19:50:26,008 - log
        - no_decay_bias : False - 2025-03-25 19:50:26,008 - log
        - adam_beta1 : 0.9 - 2025-03-25 19:50:26,008 - log
        - adam_beta2 : 0.999 - 2025-03-25 19:50:26,008 - log
        - scheduler : linear - 2025-03-25 19:50:26,008 - log
        - max_step : None - 2025-03-25 19:50:26,008 - log
        - max_epoch : 5 - 2025-03-25 19:50:26,008 - log
        - warmup_step : 500 - 2025-03-25 19:50:26,008 - log
        - i_steps : 0 - 2025-03-25 19:50:26,008 - log
        - i_lrs : 0.00025 - 2025-03-25 19:50:26,008 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 19:50:26,008 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 19:50:26,008 - log
        - train_batch_size : 2 - 2025-03-25 19:50:26,008 - log
        - valid_batch_size : 1 - 2025-03-25 19:50:26,008 - log
        - grad_acc : 2 - 2025-03-25 19:50:26,008 - log
        - clip : 0.0 - 2025-03-25 19:50:26,008 - log
        - seq_len : 64 - 2025-03-25 19:50:26,008 - log
        - model_card : gpt2.sm - 2025-03-25 19:50:26,008 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 19:50:26,008 - log
        - fp16 : False - 2025-03-25 19:50:26,008 - log
        - log_interval : 100 - 2025-03-25 19:50:26,008 - log
        - eval_interval : 2000 - 2025-03-25 19:50:26,008 - log
        - save_interval : 1000 - 2025-03-25 19:50:26,008 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 19:50:26,008 - log
        - lora_dim : 4 - 2025-03-25 19:50:26,008 - log
        - lora_alpha : 32 - 2025-03-25 19:50:26,008 - log
        - obj : clm - 2025-03-25 19:50:26,008 - log
        - lora_dropout : 0.1 - 2025-03-25 19:50:26,008 - log
        - label_smooth : 0.1 - 2025-03-25 19:50:26,008 - log
        - roll_interval : -1 - 2025-03-25 19:50:26,008 - log
        - roll_lr : 1e-05 - 2025-03-25 19:50:26,008 - log
        - roll_step : 100 - 2025-03-25 19:50:26,008 - log
        - eval_epoch : 1 - 2025-03-25 19:50:26,008 - log
        - device : cuda - 2025-03-25 19:50:26,008 - log
==================================================================================================== - 2025-03-25 19:50:26,008 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 19:50:26,008 - log
loading model pretrained weight. - 2025-03-25 19:50:26,119 - log
set max_step: 1050 - 2025-03-25 19:50:27,036 - log
start to train the model................ 1 - 2025-03-25 19:50:27,036 - log
==================================================================================================== - 2025-03-25 19:59:36,009 - log
        - random_seed : 2025 - 2025-03-25 19:59:36,009 - log
        - lr : 0.0002 - 2025-03-25 19:59:36,009 - log
        - weight_decay : 0.01 - 2025-03-25 19:59:36,009 - log
        - correct_bias : False - 2025-03-25 19:59:36,009 - log
        - adam_epislon : 1e-06 - 2025-03-25 19:59:36,009 - log
        - no_decay_bias : False - 2025-03-25 19:59:36,009 - log
        - adam_beta1 : 0.9 - 2025-03-25 19:59:36,009 - log
        - adam_beta2 : 0.999 - 2025-03-25 19:59:36,009 - log
        - scheduler : linear - 2025-03-25 19:59:36,009 - log
        - max_step : None - 2025-03-25 19:59:36,009 - log
        - max_epoch : 5 - 2025-03-25 19:59:36,009 - log
        - warmup_step : 500 - 2025-03-25 19:59:36,009 - log
        - i_steps : 0 - 2025-03-25 19:59:36,009 - log
        - i_lrs : 0.00025 - 2025-03-25 19:59:36,009 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 19:59:36,009 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 19:59:36,009 - log
        - train_batch_size : 2 - 2025-03-25 19:59:36,009 - log
        - valid_batch_size : 1 - 2025-03-25 19:59:36,009 - log
        - grad_acc : 2 - 2025-03-25 19:59:36,009 - log
        - clip : 0.0 - 2025-03-25 19:59:36,009 - log
        - seq_len : 64 - 2025-03-25 19:59:36,009 - log
        - model_card : gpt2.sm - 2025-03-25 19:59:36,009 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 19:59:36,009 - log
        - fp16 : False - 2025-03-25 19:59:36,009 - log
        - log_interval : 100 - 2025-03-25 19:59:36,009 - log
        - eval_interval : 2000 - 2025-03-25 19:59:36,009 - log
        - save_interval : 1000 - 2025-03-25 19:59:36,009 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 19:59:36,009 - log
        - lora_dim : 4 - 2025-03-25 19:59:36,009 - log
        - lora_alpha : 32 - 2025-03-25 19:59:36,009 - log
        - obj : clm - 2025-03-25 19:59:36,009 - log
        - lora_dropout : 0.1 - 2025-03-25 19:59:36,009 - log
        - label_smooth : 0.1 - 2025-03-25 19:59:36,009 - log
        - roll_interval : -1 - 2025-03-25 19:59:36,009 - log
        - roll_lr : 1e-05 - 2025-03-25 19:59:36,009 - log
        - roll_step : 100 - 2025-03-25 19:59:36,009 - log
        - eval_epoch : 1 - 2025-03-25 19:59:36,009 - log
        - device : cuda - 2025-03-25 19:59:36,009 - log
==================================================================================================== - 2025-03-25 19:59:36,009 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 19:59:36,009 - log
loading model pretrained weight. - 2025-03-25 19:59:36,120 - log
set max_step: 1050 - 2025-03-25 19:59:37,026 - log
start to train the model................ 1 - 2025-03-25 19:59:37,026 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 35.46 | loss 15.19 | avg loss 16.53 | ppl 15075421.92 - 2025-03-25 19:59:40,572 - log
==================================================================================================== - 2025-03-25 20:00:06,337 - log
        - random_seed : 2025 - 2025-03-25 20:00:06,337 - log
        - lr : 0.0002 - 2025-03-25 20:00:06,337 - log
        - weight_decay : 0.01 - 2025-03-25 20:00:06,337 - log
        - correct_bias : False - 2025-03-25 20:00:06,337 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:00:06,337 - log
        - no_decay_bias : False - 2025-03-25 20:00:06,337 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:00:06,337 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:00:06,337 - log
        - scheduler : linear - 2025-03-25 20:00:06,337 - log
        - max_step : None - 2025-03-25 20:00:06,337 - log
        - max_epoch : 5 - 2025-03-25 20:00:06,337 - log
        - warmup_step : 500 - 2025-03-25 20:00:06,337 - log
        - i_steps : 0 - 2025-03-25 20:00:06,337 - log
        - i_lrs : 0.00025 - 2025-03-25 20:00:06,337 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:00:06,337 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:00:06,337 - log
        - train_batch_size : 2 - 2025-03-25 20:00:06,337 - log
        - valid_batch_size : 1 - 2025-03-25 20:00:06,337 - log
        - grad_acc : 2 - 2025-03-25 20:00:06,337 - log
        - clip : 0.0 - 2025-03-25 20:00:06,337 - log
        - seq_len : 64 - 2025-03-25 20:00:06,337 - log
        - model_card : gpt2.sm - 2025-03-25 20:00:06,337 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:00:06,337 - log
        - fp16 : False - 2025-03-25 20:00:06,337 - log
        - log_interval : 100 - 2025-03-25 20:00:06,337 - log
        - eval_interval : 2000 - 2025-03-25 20:00:06,337 - log
        - save_interval : 1000 - 2025-03-25 20:00:06,337 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:00:06,337 - log
        - lora_dim : 4 - 2025-03-25 20:00:06,337 - log
        - lora_alpha : 32 - 2025-03-25 20:00:06,337 - log
        - obj : clm - 2025-03-25 20:00:06,337 - log
        - lora_dropout : 0.1 - 2025-03-25 20:00:06,337 - log
        - label_smooth : 0.1 - 2025-03-25 20:00:06,337 - log
        - roll_interval : -1 - 2025-03-25 20:00:06,337 - log
        - roll_lr : 1e-05 - 2025-03-25 20:00:06,337 - log
        - roll_step : 100 - 2025-03-25 20:00:06,337 - log
        - eval_epoch : 1 - 2025-03-25 20:00:06,337 - log
        - device : cuda - 2025-03-25 20:00:06,337 - log
==================================================================================================== - 2025-03-25 20:00:06,337 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:00:06,337 - log
loading model pretrained weight. - 2025-03-25 20:00:06,447 - log
set max_step: 1050 - 2025-03-25 20:00:07,267 - log
start to train the model................ 1 - 2025-03-25 20:00:07,268 - log
==================================================================================================== - 2025-03-25 20:02:59,034 - log
        - random_seed : 2025 - 2025-03-25 20:02:59,034 - log
        - lr : 0.0002 - 2025-03-25 20:02:59,034 - log
        - weight_decay : 0.01 - 2025-03-25 20:02:59,034 - log
        - correct_bias : False - 2025-03-25 20:02:59,034 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:02:59,034 - log
        - no_decay_bias : False - 2025-03-25 20:02:59,034 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:02:59,034 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:02:59,034 - log
        - scheduler : linear - 2025-03-25 20:02:59,034 - log
        - max_step : None - 2025-03-25 20:02:59,034 - log
        - max_epoch : 5 - 2025-03-25 20:02:59,034 - log
        - warmup_step : 500 - 2025-03-25 20:02:59,034 - log
        - i_steps : 0 - 2025-03-25 20:02:59,034 - log
        - i_lrs : 0.00025 - 2025-03-25 20:02:59,034 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:02:59,034 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:02:59,034 - log
        - train_batch_size : 2 - 2025-03-25 20:02:59,034 - log
        - valid_batch_size : 1 - 2025-03-25 20:02:59,034 - log
        - grad_acc : 2 - 2025-03-25 20:02:59,034 - log
        - clip : 0.0 - 2025-03-25 20:02:59,034 - log
        - seq_len : 64 - 2025-03-25 20:02:59,034 - log
        - model_card : gpt2.sm - 2025-03-25 20:02:59,034 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:02:59,034 - log
        - fp16 : False - 2025-03-25 20:02:59,034 - log
        - log_interval : 100 - 2025-03-25 20:02:59,034 - log
        - eval_interval : 2000 - 2025-03-25 20:02:59,034 - log
        - save_interval : 1000 - 2025-03-25 20:02:59,034 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:02:59,034 - log
        - lora_dim : 4 - 2025-03-25 20:02:59,034 - log
        - lora_alpha : 32 - 2025-03-25 20:02:59,034 - log
        - obj : clm - 2025-03-25 20:02:59,034 - log
        - lora_dropout : 0.1 - 2025-03-25 20:02:59,034 - log
        - label_smooth : 0.1 - 2025-03-25 20:02:59,034 - log
        - roll_interval : -1 - 2025-03-25 20:02:59,034 - log
        - roll_lr : 1e-05 - 2025-03-25 20:02:59,034 - log
        - roll_step : 100 - 2025-03-25 20:02:59,034 - log
        - eval_epoch : 1 - 2025-03-25 20:02:59,035 - log
        - device : cuda - 2025-03-25 20:02:59,035 - log
==================================================================================================== - 2025-03-25 20:02:59,035 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:02:59,035 - log
loading model pretrained weight. - 2025-03-25 20:02:59,144 - log
set max_step: 1050 - 2025-03-25 20:02:59,973 - log
start to train the model................ 1 - 2025-03-25 20:02:59,973 - log
==================================================================================================== - 2025-03-25 20:05:25,719 - log
        - random_seed : 2025 - 2025-03-25 20:05:25,719 - log
        - lr : 0.0002 - 2025-03-25 20:05:25,719 - log
        - weight_decay : 0.01 - 2025-03-25 20:05:25,719 - log
        - correct_bias : False - 2025-03-25 20:05:25,719 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:05:25,719 - log
        - no_decay_bias : False - 2025-03-25 20:05:25,719 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:05:25,719 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:05:25,719 - log
        - scheduler : linear - 2025-03-25 20:05:25,719 - log
        - max_step : None - 2025-03-25 20:05:25,719 - log
        - max_epoch : 5 - 2025-03-25 20:05:25,720 - log
        - warmup_step : 500 - 2025-03-25 20:05:25,720 - log
        - i_steps : 0 - 2025-03-25 20:05:25,720 - log
        - i_lrs : 0.00025 - 2025-03-25 20:05:25,720 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:05:25,720 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:05:25,720 - log
        - train_batch_size : 2 - 2025-03-25 20:05:25,720 - log
        - valid_batch_size : 1 - 2025-03-25 20:05:25,720 - log
        - grad_acc : 2 - 2025-03-25 20:05:25,720 - log
        - clip : 0.0 - 2025-03-25 20:05:25,720 - log
        - seq_len : 64 - 2025-03-25 20:05:25,720 - log
        - model_card : gpt2.sm - 2025-03-25 20:05:25,720 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:05:25,720 - log
        - fp16 : False - 2025-03-25 20:05:25,720 - log
        - log_interval : 100 - 2025-03-25 20:05:25,720 - log
        - eval_interval : 2000 - 2025-03-25 20:05:25,720 - log
        - save_interval : 1000 - 2025-03-25 20:05:25,720 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:05:25,720 - log
        - lora_dim : 4 - 2025-03-25 20:05:25,720 - log
        - lora_alpha : 32 - 2025-03-25 20:05:25,720 - log
        - obj : clm - 2025-03-25 20:05:25,720 - log
        - lora_dropout : 0.1 - 2025-03-25 20:05:25,720 - log
        - label_smooth : 0.1 - 2025-03-25 20:05:25,720 - log
        - roll_interval : -1 - 2025-03-25 20:05:25,720 - log
        - roll_lr : 1e-05 - 2025-03-25 20:05:25,720 - log
        - roll_step : 100 - 2025-03-25 20:05:25,720 - log
        - eval_epoch : 1 - 2025-03-25 20:05:25,720 - log
        - device : cuda - 2025-03-25 20:05:25,720 - log
==================================================================================================== - 2025-03-25 20:05:25,720 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:05:25,720 - log
loading model pretrained weight. - 2025-03-25 20:05:25,829 - log
set max_step: 1050 - 2025-03-25 20:05:26,713 - log
start to train the model................ 1 - 2025-03-25 20:05:26,713 - log
==================================================================================================== - 2025-03-25 20:06:00,563 - log
        - random_seed : 2025 - 2025-03-25 20:06:00,563 - log
        - lr : 0.0002 - 2025-03-25 20:06:00,563 - log
        - weight_decay : 0.01 - 2025-03-25 20:06:00,563 - log
        - correct_bias : False - 2025-03-25 20:06:00,563 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:06:00,563 - log
        - no_decay_bias : False - 2025-03-25 20:06:00,563 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:06:00,563 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:06:00,563 - log
        - scheduler : linear - 2025-03-25 20:06:00,563 - log
        - max_step : None - 2025-03-25 20:06:00,563 - log
        - max_epoch : 5 - 2025-03-25 20:06:00,563 - log
        - warmup_step : 500 - 2025-03-25 20:06:00,563 - log
        - i_steps : 0 - 2025-03-25 20:06:00,563 - log
        - i_lrs : 0.00025 - 2025-03-25 20:06:00,563 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:06:00,563 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:06:00,563 - log
        - train_batch_size : 2 - 2025-03-25 20:06:00,563 - log
        - valid_batch_size : 1 - 2025-03-25 20:06:00,563 - log
        - grad_acc : 2 - 2025-03-25 20:06:00,563 - log
        - clip : 0.0 - 2025-03-25 20:06:00,563 - log
        - seq_len : 64 - 2025-03-25 20:06:00,563 - log
        - model_card : gpt2.sm - 2025-03-25 20:06:00,563 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:06:00,563 - log
        - fp16 : False - 2025-03-25 20:06:00,563 - log
        - log_interval : 100 - 2025-03-25 20:06:00,563 - log
        - eval_interval : 2000 - 2025-03-25 20:06:00,564 - log
        - save_interval : 1000 - 2025-03-25 20:06:00,564 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:06:00,564 - log
        - lora_dim : 4 - 2025-03-25 20:06:00,564 - log
        - lora_alpha : 32 - 2025-03-25 20:06:00,564 - log
        - obj : clm - 2025-03-25 20:06:00,564 - log
        - lora_dropout : 0.1 - 2025-03-25 20:06:00,564 - log
        - label_smooth : 0.1 - 2025-03-25 20:06:00,564 - log
        - roll_interval : -1 - 2025-03-25 20:06:00,564 - log
        - roll_lr : 1e-05 - 2025-03-25 20:06:00,564 - log
        - roll_step : 100 - 2025-03-25 20:06:00,564 - log
        - eval_epoch : 1 - 2025-03-25 20:06:00,564 - log
        - device : cuda - 2025-03-25 20:06:00,564 - log
==================================================================================================== - 2025-03-25 20:06:00,564 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:06:00,564 - log
loading model pretrained weight. - 2025-03-25 20:06:00,680 - log
set max_step: 1050 - 2025-03-25 20:06:01,553 - log
start to train the model................ 1 - 2025-03-25 20:06:01,553 - log
==================================================================================================== - 2025-03-25 20:30:23,641 - log
        - random_seed : 2025 - 2025-03-25 20:30:23,642 - log
        - lr : 0.0002 - 2025-03-25 20:30:23,642 - log
        - weight_decay : 0.01 - 2025-03-25 20:30:23,642 - log
        - correct_bias : False - 2025-03-25 20:30:23,642 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:30:23,642 - log
        - no_decay_bias : False - 2025-03-25 20:30:23,642 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:30:23,642 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:30:23,642 - log
        - scheduler : linear - 2025-03-25 20:30:23,642 - log
        - max_step : None - 2025-03-25 20:30:23,642 - log
        - max_epoch : 5 - 2025-03-25 20:30:23,642 - log
        - warmup_step : 500 - 2025-03-25 20:30:23,642 - log
        - i_steps : 0 - 2025-03-25 20:30:23,642 - log
        - i_lrs : 0.00025 - 2025-03-25 20:30:23,642 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:30:23,642 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:30:23,642 - log
        - train_batch_size : 2 - 2025-03-25 20:30:23,642 - log
        - valid_batch_size : 1 - 2025-03-25 20:30:23,642 - log
        - grad_acc : 2 - 2025-03-25 20:30:23,642 - log
        - clip : 0.0 - 2025-03-25 20:30:23,642 - log
        - seq_len : 64 - 2025-03-25 20:30:23,642 - log
        - model_card : gpt2.sm - 2025-03-25 20:30:23,642 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:30:23,642 - log
        - fp16 : False - 2025-03-25 20:30:23,642 - log
        - log_interval : 100 - 2025-03-25 20:30:23,642 - log
        - eval_interval : 2000 - 2025-03-25 20:30:23,642 - log
        - save_interval : 1000 - 2025-03-25 20:30:23,642 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:30:23,642 - log
        - lora_dim : 4 - 2025-03-25 20:30:23,642 - log
        - lora_alpha : 32 - 2025-03-25 20:30:23,642 - log
        - obj : clm - 2025-03-25 20:30:23,642 - log
        - lora_dropout : 0.1 - 2025-03-25 20:30:23,642 - log
        - label_smooth : 0.1 - 2025-03-25 20:30:23,642 - log
        - roll_interval : -1 - 2025-03-25 20:30:23,642 - log
        - roll_lr : 1e-05 - 2025-03-25 20:30:23,642 - log
        - roll_step : 100 - 2025-03-25 20:30:23,642 - log
        - eval_epoch : 1 - 2025-03-25 20:30:23,642 - log
        - device : cuda - 2025-03-25 20:30:23,642 - log
==================================================================================================== - 2025-03-25 20:30:23,642 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:30:23,642 - log
loading model pretrained weight. - 2025-03-25 20:30:23,777 - log
set max_step: 1050 - 2025-03-25 20:30:25,327 - log
start to train the model................ 1 - 2025-03-25 20:30:25,327 - log
==================================================================================================== - 2025-03-25 20:39:04,970 - log
        - random_seed : 2025 - 2025-03-25 20:39:04,970 - log
        - lr : 0.0002 - 2025-03-25 20:39:04,970 - log
        - weight_decay : 0.01 - 2025-03-25 20:39:04,970 - log
        - correct_bias : False - 2025-03-25 20:39:04,970 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:39:04,970 - log
        - no_decay_bias : False - 2025-03-25 20:39:04,970 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:39:04,970 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:39:04,970 - log
        - scheduler : linear - 2025-03-25 20:39:04,970 - log
        - max_step : None - 2025-03-25 20:39:04,970 - log
        - max_epoch : 5 - 2025-03-25 20:39:04,970 - log
        - warmup_step : 500 - 2025-03-25 20:39:04,970 - log
        - i_steps : 0 - 2025-03-25 20:39:04,970 - log
        - i_lrs : 0.00025 - 2025-03-25 20:39:04,970 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:39:04,970 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:39:04,970 - log
        - train_batch_size : 2 - 2025-03-25 20:39:04,970 - log
        - valid_batch_size : 1 - 2025-03-25 20:39:04,970 - log
        - grad_acc : 2 - 2025-03-25 20:39:04,970 - log
        - clip : 0.0 - 2025-03-25 20:39:04,970 - log
        - seq_len : 64 - 2025-03-25 20:39:04,970 - log
        - model_card : gpt2.sm - 2025-03-25 20:39:04,970 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:39:04,970 - log
        - fp16 : False - 2025-03-25 20:39:04,970 - log
        - log_interval : 100 - 2025-03-25 20:39:04,970 - log
        - eval_interval : 2000 - 2025-03-25 20:39:04,970 - log
        - save_interval : 1000 - 2025-03-25 20:39:04,970 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:39:04,970 - log
        - lora_dim : 4 - 2025-03-25 20:39:04,971 - log
        - lora_alpha : 32 - 2025-03-25 20:39:04,971 - log
        - obj : clm - 2025-03-25 20:39:04,971 - log
        - lora_dropout : 0.1 - 2025-03-25 20:39:04,971 - log
        - label_smooth : 0.1 - 2025-03-25 20:39:04,971 - log
        - roll_interval : -1 - 2025-03-25 20:39:04,971 - log
        - roll_lr : 1e-05 - 2025-03-25 20:39:04,971 - log
        - roll_step : 100 - 2025-03-25 20:39:04,971 - log
        - eval_epoch : 1 - 2025-03-25 20:39:04,971 - log
        - device : cuda - 2025-03-25 20:39:04,971 - log
==================================================================================================== - 2025-03-25 20:39:04,971 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:39:04,971 - log
loading model pretrained weight. - 2025-03-25 20:39:05,077 - log
set max_step: 1050 - 2025-03-25 20:39:05,845 - log
start to train the model................ 1 - 2025-03-25 20:39:05,845 - log
==================================================================================================== - 2025-03-25 20:39:24,086 - log
        - random_seed : 2025 - 2025-03-25 20:39:24,086 - log
        - lr : 0.0002 - 2025-03-25 20:39:24,086 - log
        - weight_decay : 0.01 - 2025-03-25 20:39:24,086 - log
        - correct_bias : False - 2025-03-25 20:39:24,086 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:39:24,086 - log
        - no_decay_bias : False - 2025-03-25 20:39:24,086 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:39:24,086 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:39:24,086 - log
        - scheduler : linear - 2025-03-25 20:39:24,086 - log
        - max_step : None - 2025-03-25 20:39:24,086 - log
        - max_epoch : 5 - 2025-03-25 20:39:24,086 - log
        - warmup_step : 500 - 2025-03-25 20:39:24,086 - log
        - i_steps : 0 - 2025-03-25 20:39:24,086 - log
        - i_lrs : 0.00025 - 2025-03-25 20:39:24,086 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:39:24,086 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:39:24,086 - log
        - train_batch_size : 2 - 2025-03-25 20:39:24,086 - log
        - valid_batch_size : 1 - 2025-03-25 20:39:24,086 - log
        - grad_acc : 2 - 2025-03-25 20:39:24,086 - log
        - clip : 0.0 - 2025-03-25 20:39:24,086 - log
        - seq_len : 64 - 2025-03-25 20:39:24,086 - log
        - model_card : gpt2.sm - 2025-03-25 20:39:24,086 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:39:24,087 - log
        - fp16 : False - 2025-03-25 20:39:24,087 - log
        - log_interval : 100 - 2025-03-25 20:39:24,087 - log
        - eval_interval : 2000 - 2025-03-25 20:39:24,087 - log
        - save_interval : 1000 - 2025-03-25 20:39:24,087 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:39:24,087 - log
        - lora_dim : 4 - 2025-03-25 20:39:24,087 - log
        - lora_alpha : 32 - 2025-03-25 20:39:24,087 - log
        - obj : clm - 2025-03-25 20:39:24,087 - log
        - lora_dropout : 0.1 - 2025-03-25 20:39:24,087 - log
        - label_smooth : 0.1 - 2025-03-25 20:39:24,087 - log
        - roll_interval : -1 - 2025-03-25 20:39:24,087 - log
        - roll_lr : 1e-05 - 2025-03-25 20:39:24,087 - log
        - roll_step : 100 - 2025-03-25 20:39:24,087 - log
        - eval_epoch : 1 - 2025-03-25 20:39:24,087 - log
        - device : cuda - 2025-03-25 20:39:24,087 - log
==================================================================================================== - 2025-03-25 20:39:24,087 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:39:24,087 - log
loading model pretrained weight. - 2025-03-25 20:39:24,194 - log
set max_step: 1050 - 2025-03-25 20:39:24,962 - log
start to train the model................ 1 - 2025-03-25 20:39:24,962 - log
==================================================================================================== - 2025-03-25 20:39:58,480 - log
        - random_seed : 2025 - 2025-03-25 20:39:58,480 - log
        - lr : 0.0002 - 2025-03-25 20:39:58,480 - log
        - weight_decay : 0.01 - 2025-03-25 20:39:58,480 - log
        - correct_bias : False - 2025-03-25 20:39:58,480 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:39:58,480 - log
        - no_decay_bias : False - 2025-03-25 20:39:58,480 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:39:58,480 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:39:58,480 - log
        - scheduler : linear - 2025-03-25 20:39:58,480 - log
        - max_step : None - 2025-03-25 20:39:58,480 - log
        - max_epoch : 5 - 2025-03-25 20:39:58,480 - log
        - warmup_step : 500 - 2025-03-25 20:39:58,480 - log
        - i_steps : 0 - 2025-03-25 20:39:58,480 - log
        - i_lrs : 0.00025 - 2025-03-25 20:39:58,480 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:39:58,480 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:39:58,480 - log
        - train_batch_size : 2 - 2025-03-25 20:39:58,480 - log
        - valid_batch_size : 1 - 2025-03-25 20:39:58,480 - log
        - grad_acc : 2 - 2025-03-25 20:39:58,480 - log
        - clip : 0.0 - 2025-03-25 20:39:58,480 - log
        - seq_len : 64 - 2025-03-25 20:39:58,480 - log
        - model_card : gpt2.sm - 2025-03-25 20:39:58,480 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:39:58,480 - log
        - fp16 : False - 2025-03-25 20:39:58,480 - log
        - log_interval : 100 - 2025-03-25 20:39:58,480 - log
        - eval_interval : 2000 - 2025-03-25 20:39:58,480 - log
        - save_interval : 1000 - 2025-03-25 20:39:58,480 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:39:58,480 - log
        - lora_dim : 4 - 2025-03-25 20:39:58,480 - log
        - lora_alpha : 32 - 2025-03-25 20:39:58,480 - log
        - obj : clm - 2025-03-25 20:39:58,480 - log
        - lora_dropout : 0.1 - 2025-03-25 20:39:58,480 - log
        - label_smooth : 0.1 - 2025-03-25 20:39:58,480 - log
        - roll_interval : -1 - 2025-03-25 20:39:58,480 - log
        - roll_lr : 1e-05 - 2025-03-25 20:39:58,480 - log
        - roll_step : 100 - 2025-03-25 20:39:58,480 - log
        - eval_epoch : 1 - 2025-03-25 20:39:58,480 - log
        - device : cuda - 2025-03-25 20:39:58,480 - log
==================================================================================================== - 2025-03-25 20:39:58,480 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:39:58,480 - log
loading model pretrained weight. - 2025-03-25 20:39:58,586 - log
set max_step: 1050 - 2025-03-25 20:39:59,360 - log
start to train the model................ 1 - 2025-03-25 20:39:59,360 - log
==================================================================================================== - 2025-03-25 20:40:22,232 - log
        - random_seed : 2025 - 2025-03-25 20:40:22,232 - log
        - lr : 0.0002 - 2025-03-25 20:40:22,233 - log
        - weight_decay : 0.01 - 2025-03-25 20:40:22,233 - log
        - correct_bias : False - 2025-03-25 20:40:22,233 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:40:22,233 - log
        - no_decay_bias : False - 2025-03-25 20:40:22,233 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:40:22,233 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:40:22,233 - log
        - scheduler : linear - 2025-03-25 20:40:22,233 - log
        - max_step : None - 2025-03-25 20:40:22,233 - log
        - max_epoch : 5 - 2025-03-25 20:40:22,233 - log
        - warmup_step : 500 - 2025-03-25 20:40:22,233 - log
        - i_steps : 0 - 2025-03-25 20:40:22,233 - log
        - i_lrs : 0.00025 - 2025-03-25 20:40:22,233 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:40:22,233 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:40:22,233 - log
        - train_batch_size : 2 - 2025-03-25 20:40:22,233 - log
        - valid_batch_size : 1 - 2025-03-25 20:40:22,233 - log
        - grad_acc : 2 - 2025-03-25 20:40:22,233 - log
        - clip : 0.0 - 2025-03-25 20:40:22,233 - log
        - seq_len : 64 - 2025-03-25 20:40:22,233 - log
        - model_card : gpt2.sm - 2025-03-25 20:40:22,233 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:40:22,233 - log
        - fp16 : False - 2025-03-25 20:40:22,233 - log
        - log_interval : 100 - 2025-03-25 20:40:22,233 - log
        - eval_interval : 2000 - 2025-03-25 20:40:22,233 - log
        - save_interval : 1000 - 2025-03-25 20:40:22,233 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:40:22,233 - log
        - lora_dim : 4 - 2025-03-25 20:40:22,233 - log
        - lora_alpha : 32 - 2025-03-25 20:40:22,233 - log
        - obj : clm - 2025-03-25 20:40:22,233 - log
        - lora_dropout : 0.1 - 2025-03-25 20:40:22,233 - log
        - label_smooth : 0.1 - 2025-03-25 20:40:22,233 - log
        - roll_interval : -1 - 2025-03-25 20:40:22,233 - log
        - roll_lr : 1e-05 - 2025-03-25 20:40:22,233 - log
        - roll_step : 100 - 2025-03-25 20:40:22,233 - log
        - eval_epoch : 1 - 2025-03-25 20:40:22,233 - log
        - device : cuda - 2025-03-25 20:40:22,233 - log
==================================================================================================== - 2025-03-25 20:40:22,233 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:40:22,233 - log
loading model pretrained weight. - 2025-03-25 20:40:22,338 - log
set max_step: 1050 - 2025-03-25 20:40:23,112 - log
start to train the model................ 1 - 2025-03-25 20:40:23,113 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 48.65 | loss 15.19 | avg loss 16.53 | ppl 15075303.60 - 2025-03-25 20:40:27,977 - log
==================================================================================================== - 2025-03-25 20:40:46,145 - log
        - random_seed : 2025 - 2025-03-25 20:40:46,145 - log
        - lr : 0.0002 - 2025-03-25 20:40:46,145 - log
        - weight_decay : 0.01 - 2025-03-25 20:40:46,145 - log
        - correct_bias : False - 2025-03-25 20:40:46,145 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:40:46,145 - log
        - no_decay_bias : False - 2025-03-25 20:40:46,145 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:40:46,145 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:40:46,145 - log
        - scheduler : linear - 2025-03-25 20:40:46,145 - log
        - max_step : None - 2025-03-25 20:40:46,145 - log
        - max_epoch : 5 - 2025-03-25 20:40:46,145 - log
        - warmup_step : 500 - 2025-03-25 20:40:46,145 - log
        - i_steps : 0 - 2025-03-25 20:40:46,145 - log
        - i_lrs : 0.00025 - 2025-03-25 20:40:46,145 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:40:46,145 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:40:46,145 - log
        - train_batch_size : 2 - 2025-03-25 20:40:46,145 - log
        - valid_batch_size : 1 - 2025-03-25 20:40:46,145 - log
        - grad_acc : 2 - 2025-03-25 20:40:46,145 - log
        - clip : 0.0 - 2025-03-25 20:40:46,145 - log
        - seq_len : 64 - 2025-03-25 20:40:46,145 - log
        - model_card : gpt2.sm - 2025-03-25 20:40:46,145 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:40:46,145 - log
        - fp16 : False - 2025-03-25 20:40:46,145 - log
        - log_interval : 100 - 2025-03-25 20:40:46,145 - log
        - eval_interval : 2000 - 2025-03-25 20:40:46,145 - log
        - save_interval : 1000 - 2025-03-25 20:40:46,145 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:40:46,145 - log
        - lora_dim : 4 - 2025-03-25 20:40:46,145 - log
        - lora_alpha : 32 - 2025-03-25 20:40:46,145 - log
        - obj : clm - 2025-03-25 20:40:46,145 - log
        - lora_dropout : 0.1 - 2025-03-25 20:40:46,145 - log
        - label_smooth : 0.1 - 2025-03-25 20:40:46,145 - log
        - roll_interval : -1 - 2025-03-25 20:40:46,145 - log
        - roll_lr : 1e-05 - 2025-03-25 20:40:46,145 - log
        - roll_step : 100 - 2025-03-25 20:40:46,145 - log
        - eval_epoch : 1 - 2025-03-25 20:40:46,145 - log
        - device : cuda - 2025-03-25 20:40:46,145 - log
==================================================================================================== - 2025-03-25 20:40:46,145 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:40:46,145 - log
loading model pretrained weight. - 2025-03-25 20:40:46,248 - log
set max_step: 1050 - 2025-03-25 20:40:47,016 - log
start to train the model................ 1 - 2025-03-25 20:40:47,016 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 46.93 | loss 15.19 | avg loss 16.53 | ppl 15075376.49 - 2025-03-25 20:40:51,710 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 43.98 | loss 16.86 | avg loss 16.56 | ppl 15480672.87 - 2025-03-25 20:40:56,108 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 20:40:56,548 - log
start to train the model................ 2 - 2025-03-25 20:40:57,933 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 39.75 | loss 17.93 | avg loss 16.76 | ppl 19042286.86 - 2025-03-25 20:41:01,908 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 44.57 | loss 15.73 | avg loss 16.61 | ppl 16292848.08 - 2025-03-25 20:41:06,365 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 20:41:07,276 - log
start to train the model................ 3 - 2025-03-25 20:41:08,687 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 36.16 | loss 15.37 | avg loss 16.79 | ppl 19552861.50 - 2025-03-25 20:41:12,304 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 44.78 | loss 16.84 | avg loss 16.25 | ppl 11377863.35 - 2025-03-25 20:41:16,781 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 20:41:18,137 - log
start to train the model................ 4 - 2025-03-25 20:41:19,571 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 31.26 | loss 16.53 | avg loss 16.73 | ppl 18443864.91 - 2025-03-25 20:41:22,697 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 45.32 | loss 16.38 | avg loss 15.81 | ppl 7325974.77 - 2025-03-25 20:41:27,228 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 20:41:29,015 - log
start to train the model................ 5 - 2025-03-25 20:41:30,408 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 26.91 | loss 13.75 | avg loss 15.85 | ppl 7677749.02 - 2025-03-25 20:41:33,099 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 44.87 | loss 14.59 | avg loss 15.49 | ppl 5352210.95 - 2025-03-25 20:41:37,586 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 20:41:37,586 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 20:41:39,832 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 20:41:41,212 - log
End of training - 2025-03-25 20:41:41,212 - log
==================================================================================================== - 2025-03-25 20:42:33,136 - log
        - random_seed : 2025 - 2025-03-25 20:42:33,136 - log
        - lr : 0.0002 - 2025-03-25 20:42:33,136 - log
        - weight_decay : 0.01 - 2025-03-25 20:42:33,136 - log
        - correct_bias : False - 2025-03-25 20:42:33,136 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:42:33,136 - log
        - no_decay_bias : False - 2025-03-25 20:42:33,136 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:42:33,136 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:42:33,136 - log
        - scheduler : linear - 2025-03-25 20:42:33,136 - log
        - max_step : None - 2025-03-25 20:42:33,136 - log
        - max_epoch : 5 - 2025-03-25 20:42:33,136 - log
        - warmup_step : 500 - 2025-03-25 20:42:33,136 - log
        - i_steps : 0 - 2025-03-25 20:42:33,136 - log
        - i_lrs : 0.00025 - 2025-03-25 20:42:33,136 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:42:33,136 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:42:33,136 - log
        - train_batch_size : 2 - 2025-03-25 20:42:33,136 - log
        - valid_batch_size : 1 - 2025-03-25 20:42:33,136 - log
        - grad_acc : 2 - 2025-03-25 20:42:33,136 - log
        - clip : 0.0 - 2025-03-25 20:42:33,136 - log
        - seq_len : 64 - 2025-03-25 20:42:33,136 - log
        - model_card : gpt2.sm - 2025-03-25 20:42:33,136 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:42:33,136 - log
        - fp16 : False - 2025-03-25 20:42:33,136 - log
        - log_interval : 100 - 2025-03-25 20:42:33,136 - log
        - eval_interval : 2000 - 2025-03-25 20:42:33,136 - log
        - save_interval : 1000 - 2025-03-25 20:42:33,136 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:42:33,136 - log
        - lora_dim : 4 - 2025-03-25 20:42:33,136 - log
        - lora_alpha : 32 - 2025-03-25 20:42:33,136 - log
        - obj : clm - 2025-03-25 20:42:33,136 - log
        - lora_dropout : 0.1 - 2025-03-25 20:42:33,136 - log
        - label_smooth : 0.1 - 2025-03-25 20:42:33,136 - log
        - roll_interval : -1 - 2025-03-25 20:42:33,136 - log
        - roll_lr : 1e-05 - 2025-03-25 20:42:33,136 - log
        - roll_step : 100 - 2025-03-25 20:42:33,136 - log
        - eval_epoch : 1 - 2025-03-25 20:42:33,137 - log
        - device : cuda - 2025-03-25 20:42:33,137 - log
==================================================================================================== - 2025-03-25 20:42:33,137 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:42:33,137 - log
loading model pretrained weight. - 2025-03-25 20:42:33,245 - log
set max_step: 1050 - 2025-03-25 20:42:34,044 - log
start to train the model................ 1 - 2025-03-25 20:42:34,044 - log
==================================================================================================== - 2025-03-25 20:43:00,519 - log
        - random_seed : 2025 - 2025-03-25 20:43:00,519 - log
        - lr : 0.0002 - 2025-03-25 20:43:00,519 - log
        - weight_decay : 0.01 - 2025-03-25 20:43:00,519 - log
        - correct_bias : False - 2025-03-25 20:43:00,519 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:43:00,519 - log
        - no_decay_bias : False - 2025-03-25 20:43:00,519 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:43:00,519 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:43:00,520 - log
        - scheduler : linear - 2025-03-25 20:43:00,520 - log
        - max_step : None - 2025-03-25 20:43:00,520 - log
        - max_epoch : 5 - 2025-03-25 20:43:00,520 - log
        - warmup_step : 500 - 2025-03-25 20:43:00,520 - log
        - i_steps : 0 - 2025-03-25 20:43:00,520 - log
        - i_lrs : 0.00025 - 2025-03-25 20:43:00,520 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:43:00,520 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:43:00,520 - log
        - train_batch_size : 2 - 2025-03-25 20:43:00,520 - log
        - valid_batch_size : 1 - 2025-03-25 20:43:00,520 - log
        - grad_acc : 2 - 2025-03-25 20:43:00,520 - log
        - clip : 0.0 - 2025-03-25 20:43:00,520 - log
        - seq_len : 64 - 2025-03-25 20:43:00,520 - log
        - model_card : gpt2.sm - 2025-03-25 20:43:00,520 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:43:00,520 - log
        - fp16 : False - 2025-03-25 20:43:00,520 - log
        - log_interval : 100 - 2025-03-25 20:43:00,520 - log
        - eval_interval : 2000 - 2025-03-25 20:43:00,520 - log
        - save_interval : 1000 - 2025-03-25 20:43:00,520 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:43:00,520 - log
        - lora_dim : 4 - 2025-03-25 20:43:00,520 - log
        - lora_alpha : 32 - 2025-03-25 20:43:00,520 - log
        - obj : clm - 2025-03-25 20:43:00,520 - log
        - lora_dropout : 0.1 - 2025-03-25 20:43:00,520 - log
        - label_smooth : 0.1 - 2025-03-25 20:43:00,520 - log
        - roll_interval : -1 - 2025-03-25 20:43:00,520 - log
        - roll_lr : 1e-05 - 2025-03-25 20:43:00,520 - log
        - roll_step : 100 - 2025-03-25 20:43:00,520 - log
        - eval_epoch : 1 - 2025-03-25 20:43:00,520 - log
        - device : cuda - 2025-03-25 20:43:00,520 - log
==================================================================================================== - 2025-03-25 20:43:00,520 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:43:00,520 - log
loading model pretrained weight. - 2025-03-25 20:43:00,629 - log
set max_step: 1050 - 2025-03-25 20:43:01,452 - log
start to train the model................ 1 - 2025-03-25 20:43:01,452 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 47.38 | loss 15.19 | avg loss 16.53 | ppl 15075365.42 - 2025-03-25 20:43:06,190 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 44.27 | loss 16.86 | avg loss 16.56 | ppl 15480683.35 - 2025-03-25 20:43:10,617 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 20:43:11,060 - log
start to train the model................ 2 - 2025-03-25 20:43:12,549 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 40.07 | loss 17.93 | avg loss 16.76 | ppl 19042257.20 - 2025-03-25 20:43:16,557 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 44.92 | loss 15.73 | avg loss 16.61 | ppl 16292766.66 - 2025-03-25 20:43:21,049 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 20:43:21,961 - log
start to train the model................ 3 - 2025-03-25 20:43:23,425 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 36.35 | loss 15.37 | avg loss 16.79 | ppl 19551920.32 - 2025-03-25 20:43:27,060 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 45.96 | loss 16.85 | avg loss 16.25 | ppl 11380110.00 - 2025-03-25 20:43:31,656 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 20:43:33,061 - log
start to train the model................ 4 - 2025-03-25 20:43:34,612 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 32.08 | loss 16.55 | avg loss 16.73 | ppl 18501800.85 - 2025-03-25 20:43:37,820 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 45.60 | loss 16.37 | avg loss 15.81 | ppl 7321707.56 - 2025-03-25 20:43:42,380 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 20:43:44,189 - log
start to train the model................ 5 - 2025-03-25 20:43:45,696 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 27.22 | loss 13.74 | avg loss 15.89 | ppl 7964009.10 - 2025-03-25 20:43:48,418 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 45.55 | loss 14.66 | avg loss 15.49 | ppl 5339957.77 - 2025-03-25 20:43:52,974 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 20:43:52,974 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 20:43:55,253 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 20:43:56,951 - log
End of training - 2025-03-25 20:43:56,951 - log
==================================================================================================== - 2025-03-25 20:47:50,429 - log
        - random_seed : 2025 - 2025-03-25 20:47:50,429 - log
        - lr : 0.0002 - 2025-03-25 20:47:50,429 - log
        - weight_decay : 0.01 - 2025-03-25 20:47:50,429 - log
        - correct_bias : False - 2025-03-25 20:47:50,429 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:47:50,429 - log
        - no_decay_bias : False - 2025-03-25 20:47:50,429 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:47:50,429 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:47:50,429 - log
        - scheduler : linear - 2025-03-25 20:47:50,429 - log
        - max_step : None - 2025-03-25 20:47:50,429 - log
        - max_epoch : 5 - 2025-03-25 20:47:50,429 - log
        - warmup_step : 500 - 2025-03-25 20:47:50,429 - log
        - i_steps : 0 - 2025-03-25 20:47:50,429 - log
        - i_lrs : 0.00025 - 2025-03-25 20:47:50,429 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:47:50,429 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:47:50,429 - log
        - train_batch_size : 2 - 2025-03-25 20:47:50,429 - log
        - valid_batch_size : 1 - 2025-03-25 20:47:50,429 - log
        - grad_acc : 2 - 2025-03-25 20:47:50,429 - log
        - clip : 0.0 - 2025-03-25 20:47:50,429 - log
        - seq_len : 64 - 2025-03-25 20:47:50,430 - log
        - model_card : gpt2.sm - 2025-03-25 20:47:50,430 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:47:50,430 - log
        - fp16 : False - 2025-03-25 20:47:50,430 - log
        - log_interval : 100 - 2025-03-25 20:47:50,430 - log
        - eval_interval : 2000 - 2025-03-25 20:47:50,430 - log
        - save_interval : 1000 - 2025-03-25 20:47:50,430 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:47:50,430 - log
        - lora_dim : 4 - 2025-03-25 20:47:50,430 - log
        - lora_alpha : 32 - 2025-03-25 20:47:50,430 - log
        - obj : clm - 2025-03-25 20:47:50,430 - log
        - lora_dropout : 0.1 - 2025-03-25 20:47:50,430 - log
        - label_smooth : 0.1 - 2025-03-25 20:47:50,430 - log
        - roll_interval : -1 - 2025-03-25 20:47:50,430 - log
        - roll_lr : 1e-05 - 2025-03-25 20:47:50,430 - log
        - roll_step : 100 - 2025-03-25 20:47:50,430 - log
        - eval_epoch : 1 - 2025-03-25 20:47:50,430 - log
        - device : cuda - 2025-03-25 20:47:50,430 - log
==================================================================================================== - 2025-03-25 20:47:50,430 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:47:50,430 - log
loading model pretrained weight. - 2025-03-25 20:47:50,537 - log
set max_step: 1050 - 2025-03-25 20:47:51,316 - log
start to train the model................ 1 - 2025-03-25 20:47:51,316 - log
==================================================================================================== - 2025-03-25 20:48:20,596 - log
        - random_seed : 2025 - 2025-03-25 20:48:20,596 - log
        - lr : 0.0002 - 2025-03-25 20:48:20,596 - log
        - weight_decay : 0.01 - 2025-03-25 20:48:20,596 - log
        - correct_bias : False - 2025-03-25 20:48:20,596 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:48:20,596 - log
        - no_decay_bias : False - 2025-03-25 20:48:20,596 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:48:20,596 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:48:20,596 - log
        - scheduler : linear - 2025-03-25 20:48:20,596 - log
        - max_step : None - 2025-03-25 20:48:20,596 - log
        - max_epoch : 5 - 2025-03-25 20:48:20,597 - log
        - warmup_step : 500 - 2025-03-25 20:48:20,597 - log
        - i_steps : 0 - 2025-03-25 20:48:20,597 - log
        - i_lrs : 0.00025 - 2025-03-25 20:48:20,597 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:48:20,597 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:48:20,597 - log
        - train_batch_size : 2 - 2025-03-25 20:48:20,597 - log
        - valid_batch_size : 1 - 2025-03-25 20:48:20,597 - log
        - grad_acc : 2 - 2025-03-25 20:48:20,597 - log
        - clip : 0.0 - 2025-03-25 20:48:20,597 - log
        - seq_len : 64 - 2025-03-25 20:48:20,597 - log
        - model_card : gpt2.sm - 2025-03-25 20:48:20,597 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:48:20,597 - log
        - fp16 : False - 2025-03-25 20:48:20,597 - log
        - log_interval : 100 - 2025-03-25 20:48:20,597 - log
        - eval_interval : 2000 - 2025-03-25 20:48:20,597 - log
        - save_interval : 1000 - 2025-03-25 20:48:20,597 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:48:20,597 - log
        - lora_dim : 4 - 2025-03-25 20:48:20,597 - log
        - lora_alpha : 32 - 2025-03-25 20:48:20,597 - log
        - obj : clm - 2025-03-25 20:48:20,597 - log
        - lora_dropout : 0.1 - 2025-03-25 20:48:20,597 - log
        - label_smooth : 0.1 - 2025-03-25 20:48:20,597 - log
        - roll_interval : -1 - 2025-03-25 20:48:20,597 - log
        - roll_lr : 1e-05 - 2025-03-25 20:48:20,597 - log
        - roll_step : 100 - 2025-03-25 20:48:20,597 - log
        - eval_epoch : 1 - 2025-03-25 20:48:20,597 - log
        - device : cuda - 2025-03-25 20:48:20,597 - log
==================================================================================================== - 2025-03-25 20:48:20,597 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:48:20,597 - log
loading model pretrained weight. - 2025-03-25 20:48:20,704 - log
set max_step: 1050 - 2025-03-25 20:48:21,481 - log
start to train the model................ 1 - 2025-03-25 20:48:21,481 - log
==================================================================================================== - 2025-03-25 20:51:18,483 - log
        - random_seed : 2025 - 2025-03-25 20:51:18,483 - log
        - lr : 0.0002 - 2025-03-25 20:51:18,483 - log
        - weight_decay : 0.01 - 2025-03-25 20:51:18,483 - log
        - correct_bias : False - 2025-03-25 20:51:18,483 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:51:18,483 - log
        - no_decay_bias : False - 2025-03-25 20:51:18,483 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:51:18,483 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:51:18,483 - log
        - scheduler : linear - 2025-03-25 20:51:18,483 - log
        - max_step : None - 2025-03-25 20:51:18,483 - log
        - max_epoch : 5 - 2025-03-25 20:51:18,483 - log
        - warmup_step : 500 - 2025-03-25 20:51:18,483 - log
        - i_steps : 0 - 2025-03-25 20:51:18,483 - log
        - i_lrs : 0.00025 - 2025-03-25 20:51:18,483 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:51:18,483 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:51:18,483 - log
        - train_batch_size : 2 - 2025-03-25 20:51:18,483 - log
        - valid_batch_size : 1 - 2025-03-25 20:51:18,483 - log
        - grad_acc : 2 - 2025-03-25 20:51:18,483 - log
        - clip : 0.0 - 2025-03-25 20:51:18,484 - log
        - seq_len : 64 - 2025-03-25 20:51:18,484 - log
        - model_card : gpt2.sm - 2025-03-25 20:51:18,484 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:51:18,484 - log
        - fp16 : False - 2025-03-25 20:51:18,484 - log
        - log_interval : 100 - 2025-03-25 20:51:18,484 - log
        - eval_interval : 2000 - 2025-03-25 20:51:18,484 - log
        - save_interval : 1000 - 2025-03-25 20:51:18,484 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:51:18,484 - log
        - lora_dim : 4 - 2025-03-25 20:51:18,484 - log
        - lora_alpha : 32 - 2025-03-25 20:51:18,484 - log
        - obj : clm - 2025-03-25 20:51:18,484 - log
        - lora_dropout : 0.1 - 2025-03-25 20:51:18,484 - log
        - label_smooth : 0.1 - 2025-03-25 20:51:18,484 - log
        - roll_interval : -1 - 2025-03-25 20:51:18,484 - log
        - roll_lr : 1e-05 - 2025-03-25 20:51:18,484 - log
        - roll_step : 100 - 2025-03-25 20:51:18,484 - log
        - eval_epoch : 1 - 2025-03-25 20:51:18,484 - log
        - device : cuda - 2025-03-25 20:51:18,484 - log
==================================================================================================== - 2025-03-25 20:51:18,484 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:51:18,484 - log
loading model pretrained weight. - 2025-03-25 20:51:18,593 - log
set max_step: 1050 - 2025-03-25 20:51:19,365 - log
start to train the model................ 1 - 2025-03-25 20:51:19,365 - log
==================================================================================================== - 2025-03-25 20:51:28,942 - log
        - random_seed : 2025 - 2025-03-25 20:51:28,942 - log
        - lr : 0.0002 - 2025-03-25 20:51:28,942 - log
        - weight_decay : 0.01 - 2025-03-25 20:51:28,942 - log
        - correct_bias : False - 2025-03-25 20:51:28,942 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:51:28,942 - log
        - no_decay_bias : False - 2025-03-25 20:51:28,942 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:51:28,942 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:51:28,942 - log
        - scheduler : linear - 2025-03-25 20:51:28,942 - log
        - max_step : None - 2025-03-25 20:51:28,942 - log
        - max_epoch : 5 - 2025-03-25 20:51:28,942 - log
        - warmup_step : 500 - 2025-03-25 20:51:28,942 - log
        - i_steps : 0 - 2025-03-25 20:51:28,942 - log
        - i_lrs : 0.00025 - 2025-03-25 20:51:28,942 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:51:28,942 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:51:28,942 - log
        - train_batch_size : 2 - 2025-03-25 20:51:28,942 - log
        - valid_batch_size : 1 - 2025-03-25 20:51:28,942 - log
        - grad_acc : 2 - 2025-03-25 20:51:28,942 - log
        - clip : 0.0 - 2025-03-25 20:51:28,942 - log
        - seq_len : 64 - 2025-03-25 20:51:28,942 - log
        - model_card : gpt2.sm - 2025-03-25 20:51:28,942 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:51:28,942 - log
        - fp16 : False - 2025-03-25 20:51:28,942 - log
        - log_interval : 100 - 2025-03-25 20:51:28,942 - log
        - eval_interval : 2000 - 2025-03-25 20:51:28,942 - log
        - save_interval : 1000 - 2025-03-25 20:51:28,942 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:51:28,942 - log
        - lora_dim : 4 - 2025-03-25 20:51:28,942 - log
        - lora_alpha : 32 - 2025-03-25 20:51:28,942 - log
        - obj : clm - 2025-03-25 20:51:28,942 - log
        - lora_dropout : 0.1 - 2025-03-25 20:51:28,942 - log
        - label_smooth : 0.1 - 2025-03-25 20:51:28,942 - log
        - roll_interval : -1 - 2025-03-25 20:51:28,942 - log
        - roll_lr : 1e-05 - 2025-03-25 20:51:28,942 - log
        - roll_step : 100 - 2025-03-25 20:51:28,942 - log
        - eval_epoch : 1 - 2025-03-25 20:51:28,942 - log
        - device : cuda - 2025-03-25 20:51:28,942 - log
==================================================================================================== - 2025-03-25 20:51:28,942 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:51:28,942 - log
loading model pretrained weight. - 2025-03-25 20:51:29,048 - log
set max_step: 1050 - 2025-03-25 20:51:29,822 - log
start to train the model................ 1 - 2025-03-25 20:51:29,822 - log
==================================================================================================== - 2025-03-25 20:51:58,882 - log
        - random_seed : 2025 - 2025-03-25 20:51:58,882 - log
        - lr : 0.0002 - 2025-03-25 20:51:58,882 - log
        - weight_decay : 0.01 - 2025-03-25 20:51:58,882 - log
        - correct_bias : False - 2025-03-25 20:51:58,882 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:51:58,882 - log
        - no_decay_bias : False - 2025-03-25 20:51:58,882 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:51:58,882 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:51:58,882 - log
        - scheduler : linear - 2025-03-25 20:51:58,882 - log
        - max_step : None - 2025-03-25 20:51:58,882 - log
        - max_epoch : 5 - 2025-03-25 20:51:58,882 - log
        - warmup_step : 500 - 2025-03-25 20:51:58,882 - log
        - i_steps : 0 - 2025-03-25 20:51:58,882 - log
        - i_lrs : 0.00025 - 2025-03-25 20:51:58,882 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:51:58,882 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:51:58,882 - log
        - train_batch_size : 2 - 2025-03-25 20:51:58,882 - log
        - valid_batch_size : 1 - 2025-03-25 20:51:58,882 - log
        - grad_acc : 2 - 2025-03-25 20:51:58,882 - log
        - clip : 0.0 - 2025-03-25 20:51:58,882 - log
        - seq_len : 64 - 2025-03-25 20:51:58,882 - log
        - model_card : gpt2.sm - 2025-03-25 20:51:58,882 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:51:58,882 - log
        - fp16 : False - 2025-03-25 20:51:58,882 - log
        - log_interval : 100 - 2025-03-25 20:51:58,882 - log
        - eval_interval : 2000 - 2025-03-25 20:51:58,882 - log
        - save_interval : 1000 - 2025-03-25 20:51:58,882 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:51:58,882 - log
        - lora_dim : 4 - 2025-03-25 20:51:58,882 - log
        - lora_alpha : 32 - 2025-03-25 20:51:58,882 - log
        - obj : clm - 2025-03-25 20:51:58,882 - log
        - lora_dropout : 0.1 - 2025-03-25 20:51:58,882 - log
        - label_smooth : 0.1 - 2025-03-25 20:51:58,882 - log
        - roll_interval : -1 - 2025-03-25 20:51:58,882 - log
        - roll_lr : 1e-05 - 2025-03-25 20:51:58,882 - log
        - roll_step : 100 - 2025-03-25 20:51:58,882 - log
        - eval_epoch : 1 - 2025-03-25 20:51:58,882 - log
        - device : cuda - 2025-03-25 20:51:58,882 - log
==================================================================================================== - 2025-03-25 20:51:58,882 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:51:58,882 - log
loading model pretrained weight. - 2025-03-25 20:51:58,991 - log
set max_step: 1050 - 2025-03-25 20:51:59,768 - log
start to train the model................ 1 - 2025-03-25 20:51:59,768 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 38.00 | loss 15.19 | avg loss 16.53 | ppl 15075301.87 - 2025-03-25 20:52:03,569 - log
==================================================================================================== - 2025-03-25 20:54:17,537 - log
        - random_seed : 2025 - 2025-03-25 20:54:17,537 - log
        - lr : 0.0002 - 2025-03-25 20:54:17,538 - log
        - weight_decay : 0.01 - 2025-03-25 20:54:17,538 - log
        - correct_bias : False - 2025-03-25 20:54:17,538 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:54:17,538 - log
        - no_decay_bias : False - 2025-03-25 20:54:17,538 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:54:17,538 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:54:17,538 - log
        - scheduler : linear - 2025-03-25 20:54:17,538 - log
        - max_step : None - 2025-03-25 20:54:17,538 - log
        - max_epoch : 5 - 2025-03-25 20:54:17,538 - log
        - warmup_step : 500 - 2025-03-25 20:54:17,538 - log
        - i_steps : 0 - 2025-03-25 20:54:17,538 - log
        - i_lrs : 0.00025 - 2025-03-25 20:54:17,538 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:54:17,538 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:54:17,538 - log
        - train_batch_size : 2 - 2025-03-25 20:54:17,538 - log
        - valid_batch_size : 1 - 2025-03-25 20:54:17,538 - log
        - grad_acc : 2 - 2025-03-25 20:54:17,538 - log
        - clip : 0.0 - 2025-03-25 20:54:17,538 - log
        - seq_len : 64 - 2025-03-25 20:54:17,538 - log
        - model_card : gpt2.sm - 2025-03-25 20:54:17,538 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:54:17,538 - log
        - fp16 : False - 2025-03-25 20:54:17,538 - log
        - log_interval : 100 - 2025-03-25 20:54:17,538 - log
        - eval_interval : 2000 - 2025-03-25 20:54:17,538 - log
        - save_interval : 1000 - 2025-03-25 20:54:17,538 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:54:17,538 - log
        - lora_dim : 4 - 2025-03-25 20:54:17,538 - log
        - lora_alpha : 32 - 2025-03-25 20:54:17,538 - log
        - obj : clm - 2025-03-25 20:54:17,538 - log
        - lora_dropout : 0.1 - 2025-03-25 20:54:17,538 - log
        - label_smooth : 0.1 - 2025-03-25 20:54:17,538 - log
        - roll_interval : -1 - 2025-03-25 20:54:17,538 - log
        - roll_lr : 1e-05 - 2025-03-25 20:54:17,538 - log
        - roll_step : 100 - 2025-03-25 20:54:17,538 - log
        - eval_epoch : 1 - 2025-03-25 20:54:17,538 - log
        - device : cuda - 2025-03-25 20:54:17,538 - log
==================================================================================================== - 2025-03-25 20:54:17,538 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:54:17,538 - log
loading model pretrained weight. - 2025-03-25 20:54:17,649 - log
set max_step: 1050 - 2025-03-25 20:54:18,430 - log
start to train the model................ 1 - 2025-03-25 20:54:18,430 - log
==================================================================================================== - 2025-03-25 21:41:16,593 - log
        - random_seed : 2025 - 2025-03-25 21:41:16,593 - log
        - lr : 0.0002 - 2025-03-25 21:41:16,593 - log
        - weight_decay : 0.01 - 2025-03-25 21:41:16,593 - log
        - correct_bias : False - 2025-03-25 21:41:16,593 - log
        - adam_epislon : 1e-06 - 2025-03-25 21:41:16,593 - log
        - no_decay_bias : False - 2025-03-25 21:41:16,593 - log
        - adam_beta1 : 0.9 - 2025-03-25 21:41:16,593 - log
        - adam_beta2 : 0.999 - 2025-03-25 21:41:16,593 - log
        - scheduler : linear - 2025-03-25 21:41:16,593 - log
        - max_step : None - 2025-03-25 21:41:16,593 - log
        - max_epoch : 5 - 2025-03-25 21:41:16,593 - log
        - warmup_step : 500 - 2025-03-25 21:41:16,593 - log
        - i_steps : 0 - 2025-03-25 21:41:16,593 - log
        - i_lrs : 0.00025 - 2025-03-25 21:41:16,593 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 21:41:16,593 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 21:41:16,593 - log
        - train_batch_size : 2 - 2025-03-25 21:41:16,593 - log
        - valid_batch_size : 1 - 2025-03-25 21:41:16,593 - log
        - grad_acc : 2 - 2025-03-25 21:41:16,593 - log
        - clip : 0.0 - 2025-03-25 21:41:16,593 - log
        - seq_len : 64 - 2025-03-25 21:41:16,593 - log
        - model_card : gpt2.sm - 2025-03-25 21:41:16,593 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 21:41:16,593 - log
        - fp16 : False - 2025-03-25 21:41:16,593 - log
        - log_interval : 100 - 2025-03-25 21:41:16,593 - log
        - eval_interval : 2000 - 2025-03-25 21:41:16,593 - log
        - save_interval : 1000 - 2025-03-25 21:41:16,593 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 21:41:16,594 - log
        - lora_dim : 4 - 2025-03-25 21:41:16,594 - log
        - lora_alpha : 32 - 2025-03-25 21:41:16,594 - log
        - obj : clm - 2025-03-25 21:41:16,594 - log
        - lora_dropout : 0.1 - 2025-03-25 21:41:16,594 - log
        - label_smooth : 0.1 - 2025-03-25 21:41:16,594 - log
        - roll_interval : -1 - 2025-03-25 21:41:16,594 - log
        - roll_lr : 1e-05 - 2025-03-25 21:41:16,594 - log
        - roll_step : 100 - 2025-03-25 21:41:16,594 - log
        - eval_epoch : 1 - 2025-03-25 21:41:16,594 - log
        - device : cuda - 2025-03-25 21:41:16,594 - log
==================================================================================================== - 2025-03-25 21:41:16,594 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 21:41:16,594 - log
loading model pretrained weight. - 2025-03-25 21:41:16,708 - log
set max_step: 1050 - 2025-03-25 21:41:18,299 - log
start to train the model................ 1 - 2025-03-25 21:41:18,299 - log
==================================================================================================== - 2025-03-25 21:41:45,352 - log
        - random_seed : 2025 - 2025-03-25 21:41:45,352 - log
        - lr : 0.0002 - 2025-03-25 21:41:45,352 - log
        - weight_decay : 0.01 - 2025-03-25 21:41:45,352 - log
        - correct_bias : False - 2025-03-25 21:41:45,352 - log
        - adam_epislon : 1e-06 - 2025-03-25 21:41:45,352 - log
        - no_decay_bias : False - 2025-03-25 21:41:45,352 - log
        - adam_beta1 : 0.9 - 2025-03-25 21:41:45,352 - log
        - adam_beta2 : 0.999 - 2025-03-25 21:41:45,352 - log
        - scheduler : linear - 2025-03-25 21:41:45,352 - log
        - max_step : None - 2025-03-25 21:41:45,352 - log
        - max_epoch : 5 - 2025-03-25 21:41:45,352 - log
        - warmup_step : 500 - 2025-03-25 21:41:45,352 - log
        - i_steps : 0 - 2025-03-25 21:41:45,352 - log
        - i_lrs : 0.00025 - 2025-03-25 21:41:45,352 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 21:41:45,352 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 21:41:45,352 - log
        - train_batch_size : 2 - 2025-03-25 21:41:45,352 - log
        - valid_batch_size : 1 - 2025-03-25 21:41:45,352 - log
        - grad_acc : 2 - 2025-03-25 21:41:45,352 - log
        - clip : 0.0 - 2025-03-25 21:41:45,352 - log
        - seq_len : 64 - 2025-03-25 21:41:45,352 - log
        - model_card : gpt2.sm - 2025-03-25 21:41:45,352 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 21:41:45,352 - log
        - fp16 : False - 2025-03-25 21:41:45,352 - log
        - log_interval : 100 - 2025-03-25 21:41:45,352 - log
        - eval_interval : 2000 - 2025-03-25 21:41:45,352 - log
        - save_interval : 1000 - 2025-03-25 21:41:45,352 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 21:41:45,352 - log
        - lora_dim : 4 - 2025-03-25 21:41:45,352 - log
        - lora_alpha : 32 - 2025-03-25 21:41:45,352 - log
        - obj : clm - 2025-03-25 21:41:45,352 - log
        - lora_dropout : 0.1 - 2025-03-25 21:41:45,352 - log
        - label_smooth : 0.1 - 2025-03-25 21:41:45,352 - log
        - roll_interval : -1 - 2025-03-25 21:41:45,352 - log
        - roll_lr : 1e-05 - 2025-03-25 21:41:45,352 - log
        - roll_step : 100 - 2025-03-25 21:41:45,352 - log
        - eval_epoch : 1 - 2025-03-25 21:41:45,352 - log
        - device : cuda - 2025-03-25 21:41:45,352 - log
==================================================================================================== - 2025-03-25 21:41:45,352 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 21:41:45,352 - log
loading model pretrained weight. - 2025-03-25 21:41:45,459 - log
set max_step: 1050 - 2025-03-25 21:41:46,233 - log
start to train the model................ 1 - 2025-03-25 21:41:46,234 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 137.77 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-25 21:42:00,010 - log
==================================================================================================== - 2025-03-25 21:42:36,455 - log
        - random_seed : 2025 - 2025-03-25 21:42:36,456 - log
        - lr : 0.0002 - 2025-03-25 21:42:36,456 - log
        - weight_decay : 0.01 - 2025-03-25 21:42:36,456 - log
        - correct_bias : False - 2025-03-25 21:42:36,456 - log
        - adam_epislon : 1e-06 - 2025-03-25 21:42:36,456 - log
        - no_decay_bias : False - 2025-03-25 21:42:36,456 - log
        - adam_beta1 : 0.9 - 2025-03-25 21:42:36,456 - log
        - adam_beta2 : 0.999 - 2025-03-25 21:42:36,456 - log
        - scheduler : linear - 2025-03-25 21:42:36,456 - log
        - max_step : None - 2025-03-25 21:42:36,456 - log
        - max_epoch : 5 - 2025-03-25 21:42:36,456 - log
        - warmup_step : 500 - 2025-03-25 21:42:36,456 - log
        - i_steps : 0 - 2025-03-25 21:42:36,456 - log
        - i_lrs : 0.00025 - 2025-03-25 21:42:36,456 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 21:42:36,456 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 21:42:36,456 - log
        - train_batch_size : 2 - 2025-03-25 21:42:36,456 - log
        - valid_batch_size : 1 - 2025-03-25 21:42:36,456 - log
        - grad_acc : 2 - 2025-03-25 21:42:36,456 - log
        - clip : 0.0 - 2025-03-25 21:42:36,456 - log
        - seq_len : 64 - 2025-03-25 21:42:36,456 - log
        - model_card : gpt2.sm - 2025-03-25 21:42:36,456 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 21:42:36,456 - log
        - fp16 : False - 2025-03-25 21:42:36,456 - log
        - log_interval : 100 - 2025-03-25 21:42:36,456 - log
        - eval_interval : 2000 - 2025-03-25 21:42:36,456 - log
        - save_interval : 1000 - 2025-03-25 21:42:36,456 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 21:42:36,456 - log
        - lora_dim : 4 - 2025-03-25 21:42:36,456 - log
        - lora_alpha : 32 - 2025-03-25 21:42:36,456 - log
        - obj : clm - 2025-03-25 21:42:36,456 - log
        - lora_dropout : 0.1 - 2025-03-25 21:42:36,456 - log
        - label_smooth : 0.1 - 2025-03-25 21:42:36,456 - log
        - roll_interval : -1 - 2025-03-25 21:42:36,456 - log
        - roll_lr : 1e-05 - 2025-03-25 21:42:36,456 - log
        - roll_step : 100 - 2025-03-25 21:42:36,456 - log
        - eval_epoch : 1 - 2025-03-25 21:42:36,456 - log
        - device : cuda - 2025-03-25 21:42:36,456 - log
==================================================================================================== - 2025-03-25 21:42:36,456 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 21:42:36,456 - log
loading model pretrained weight. - 2025-03-25 21:42:36,563 - log
set max_step: 1050 - 2025-03-25 21:42:37,422 - log
start to train the model................ 1 - 2025-03-25 21:42:37,423 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.52 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-25 21:42:40,874 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.19 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-25 21:42:44,093 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 21:42:44,415 - log
start to train the model................ 2 - 2025-03-25 21:42:45,804 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.25 | loss  3.23 | avg loss  3.57 | ppl 35.69 - 2025-03-25 21:42:48,730 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.33 | loss  4.11 | avg loss  3.40 | ppl 30.03 - 2025-03-25 21:42:51,963 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 21:42:52,655 - log
start to train the model................ 3 - 2025-03-25 21:42:54,116 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.35 | loss  3.38 | avg loss  3.23 | ppl 25.37 - 2025-03-25 21:42:56,751 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 33.52 | loss  3.04 | avg loss  3.20 | ppl 24.49 - 2025-03-25 21:43:00,103 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 21:43:01,265 - log
start to train the model................ 4 - 2025-03-25 21:43:02,657 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 49.39 | loss  3.46 | avg loss  3.15 | ppl 23.22 - 2025-03-25 21:43:07,596 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 61.31 | loss  3.04 | avg loss  3.01 | ppl 20.38 - 2025-03-25 21:43:13,727 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 21:43:16,505 - log
start to train the model................ 5 - 2025-03-25 21:43:17,903 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 36.00 | loss  2.47 | avg loss  3.04 | ppl 20.95 - 2025-03-25 21:43:21,503 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 69.75 | loss  3.83 | avg loss  3.01 | ppl 20.36 - 2025-03-25 21:43:28,478 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 21:43:28,478 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 21:43:31,275 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 21:43:32,717 - log
End of training - 2025-03-25 21:43:32,717 - log
==================================================================================================== - 2025-03-25 21:44:18,478 - log
        - random_seed : 2025 - 2025-03-25 21:44:18,478 - log
        - lr : 0.0002 - 2025-03-25 21:44:18,478 - log
        - weight_decay : 0.01 - 2025-03-25 21:44:18,478 - log
        - correct_bias : False - 2025-03-25 21:44:18,478 - log
        - adam_epislon : 1e-06 - 2025-03-25 21:44:18,478 - log
        - no_decay_bias : False - 2025-03-25 21:44:18,478 - log
        - adam_beta1 : 0.9 - 2025-03-25 21:44:18,478 - log
        - adam_beta2 : 0.999 - 2025-03-25 21:44:18,478 - log
        - scheduler : linear - 2025-03-25 21:44:18,478 - log
        - max_step : None - 2025-03-25 21:44:18,478 - log
        - max_epoch : 5 - 2025-03-25 21:44:18,478 - log
        - warmup_step : 500 - 2025-03-25 21:44:18,478 - log
        - i_steps : 0 - 2025-03-25 21:44:18,478 - log
        - i_lrs : 0.00025 - 2025-03-25 21:44:18,478 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 21:44:18,478 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 21:44:18,478 - log
        - train_batch_size : 2 - 2025-03-25 21:44:18,478 - log
        - valid_batch_size : 1 - 2025-03-25 21:44:18,478 - log
        - grad_acc : 2 - 2025-03-25 21:44:18,478 - log
        - clip : 0.0 - 2025-03-25 21:44:18,478 - log
        - seq_len : 64 - 2025-03-25 21:44:18,478 - log
        - model_card : gpt2.sm - 2025-03-25 21:44:18,478 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 21:44:18,478 - log
        - fp16 : False - 2025-03-25 21:44:18,478 - log
        - log_interval : 100 - 2025-03-25 21:44:18,478 - log
        - eval_interval : 2000 - 2025-03-25 21:44:18,478 - log
        - save_interval : 1000 - 2025-03-25 21:44:18,478 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 21:44:18,478 - log
        - lora_dim : 4 - 2025-03-25 21:44:18,478 - log
        - lora_alpha : 32 - 2025-03-25 21:44:18,478 - log
        - obj : clm - 2025-03-25 21:44:18,478 - log
        - lora_dropout : 0.1 - 2025-03-25 21:44:18,479 - log
        - label_smooth : 0.1 - 2025-03-25 21:44:18,479 - log
        - roll_interval : -1 - 2025-03-25 21:44:18,479 - log
        - roll_lr : 1e-05 - 2025-03-25 21:44:18,479 - log
        - roll_step : 100 - 2025-03-25 21:44:18,479 - log
        - eval_epoch : 1 - 2025-03-25 21:44:18,479 - log
        - device : cuda - 2025-03-25 21:44:18,479 - log
==================================================================================================== - 2025-03-25 21:44:18,479 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 21:44:18,479 - log
loading model pretrained weight. - 2025-03-25 21:44:18,590 - log
set max_step: 1050 - 2025-03-25 21:44:20,306 - log
start to train the model................ 1 - 2025-03-25 21:44:20,306 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 36.03 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-25 21:44:23,909 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.57 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-25 21:44:27,166 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 21:44:27,493 - log
start to train the model................ 2 - 2025-03-25 21:44:28,967 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.37 | loss  3.23 | avg loss  3.57 | ppl 35.69 - 2025-03-25 21:44:31,905 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.68 | loss  4.11 | avg loss  3.40 | ppl 30.03 - 2025-03-25 21:44:35,172 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 21:44:35,828 - log
start to train the model................ 3 - 2025-03-25 21:44:37,238 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.15 | loss  3.38 | avg loss  3.23 | ppl 25.37 - 2025-03-25 21:44:39,853 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.82 | loss  3.04 | avg loss  3.20 | ppl 24.49 - 2025-03-25 21:44:43,136 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 21:44:44,119 - log
start to train the model................ 4 - 2025-03-25 21:44:45,591 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 23.00 | loss  3.46 | avg loss  3.15 | ppl 23.22 - 2025-03-25 21:44:47,891 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.87 | loss  3.04 | avg loss  3.01 | ppl 20.38 - 2025-03-25 21:44:51,178 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 21:44:52,498 - log
start to train the model................ 5 - 2025-03-25 21:44:53,926 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.73 | loss  2.47 | avg loss  3.04 | ppl 20.95 - 2025-03-25 21:44:55,899 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.95 | loss  3.83 | avg loss  3.01 | ppl 20.36 - 2025-03-25 21:44:59,194 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 21:44:59,194 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 21:45:00,843 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 21:45:02,274 - log
End of training - 2025-03-25 21:45:02,274 - log
==================================================================================================== - 2025-03-25 21:51:28,316 - log
        - random_seed : 2025 - 2025-03-25 21:51:28,316 - log
        - lr : 0.0002 - 2025-03-25 21:51:28,316 - log
        - weight_decay : 0.01 - 2025-03-25 21:51:28,316 - log
        - correct_bias : False - 2025-03-25 21:51:28,316 - log
        - adam_epislon : 1e-06 - 2025-03-25 21:51:28,316 - log
        - no_decay_bias : False - 2025-03-25 21:51:28,316 - log
        - adam_beta1 : 0.9 - 2025-03-25 21:51:28,316 - log
        - adam_beta2 : 0.999 - 2025-03-25 21:51:28,316 - log
        - scheduler : linear - 2025-03-25 21:51:28,316 - log
        - max_step : None - 2025-03-25 21:51:28,316 - log
        - max_epoch : 5 - 2025-03-25 21:51:28,316 - log
        - warmup_step : 500 - 2025-03-25 21:51:28,316 - log
        - i_steps : 0 - 2025-03-25 21:51:28,316 - log
        - i_lrs : 0.00025 - 2025-03-25 21:51:28,316 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 21:51:28,316 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 21:51:28,317 - log
        - train_batch_size : 2 - 2025-03-25 21:51:28,317 - log
        - valid_batch_size : 1 - 2025-03-25 21:51:28,317 - log
        - grad_acc : 2 - 2025-03-25 21:51:28,317 - log
        - clip : 0.0 - 2025-03-25 21:51:28,317 - log
        - seq_len : 64 - 2025-03-25 21:51:28,317 - log
        - model_card : gpt2.sm - 2025-03-25 21:51:28,317 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 21:51:28,317 - log
        - fp16 : False - 2025-03-25 21:51:28,317 - log
        - log_interval : 100 - 2025-03-25 21:51:28,317 - log
        - eval_interval : 2000 - 2025-03-25 21:51:28,317 - log
        - save_interval : 1000 - 2025-03-25 21:51:28,317 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 21:51:28,317 - log
        - lora_dim : 4 - 2025-03-25 21:51:28,317 - log
        - lora_alpha : 32 - 2025-03-25 21:51:28,317 - log
        - obj : clm - 2025-03-25 21:51:28,317 - log
        - lora_dropout : 0.1 - 2025-03-25 21:51:28,317 - log
        - label_smooth : 0.1 - 2025-03-25 21:51:28,317 - log
        - roll_interval : -1 - 2025-03-25 21:51:28,317 - log
        - roll_lr : 1e-05 - 2025-03-25 21:51:28,317 - log
        - roll_step : 100 - 2025-03-25 21:51:28,317 - log
        - eval_epoch : 1 - 2025-03-25 21:51:28,317 - log
        - device : cuda - 2025-03-25 21:51:28,317 - log
==================================================================================================== - 2025-03-25 21:51:28,317 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 21:51:28,317 - log
loading model pretrained weight. - 2025-03-25 21:51:28,424 - log
set max_step: 1050 - 2025-03-25 21:51:29,337 - log
start to train the model................ 1 - 2025-03-25 21:51:29,338 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 33.98 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-25 21:51:32,736 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 31.50 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-25 21:51:35,886 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 21:51:36,202 - log
start to train the model................ 2 - 2025-03-25 21:51:37,607 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 28.54 | loss  3.23 | avg loss  3.57 | ppl 35.69 - 2025-03-25 21:51:40,461 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 31.90 | loss  4.11 | avg loss  3.40 | ppl 30.03 - 2025-03-25 21:51:43,651 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 21:51:44,290 - log
start to train the model................ 3 - 2025-03-25 21:51:45,732 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 25.51 | loss  3.38 | avg loss  3.23 | ppl 25.37 - 2025-03-25 21:51:48,284 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.00 | loss  3.04 | avg loss  3.20 | ppl 24.49 - 2025-03-25 21:51:51,484 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 21:51:52,442 - log
start to train the model................ 4 - 2025-03-25 21:51:53,859 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.37 | loss  3.46 | avg loss  3.15 | ppl 23.22 - 2025-03-25 21:51:56,096 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.09 | loss  3.04 | avg loss  3.01 | ppl 20.38 - 2025-03-25 21:51:59,306 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 21:52:00,591 - log
start to train the model................ 5 - 2025-03-25 21:52:02,076 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.26 | loss  2.47 | avg loss  3.04 | ppl 20.95 - 2025-03-25 21:52:04,002 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.20 | loss  3.83 | avg loss  3.01 | ppl 20.36 - 2025-03-25 21:52:07,222 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 21:52:07,222 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 21:52:08,836 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 21:52:10,286 - log
End of training - 2025-03-25 21:52:10,286 - log
==================================================================================================== - 2025-03-26 13:37:42,726 - log
        - random_seed : 2025 - 2025-03-26 13:37:42,726 - log
        - lr : 0.0002 - 2025-03-26 13:37:42,726 - log
        - weight_decay : 0.01 - 2025-03-26 13:37:42,726 - log
        - correct_bias : False - 2025-03-26 13:37:42,726 - log
        - adam_epislon : 1e-06 - 2025-03-26 13:37:42,726 - log
        - no_decay_bias : False - 2025-03-26 13:37:42,727 - log
        - adam_beta1 : 0.9 - 2025-03-26 13:37:42,727 - log
        - adam_beta2 : 0.999 - 2025-03-26 13:37:42,727 - log
        - scheduler : linear - 2025-03-26 13:37:42,727 - log
        - max_step : None - 2025-03-26 13:37:42,727 - log
        - max_epoch : 5 - 2025-03-26 13:37:42,727 - log
        - warmup_step : 500 - 2025-03-26 13:37:42,727 - log
        - i_steps : 0 - 2025-03-26 13:37:42,727 - log
        - i_lrs : 0.00025 - 2025-03-26 13:37:42,727 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-26 13:37:42,727 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-26 13:37:42,727 - log
        - train_batch_size : 2 - 2025-03-26 13:37:42,727 - log
        - valid_batch_size : 1 - 2025-03-26 13:37:42,727 - log
        - grad_acc : 2 - 2025-03-26 13:37:42,727 - log
        - clip : 0.0 - 2025-03-26 13:37:42,727 - log
        - seq_len : 64 - 2025-03-26 13:37:42,727 - log
        - model_card : gpt2.sm - 2025-03-26 13:37:42,727 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-26 13:37:42,727 - log
        - fp16 : False - 2025-03-26 13:37:42,727 - log
        - log_interval : 100 - 2025-03-26 13:37:42,727 - log
        - eval_interval : 2000 - 2025-03-26 13:37:42,727 - log
        - save_interval : 1000 - 2025-03-26 13:37:42,727 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-26 13:37:42,727 - log
        - lora_dim : 4 - 2025-03-26 13:37:42,727 - log
        - lora_alpha : 32 - 2025-03-26 13:37:42,727 - log
        - obj : clm - 2025-03-26 13:37:42,727 - log
        - lora_dropout : 0.1 - 2025-03-26 13:37:42,727 - log
        - label_smooth : 0.1 - 2025-03-26 13:37:42,727 - log
        - roll_interval : -1 - 2025-03-26 13:37:42,727 - log
        - roll_lr : 1e-05 - 2025-03-26 13:37:42,727 - log
        - roll_step : 100 - 2025-03-26 13:37:42,727 - log
        - eval_epoch : 1 - 2025-03-26 13:37:42,727 - log
        - device : cuda - 2025-03-26 13:37:42,727 - log
==================================================================================================== - 2025-03-26 13:37:42,727 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-26 13:37:42,727 - log
loading model pretrained weight. - 2025-03-26 13:37:54,062 - log
==================================================================================================== - 2025-03-26 13:51:36,113 - log
        - random_seed : 2025 - 2025-03-26 13:51:36,113 - log
        - lr : 0.0002 - 2025-03-26 13:51:36,113 - log
        - weight_decay : 0.01 - 2025-03-26 13:51:36,113 - log
        - correct_bias : False - 2025-03-26 13:51:36,113 - log
        - adam_epislon : 1e-06 - 2025-03-26 13:51:36,113 - log
        - no_decay_bias : False - 2025-03-26 13:51:36,113 - log
        - adam_beta1 : 0.9 - 2025-03-26 13:51:36,113 - log
        - adam_beta2 : 0.999 - 2025-03-26 13:51:36,113 - log
        - scheduler : linear - 2025-03-26 13:51:36,113 - log
        - max_step : None - 2025-03-26 13:51:36,113 - log
        - max_epoch : 5 - 2025-03-26 13:51:36,113 - log
        - warmup_step : 500 - 2025-03-26 13:51:36,113 - log
        - i_steps : 0 - 2025-03-26 13:51:36,113 - log
        - i_lrs : 0.00025 - 2025-03-26 13:51:36,113 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-26 13:51:36,113 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-26 13:51:36,113 - log
        - train_batch_size : 2 - 2025-03-26 13:51:36,113 - log
        - valid_batch_size : 1 - 2025-03-26 13:51:36,113 - log
        - grad_acc : 2 - 2025-03-26 13:51:36,113 - log
        - clip : 0.0 - 2025-03-26 13:51:36,113 - log
        - seq_len : 64 - 2025-03-26 13:51:36,113 - log
        - model_card : gpt2.sm - 2025-03-26 13:51:36,113 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-26 13:51:36,113 - log
        - fp16 : False - 2025-03-26 13:51:36,113 - log
        - log_interval : 100 - 2025-03-26 13:51:36,113 - log
        - eval_interval : 2000 - 2025-03-26 13:51:36,113 - log
        - save_interval : 1000 - 2025-03-26 13:51:36,113 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-26 13:51:36,113 - log
        - lora_dim : 4 - 2025-03-26 13:51:36,113 - log
        - lora_alpha : 32 - 2025-03-26 13:51:36,113 - log
        - obj : clm - 2025-03-26 13:51:36,113 - log
        - lora_dropout : 0.1 - 2025-03-26 13:51:36,113 - log
        - label_smooth : 0.1 - 2025-03-26 13:51:36,113 - log
        - roll_interval : -1 - 2025-03-26 13:51:36,113 - log
        - roll_lr : 1e-05 - 2025-03-26 13:51:36,113 - log
        - roll_step : 100 - 2025-03-26 13:51:36,113 - log
        - eval_epoch : 1 - 2025-03-26 13:51:36,113 - log
        - device : cuda - 2025-03-26 13:51:36,113 - log
==================================================================================================== - 2025-03-26 13:51:36,113 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-26 13:51:36,113 - log
loading model pretrained weight. - 2025-03-26 13:51:36,218 - log
set max_step: 1050 - 2025-03-26 13:51:43,671 - log
start to train the model................ 1 - 2025-03-26 13:51:43,671 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 760.97 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-26 13:52:59,768 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.33 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-26 13:53:03,001 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-26 13:53:03,323 - log
==================================================================================================== - 2025-03-26 14:04:07,954 - log
        - random_seed : 2025 - 2025-03-26 14:04:07,954 - log
        - lr : 0.0002 - 2025-03-26 14:04:07,954 - log
        - weight_decay : 0.01 - 2025-03-26 14:04:07,954 - log
        - correct_bias : False - 2025-03-26 14:04:07,954 - log
        - adam_epislon : 1e-06 - 2025-03-26 14:04:07,954 - log
        - no_decay_bias : False - 2025-03-26 14:04:07,954 - log
        - adam_beta1 : 0.9 - 2025-03-26 14:04:07,954 - log
        - adam_beta2 : 0.999 - 2025-03-26 14:04:07,954 - log
        - scheduler : linear - 2025-03-26 14:04:07,954 - log
        - max_step : None - 2025-03-26 14:04:07,954 - log
        - max_epoch : 5 - 2025-03-26 14:04:07,954 - log
        - warmup_step : 500 - 2025-03-26 14:04:07,954 - log
        - i_steps : 0 - 2025-03-26 14:04:07,954 - log
        - i_lrs : 0.00025 - 2025-03-26 14:04:07,954 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-26 14:04:07,954 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-26 14:04:07,954 - log
        - train_batch_size : 2 - 2025-03-26 14:04:07,954 - log
        - valid_batch_size : 1 - 2025-03-26 14:04:07,954 - log
        - grad_acc : 2 - 2025-03-26 14:04:07,954 - log
        - clip : 0.0 - 2025-03-26 14:04:07,954 - log
        - seq_len : 64 - 2025-03-26 14:04:07,954 - log
        - model_card : gpt2.sm - 2025-03-26 14:04:07,954 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-26 14:04:07,954 - log
        - fp16 : False - 2025-03-26 14:04:07,954 - log
        - log_interval : 100 - 2025-03-26 14:04:07,954 - log
        - eval_interval : 2000 - 2025-03-26 14:04:07,954 - log
        - save_interval : 1000 - 2025-03-26 14:04:07,954 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-26 14:04:07,954 - log
        - lora_dim : 4 - 2025-03-26 14:04:07,954 - log
        - lora_alpha : 32 - 2025-03-26 14:04:07,954 - log
        - obj : clm - 2025-03-26 14:04:07,954 - log
        - lora_dropout : 0.1 - 2025-03-26 14:04:07,954 - log
        - label_smooth : 0.1 - 2025-03-26 14:04:07,954 - log
        - roll_interval : -1 - 2025-03-26 14:04:07,954 - log
        - roll_lr : 1e-05 - 2025-03-26 14:04:07,954 - log
        - roll_step : 100 - 2025-03-26 14:04:07,954 - log
        - eval_epoch : 1 - 2025-03-26 14:04:07,954 - log
        - device : cuda - 2025-03-26 14:04:07,954 - log
==================================================================================================== - 2025-03-26 14:04:07,954 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-26 14:04:07,954 - log
loading model pretrained weight. - 2025-03-26 14:04:08,057 - log
set max_step: 1050 - 2025-03-26 14:04:08,832 - log
start to train the model................ 1 - 2025-03-26 14:04:08,832 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.15 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-26 14:04:12,247 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 31.76 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-26 14:04:15,423 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-26 14:04:15,741 - log
==================================================================================================== - 2025-03-26 14:16:58,593 - log
        - random_seed : 2025 - 2025-03-26 14:16:58,593 - log
        - lr : 0.0002 - 2025-03-26 14:16:58,593 - log
        - weight_decay : 0.01 - 2025-03-26 14:16:58,593 - log
        - correct_bias : False - 2025-03-26 14:16:58,593 - log
        - adam_epislon : 1e-06 - 2025-03-26 14:16:58,593 - log
        - no_decay_bias : False - 2025-03-26 14:16:58,593 - log
        - adam_beta1 : 0.9 - 2025-03-26 14:16:58,593 - log
        - adam_beta2 : 0.999 - 2025-03-26 14:16:58,593 - log
        - scheduler : linear - 2025-03-26 14:16:58,593 - log
        - max_step : None - 2025-03-26 14:16:58,593 - log
        - max_epoch : 5 - 2025-03-26 14:16:58,593 - log
        - warmup_step : 500 - 2025-03-26 14:16:58,593 - log
        - i_steps : 0 - 2025-03-26 14:16:58,593 - log
        - i_lrs : 0.00025 - 2025-03-26 14:16:58,593 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-26 14:16:58,593 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-26 14:16:58,593 - log
        - train_batch_size : 2 - 2025-03-26 14:16:58,593 - log
        - valid_batch_size : 1 - 2025-03-26 14:16:58,593 - log
        - grad_acc : 2 - 2025-03-26 14:16:58,593 - log
        - clip : 0.0 - 2025-03-26 14:16:58,593 - log
        - seq_len : 64 - 2025-03-26 14:16:58,593 - log
        - model_card : gpt2.sm - 2025-03-26 14:16:58,593 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-26 14:16:58,593 - log
        - fp16 : False - 2025-03-26 14:16:58,593 - log
        - log_interval : 100 - 2025-03-26 14:16:58,593 - log
        - eval_interval : 2000 - 2025-03-26 14:16:58,593 - log
        - save_interval : 1000 - 2025-03-26 14:16:58,593 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-26 14:16:58,593 - log
        - lora_dim : 4 - 2025-03-26 14:16:58,593 - log
        - lora_alpha : 32 - 2025-03-26 14:16:58,593 - log
        - obj : clm - 2025-03-26 14:16:58,593 - log
        - lora_dropout : 0.1 - 2025-03-26 14:16:58,593 - log
        - label_smooth : 0.1 - 2025-03-26 14:16:58,593 - log
        - roll_interval : -1 - 2025-03-26 14:16:58,593 - log
        - roll_lr : 1e-05 - 2025-03-26 14:16:58,593 - log
        - roll_step : 100 - 2025-03-26 14:16:58,593 - log
        - eval_epoch : 1 - 2025-03-26 14:16:58,593 - log
        - device : cuda - 2025-03-26 14:16:58,593 - log
==================================================================================================== - 2025-03-26 14:16:58,593 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-26 14:16:58,593 - log
loading model pretrained weight. - 2025-03-26 14:16:58,712 - log
set max_step: 1050 - 2025-03-26 14:16:59,709 - log
start to train the model................ 1 - 2025-03-26 14:16:59,709 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.22 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-26 14:17:03,131 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.04 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-26 14:17:06,335 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210 - 2025-03-26 14:17:06,656 - log
==================================================================================================== - 2025-03-26 14:23:41,412 - log
        - random_seed : 2025 - 2025-03-26 14:23:41,412 - log
        - lr : 0.0002 - 2025-03-26 14:23:41,412 - log
        - weight_decay : 0.01 - 2025-03-26 14:23:41,412 - log
        - correct_bias : False - 2025-03-26 14:23:41,412 - log
        - adam_epislon : 1e-06 - 2025-03-26 14:23:41,412 - log
        - no_decay_bias : False - 2025-03-26 14:23:41,412 - log
        - adam_beta1 : 0.9 - 2025-03-26 14:23:41,412 - log
        - adam_beta2 : 0.999 - 2025-03-26 14:23:41,412 - log
        - scheduler : linear - 2025-03-26 14:23:41,412 - log
        - max_step : None - 2025-03-26 14:23:41,412 - log
        - max_epoch : 5 - 2025-03-26 14:23:41,412 - log
        - warmup_step : 500 - 2025-03-26 14:23:41,412 - log
        - i_steps : 0 - 2025-03-26 14:23:41,412 - log
        - i_lrs : 0.00025 - 2025-03-26 14:23:41,412 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-26 14:23:41,412 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-26 14:23:41,412 - log
        - train_batch_size : 2 - 2025-03-26 14:23:41,412 - log
        - valid_batch_size : 1 - 2025-03-26 14:23:41,412 - log
        - grad_acc : 2 - 2025-03-26 14:23:41,412 - log
        - clip : 0.0 - 2025-03-26 14:23:41,412 - log
        - seq_len : 64 - 2025-03-26 14:23:41,412 - log
        - model_card : gpt2.sm - 2025-03-26 14:23:41,412 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-26 14:23:41,412 - log
        - fp16 : False - 2025-03-26 14:23:41,412 - log
        - log_interval : 100 - 2025-03-26 14:23:41,412 - log
        - eval_interval : 2000 - 2025-03-26 14:23:41,412 - log
        - save_interval : 1000 - 2025-03-26 14:23:41,412 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-26 14:23:41,412 - log
        - lora_dim : 4 - 2025-03-26 14:23:41,412 - log
        - lora_alpha : 32 - 2025-03-26 14:23:41,412 - log
        - obj : clm - 2025-03-26 14:23:41,412 - log
        - lora_dropout : 0.1 - 2025-03-26 14:23:41,412 - log
        - label_smooth : 0.1 - 2025-03-26 14:23:41,412 - log
        - roll_interval : -1 - 2025-03-26 14:23:41,412 - log
        - roll_lr : 1e-05 - 2025-03-26 14:23:41,412 - log
        - roll_step : 100 - 2025-03-26 14:23:41,412 - log
        - eval_epoch : 1 - 2025-03-26 14:23:41,412 - log
        - device : cuda - 2025-03-26 14:23:41,412 - log
==================================================================================================== - 2025-03-26 14:23:41,412 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-26 14:23:41,412 - log
loading model pretrained weight. - 2025-03-26 14:23:41,515 - log
set max_step: 1050 - 2025-03-26 14:23:42,279 - log
start to train the model................ 1 - 2025-03-26 14:23:42,279 - log
==================================================================================================== - 2025-03-26 14:24:40,035 - log
        - random_seed : 2025 - 2025-03-26 14:24:40,035 - log
        - lr : 0.0002 - 2025-03-26 14:24:40,035 - log
        - weight_decay : 0.01 - 2025-03-26 14:24:40,035 - log
        - correct_bias : False - 2025-03-26 14:24:40,035 - log
        - adam_epislon : 1e-06 - 2025-03-26 14:24:40,035 - log
        - no_decay_bias : False - 2025-03-26 14:24:40,035 - log
        - adam_beta1 : 0.9 - 2025-03-26 14:24:40,035 - log
        - adam_beta2 : 0.999 - 2025-03-26 14:24:40,035 - log
        - scheduler : linear - 2025-03-26 14:24:40,035 - log
        - max_step : None - 2025-03-26 14:24:40,035 - log
        - max_epoch : 5 - 2025-03-26 14:24:40,035 - log
        - warmup_step : 500 - 2025-03-26 14:24:40,035 - log
        - i_steps : 0 - 2025-03-26 14:24:40,035 - log
        - i_lrs : 0.00025 - 2025-03-26 14:24:40,035 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-26 14:24:40,035 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-26 14:24:40,035 - log
        - train_batch_size : 2 - 2025-03-26 14:24:40,035 - log
        - valid_batch_size : 1 - 2025-03-26 14:24:40,035 - log
        - grad_acc : 2 - 2025-03-26 14:24:40,035 - log
        - clip : 0.0 - 2025-03-26 14:24:40,035 - log
        - seq_len : 64 - 2025-03-26 14:24:40,035 - log
        - model_card : gpt2.sm - 2025-03-26 14:24:40,035 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-26 14:24:40,035 - log
        - fp16 : False - 2025-03-26 14:24:40,035 - log
        - log_interval : 100 - 2025-03-26 14:24:40,035 - log
        - eval_interval : 2000 - 2025-03-26 14:24:40,035 - log
        - save_interval : 1000 - 2025-03-26 14:24:40,035 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-26 14:24:40,035 - log
        - lora_dim : 4 - 2025-03-26 14:24:40,035 - log
        - lora_alpha : 32 - 2025-03-26 14:24:40,035 - log
        - obj : clm - 2025-03-26 14:24:40,035 - log
        - lora_dropout : 0.1 - 2025-03-26 14:24:40,035 - log
        - label_smooth : 0.1 - 2025-03-26 14:24:40,035 - log
        - roll_interval : -1 - 2025-03-26 14:24:40,035 - log
        - roll_lr : 1e-05 - 2025-03-26 14:24:40,035 - log
        - roll_step : 100 - 2025-03-26 14:24:40,035 - log
        - eval_epoch : 1 - 2025-03-26 14:24:40,035 - log
        - device : cuda - 2025-03-26 14:24:40,035 - log
==================================================================================================== - 2025-03-26 14:24:40,035 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-26 14:24:40,035 - log
loading model pretrained weight. - 2025-03-26 14:24:40,142 - log
set max_step: 1050 - 2025-03-26 14:24:40,921 - log
start to train the model................ 1 - 2025-03-26 14:24:40,921 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.00 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-26 14:24:44,321 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 31.68 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-26 14:24:47,490 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pkl - 2025-03-26 14:24:47,808 - log
==================================================================================================== - 2025-03-26 14:27:07,810 - log
        - random_seed : 2025 - 2025-03-26 14:27:07,810 - log
        - lr : 0.0002 - 2025-03-26 14:27:07,810 - log
        - weight_decay : 0.01 - 2025-03-26 14:27:07,810 - log
        - correct_bias : False - 2025-03-26 14:27:07,810 - log
        - adam_epislon : 1e-06 - 2025-03-26 14:27:07,810 - log
        - no_decay_bias : False - 2025-03-26 14:27:07,810 - log
        - adam_beta1 : 0.9 - 2025-03-26 14:27:07,810 - log
        - adam_beta2 : 0.999 - 2025-03-26 14:27:07,810 - log
        - scheduler : linear - 2025-03-26 14:27:07,810 - log
        - max_step : None - 2025-03-26 14:27:07,810 - log
        - max_epoch : 5 - 2025-03-26 14:27:07,810 - log
        - warmup_step : 500 - 2025-03-26 14:27:07,810 - log
        - i_steps : 0 - 2025-03-26 14:27:07,810 - log
        - i_lrs : 0.00025 - 2025-03-26 14:27:07,810 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-26 14:27:07,810 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-26 14:27:07,810 - log
        - train_batch_size : 2 - 2025-03-26 14:27:07,810 - log
        - valid_batch_size : 1 - 2025-03-26 14:27:07,810 - log
        - grad_acc : 2 - 2025-03-26 14:27:07,810 - log
        - clip : 0.0 - 2025-03-26 14:27:07,810 - log
        - seq_len : 64 - 2025-03-26 14:27:07,810 - log
        - model_card : gpt2.sm - 2025-03-26 14:27:07,810 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-26 14:27:07,810 - log
        - fp16 : False - 2025-03-26 14:27:07,810 - log
        - log_interval : 100 - 2025-03-26 14:27:07,810 - log
        - eval_interval : 2000 - 2025-03-26 14:27:07,810 - log
        - save_interval : 1000 - 2025-03-26 14:27:07,810 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-26 14:27:07,810 - log
        - lora_dim : 4 - 2025-03-26 14:27:07,810 - log
        - lora_alpha : 32 - 2025-03-26 14:27:07,810 - log
        - obj : clm - 2025-03-26 14:27:07,810 - log
        - lora_dropout : 0.1 - 2025-03-26 14:27:07,810 - log
        - label_smooth : 0.1 - 2025-03-26 14:27:07,810 - log
        - roll_interval : -1 - 2025-03-26 14:27:07,810 - log
        - roll_lr : 1e-05 - 2025-03-26 14:27:07,810 - log
        - roll_step : 100 - 2025-03-26 14:27:07,810 - log
        - eval_epoch : 1 - 2025-03-26 14:27:07,810 - log
        - device : cuda - 2025-03-26 14:27:07,810 - log
==================================================================================================== - 2025-03-26 14:27:07,810 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-26 14:27:07,810 - log
loading model pretrained weight. - 2025-03-26 14:27:07,924 - log
set max_step: 1050 - 2025-03-26 14:27:08,717 - log
start to train the model................ 1 - 2025-03-26 14:27:08,718 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.45 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-26 14:27:12,163 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 31.77 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-26 14:27:15,340 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pkl - 2025-03-26 14:27:15,655 - log
start to train the model................ 2 - 2025-03-26 14:27:17,157 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 28.62 | loss  3.23 | avg loss  3.57 | ppl 35.69 - 2025-03-26 14:27:20,019 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.29 | loss  4.11 | avg loss  3.40 | ppl 30.03 - 2025-03-26 14:27:23,249 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pkl - 2025-03-26 14:27:23,889 - log
start to train the model................ 3 - 2025-03-26 14:27:25,444 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 25.59 | loss  3.38 | avg loss  3.23 | ppl 25.37 - 2025-03-26 14:27:28,003 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.09 | loss  3.04 | avg loss  3.20 | ppl 24.49 - 2025-03-26 14:27:31,212 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pkl - 2025-03-26 14:27:32,170 - log
start to train the model................ 4 - 2025-03-26 14:27:33,764 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.41 | loss  3.46 | avg loss  3.15 | ppl 23.22 - 2025-03-26 14:27:36,005 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.11 | loss  3.04 | avg loss  3.01 | ppl 20.38 - 2025-03-26 14:27:39,216 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pkl - 2025-03-26 14:27:40,501 - log
start to train the model................ 5 - 2025-03-26 14:27:42,040 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.25 | loss  2.47 | avg loss  3.04 | ppl 20.95 - 2025-03-26 14:27:43,964 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.19 | loss  3.83 | avg loss  3.01 | ppl 20.36 - 2025-03-26 14:27:47,183 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-03-26 14:27:47,183 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pkl - 2025-03-26 14:27:48,799 - log
---------------------------------------------------------------------------------------------------- - 2025-03-26 14:27:52,685 - log
End of training - 2025-03-26 14:27:52,685 - log
==================================================================================================== - 2025-03-26 14:28:49,719 - log
        - random_seed : 2025 - 2025-03-26 14:28:49,719 - log
        - lr : 0.0002 - 2025-03-26 14:28:49,719 - log
        - weight_decay : 0.01 - 2025-03-26 14:28:49,719 - log
        - correct_bias : False - 2025-03-26 14:28:49,719 - log
        - adam_epislon : 1e-06 - 2025-03-26 14:28:49,719 - log
        - no_decay_bias : False - 2025-03-26 14:28:49,719 - log
        - adam_beta1 : 0.9 - 2025-03-26 14:28:49,719 - log
        - adam_beta2 : 0.999 - 2025-03-26 14:28:49,719 - log
        - scheduler : linear - 2025-03-26 14:28:49,719 - log
        - max_step : None - 2025-03-26 14:28:49,719 - log
        - max_epoch : 5 - 2025-03-26 14:28:49,719 - log
        - warmup_step : 500 - 2025-03-26 14:28:49,719 - log
        - i_steps : 0 - 2025-03-26 14:28:49,719 - log
        - i_lrs : 0.00025 - 2025-03-26 14:28:49,719 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-26 14:28:49,719 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-26 14:28:49,719 - log
        - train_batch_size : 2 - 2025-03-26 14:28:49,720 - log
        - valid_batch_size : 1 - 2025-03-26 14:28:49,720 - log
        - grad_acc : 2 - 2025-03-26 14:28:49,720 - log
        - clip : 0.0 - 2025-03-26 14:28:49,720 - log
        - seq_len : 64 - 2025-03-26 14:28:49,720 - log
        - model_card : gpt2.sm - 2025-03-26 14:28:49,720 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-26 14:28:49,720 - log
        - fp16 : False - 2025-03-26 14:28:49,720 - log
        - log_interval : 100 - 2025-03-26 14:28:49,720 - log
        - eval_interval : 2000 - 2025-03-26 14:28:49,720 - log
        - save_interval : 1000 - 2025-03-26 14:28:49,720 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-26 14:28:49,720 - log
        - lora_dim : 4 - 2025-03-26 14:28:49,720 - log
        - lora_alpha : 32 - 2025-03-26 14:28:49,720 - log
        - obj : clm - 2025-03-26 14:28:49,720 - log
        - lora_dropout : 0.1 - 2025-03-26 14:28:49,720 - log
        - label_smooth : 0.1 - 2025-03-26 14:28:49,720 - log
        - roll_interval : -1 - 2025-03-26 14:28:49,720 - log
        - roll_lr : 1e-05 - 2025-03-26 14:28:49,720 - log
        - roll_step : 100 - 2025-03-26 14:28:49,720 - log
        - eval_epoch : 1 - 2025-03-26 14:28:49,720 - log
        - device : cuda - 2025-03-26 14:28:49,720 - log
==================================================================================================== - 2025-03-26 14:28:49,720 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-26 14:28:49,720 - log
loading model pretrained weight. - 2025-03-26 14:28:49,828 - log
set max_step: 1050 - 2025-03-26 14:28:50,711 - log
start to train the model................ 1 - 2025-03-26 14:28:50,711 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.88 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-03-26 14:28:54,199 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.16 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-03-26 14:28:57,415 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pkl - 2025-03-26 14:28:57,739 - log
start to train the model................ 2 - 2025-03-26 14:28:59,522 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 29.28 | loss  3.23 | avg loss  3.57 | ppl 35.69 - 2025-03-26 14:29:02,450 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 32.42 | loss  4.11 | avg loss  3.40 | ppl 30.03 - 2025-03-26 14:29:05,692 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pkl - 2025-03-26 14:29:06,340 - log
start to train the model................ 3 - 2025-03-26 14:29:08,101 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 26.28 | loss  3.38 | avg loss  3.23 | ppl 25.37 - 2025-03-26 14:29:10,729 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 32.43 | loss  3.04 | avg loss  3.20 | ppl 24.49 - 2025-03-26 14:29:13,972 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pkl - 2025-03-26 14:29:14,948 - log
start to train the model................ 4 - 2025-03-26 14:29:16,749 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 22.93 | loss  3.46 | avg loss  3.15 | ppl 23.22 - 2025-03-26 14:29:19,043 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 32.45 | loss  3.04 | avg loss  3.01 | ppl 20.38 - 2025-03-26 14:29:22,288 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pkl - 2025-03-26 14:29:23,591 - log
start to train the model................ 5 - 2025-03-26 14:29:25,335 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.74 | loss  2.47 | avg loss  3.04 | ppl 20.95 - 2025-03-26 14:29:27,309 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 32.49 | loss  3.83 | avg loss  3.01 | ppl 20.36 - 2025-03-26 14:29:30,558 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-03-26 14:29:30,558 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pkl - 2025-03-26 14:29:32,191 - log
---------------------------------------------------------------------------------------------------- - 2025-03-26 14:29:33,945 - log
End of training - 2025-03-26 14:29:33,945 - log
