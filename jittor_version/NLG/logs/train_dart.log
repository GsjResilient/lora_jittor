==================================================================================================== - 2025-03-27 09:33:46,754 - log
        - random_seed : 2025 - 2025-03-27 09:33:46,754 - log
        - lr : 0.0002 - 2025-03-27 09:33:46,754 - log
        - weight_decay : 0.01 - 2025-03-27 09:33:46,754 - log
        - correct_bias : False - 2025-03-27 09:33:46,754 - log
        - adam_epislon : 1e-06 - 2025-03-27 09:33:46,754 - log
        - no_decay_bias : False - 2025-03-27 09:33:46,754 - log
        - adam_beta1 : 0.9 - 2025-03-27 09:33:46,754 - log
        - adam_beta2 : 0.999 - 2025-03-27 09:33:46,754 - log
        - scheduler : linear - 2025-03-27 09:33:46,754 - log
        - max_step : None - 2025-03-27 09:33:46,754 - log
        - max_epoch : 5 - 2025-03-27 09:33:46,754 - log
        - warmup_step : 500 - 2025-03-27 09:33:46,754 - log
        - i_steps : 0 - 2025-03-27 09:33:46,754 - log
        - i_lrs : 0.00025 - 2025-03-27 09:33:46,755 - log
        - train_data : ./data/dart/train.jsonl - 2025-03-27 09:33:46,755 - log
        - valid_data : ./data/dart/valid.jsonl - 2025-03-27 09:33:46,755 - log
        - train_batch_size : 2 - 2025-03-27 09:33:46,755 - log
        - valid_batch_size : 1 - 2025-03-27 09:33:46,755 - log
        - grad_acc : 2 - 2025-03-27 09:33:46,755 - log
        - clip : 0.0 - 2025-03-27 09:33:46,755 - log
        - seq_len : 64 - 2025-03-27 09:33:46,755 - log
        - model_card : gpt2.sm - 2025-03-27 09:33:46,755 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-27 09:33:46,755 - log
        - fp16 : False - 2025-03-27 09:33:46,755 - log
        - log_interval : 100 - 2025-03-27 09:33:46,755 - log
        - eval_interval : 2000 - 2025-03-27 09:33:46,755 - log
        - save_interval : 1000 - 2025-03-27 09:33:46,755 - log
        - work_dir : ./trained_models/GPT2_M/dart - 2025-03-27 09:33:46,755 - log
        - lora_dim : 4 - 2025-03-27 09:33:46,755 - log
        - lora_alpha : 32 - 2025-03-27 09:33:46,755 - log
        - obj : clm - 2025-03-27 09:33:46,755 - log
        - lora_dropout : 0.1 - 2025-03-27 09:33:46,755 - log
        - label_smooth : 0.1 - 2025-03-27 09:33:46,755 - log
        - roll_interval : -1 - 2025-03-27 09:33:46,755 - log
        - roll_lr : 1e-05 - 2025-03-27 09:33:46,755 - log
        - roll_step : 100 - 2025-03-27 09:33:46,755 - log
        - eval_epoch : 1 - 2025-03-27 09:33:46,755 - log
        - device : cuda - 2025-03-27 09:33:46,755 - log
==================================================================================================== - 2025-03-27 09:33:46,755 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-27 09:33:46,755 - log
loading model pretrained weight. - 2025-03-27 09:33:46,865 - log
set max_step: 1570 - 2025-03-27 09:33:47,688 - log
start to train the model................ 1 - 2025-03-27 09:33:47,688 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.34 | loss  6.12 | avg loss  5.84 | ppl 343.45 - 2025-03-27 09:33:51,122 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 31.54 | loss  4.45 | avg loss  5.04 | ppl 154.28 - 2025-03-27 09:33:54,276 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 31.54 | loss  2.77 | avg loss  3.73 | ppl 41.81 - 2025-03-27 09:33:57,431 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.314.pkl - 2025-03-27 09:33:57,846 - log
start to train the model................ 2 - 2025-03-27 09:33:59,391 - log
| epoch   2 step      400 |     86 batches | lr 0.00016 | ms/batch 27.34 | loss  3.41 | avg loss  3.39 | ppl 29.69 - 2025-03-27 09:34:02,125 - log
| epoch   2 step      500 |    186 batches | lr 0.0002 | ms/batch 31.53 | loss  3.32 | avg loss  3.39 | ppl 29.81 - 2025-03-27 09:34:05,278 - log
| epoch   2 step      600 |    286 batches | lr 0.000181 | ms/batch 31.61 | loss  3.73 | avg loss  3.35 | ppl 28.40 - 2025-03-27 09:34:08,440 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.628.pkl - 2025-03-27 09:34:09,296 - log
start to train the model................ 3 - 2025-03-27 09:34:10,841 - log
| epoch   3 step      700 |     72 batches | lr 0.000163 | ms/batch 23.29 | loss  2.60 | avg loss  3.10 | ppl 22.25 - 2025-03-27 09:34:13,170 - log
| epoch   3 step      800 |    172 batches | lr 0.000144 | ms/batch 31.89 | loss  3.29 | avg loss  3.20 | ppl 24.63 - 2025-03-27 09:34:16,359 - log
| epoch   3 step      900 |    272 batches | lr 0.000125 | ms/batch 31.86 | loss  3.21 | avg loss  3.20 | ppl 24.52 - 2025-03-27 09:34:19,546 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.942.pkl - 2025-03-27 09:34:20,859 - log
start to train the model................ 4 - 2025-03-27 09:34:22,428 - log
| epoch   4 step     1000 |     58 batches | lr 0.000107 | ms/batch 18.67 | loss  4.00 | avg loss  3.10 | ppl 22.11 - 2025-03-27 09:34:24,296 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.1000.ckpt - 2025-03-27 09:34:24,296 - log
| epoch   4 step     1100 |    158 batches | lr 8.79e-05 | ms/batch 31.99 | loss  3.49 | avg loss  3.13 | ppl 22.81 - 2025-03-27 09:34:27,495 - log
| epoch   4 step     1200 |    258 batches | lr 6.92e-05 | ms/batch 32.04 | loss  3.67 | avg loss  3.03 | ppl 20.76 - 2025-03-27 09:34:30,699 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.1256.pkl - 2025-03-27 09:34:32,455 - log
start to train the model................ 5 - 2025-03-27 09:34:33,983 - log
| epoch   5 step     1300 |     44 batches | lr 5.05e-05 | ms/batch 14.45 | loss  2.98 | avg loss  3.00 | ppl 20.00 - 2025-03-27 09:34:35,428 - log
| epoch   5 step     1400 |    144 batches | lr 3.18e-05 | ms/batch 32.13 | loss  3.28 | avg loss  3.08 | ppl 21.79 - 2025-03-27 09:34:38,641 - log
| epoch   5 step     1500 |    244 batches | lr 1.31e-05 | ms/batch 32.16 | loss  2.38 | avg loss  3.03 | ppl 20.70 - 2025-03-27 09:34:41,857 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.1570.pkl - 2025-03-27 09:34:44,080 - log
---------------------------------------------------------------------------------------------------- - 2025-03-27 09:34:45,673 - log
End of training - 2025-03-27 09:34:45,673 - log
ms/batch 29.09 - 2025-03-27 09:34:45,673 - log
==================================================================================================== - 2025-03-27 11:24:37,441 - log
        - random_seed : 2025 - 2025-03-27 11:24:37,441 - log
        - lr : 0.0002 - 2025-03-27 11:24:37,441 - log
        - weight_decay : 0.01 - 2025-03-27 11:24:37,441 - log
        - correct_bias : False - 2025-03-27 11:24:37,441 - log
        - adam_epislon : 1e-06 - 2025-03-27 11:24:37,441 - log
        - no_decay_bias : False - 2025-03-27 11:24:37,441 - log
        - adam_beta1 : 0.9 - 2025-03-27 11:24:37,441 - log
        - adam_beta2 : 0.999 - 2025-03-27 11:24:37,441 - log
        - scheduler : linear - 2025-03-27 11:24:37,441 - log
        - max_step : None - 2025-03-27 11:24:37,441 - log
        - max_epoch : 5 - 2025-03-27 11:24:37,441 - log
        - warmup_step : 500 - 2025-03-27 11:24:37,441 - log
        - i_steps : 0 - 2025-03-27 11:24:37,441 - log
        - i_lrs : 0.00025 - 2025-03-27 11:24:37,441 - log
        - train_data : ./data/dart/train.jsonl - 2025-03-27 11:24:37,441 - log
        - valid_data : ./data/dart/valid.jsonl - 2025-03-27 11:24:37,441 - log
        - train_batch_size : 2 - 2025-03-27 11:24:37,441 - log
        - valid_batch_size : 1 - 2025-03-27 11:24:37,441 - log
        - grad_acc : 2 - 2025-03-27 11:24:37,441 - log
        - clip : 0.0 - 2025-03-27 11:24:37,441 - log
        - seq_len : 64 - 2025-03-27 11:24:37,441 - log
        - model_card : gpt2.sm - 2025-03-27 11:24:37,441 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-27 11:24:37,441 - log
        - fp16 : False - 2025-03-27 11:24:37,441 - log
        - log_interval : 100 - 2025-03-27 11:24:37,441 - log
        - eval_interval : 2000 - 2025-03-27 11:24:37,441 - log
        - save_interval : 1000 - 2025-03-27 11:24:37,441 - log
        - work_dir : ./trained_models/GPT2_M/dart - 2025-03-27 11:24:37,441 - log
        - lora_dim : 4 - 2025-03-27 11:24:37,441 - log
        - lora_alpha : 32 - 2025-03-27 11:24:37,441 - log
        - obj : clm - 2025-03-27 11:24:37,441 - log
        - lora_dropout : 0.1 - 2025-03-27 11:24:37,441 - log
        - label_smooth : 0.1 - 2025-03-27 11:24:37,442 - log
        - roll_interval : -1 - 2025-03-27 11:24:37,442 - log
        - roll_lr : 1e-05 - 2025-03-27 11:24:37,442 - log
        - roll_step : 100 - 2025-03-27 11:24:37,442 - log
        - eval_epoch : 1 - 2025-03-27 11:24:37,442 - log
        - device : cuda - 2025-03-27 11:24:37,442 - log
==================================================================================================== - 2025-03-27 11:24:37,442 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-27 11:24:37,442 - log
loading model pretrained weight. - 2025-03-27 11:24:37,544 - log
set max_step: 1570 - 2025-03-27 11:24:38,347 - log
start to train the model................ 1 - 2025-03-27 11:24:38,347 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.56 | loss  6.12 | avg loss  5.84 | ppl 343.45 - 2025-03-27 11:24:41,803 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 31.49 | loss  4.45 | avg loss  5.04 | ppl 154.28 - 2025-03-27 11:24:44,951 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 31.48 | loss  2.77 | avg loss  3.73 | ppl 41.81 - 2025-03-27 11:24:48,099 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.314.pkl - 2025-03-27 11:24:48,511 - log
start to train the model................ 2 - 2025-03-27 11:24:50,291 - log
| epoch   2 step      400 |     86 batches | lr 0.00016 | ms/batch 27.77 | loss  3.41 | avg loss  3.39 | ppl 29.69 - 2025-03-27 11:24:53,067 - log
| epoch   2 step      500 |    186 batches | lr 0.0002 | ms/batch 31.65 | loss  3.32 | avg loss  3.39 | ppl 29.81 - 2025-03-27 11:24:56,232 - log
| epoch   2 step      600 |    286 batches | lr 0.000181 | ms/batch 31.71 | loss  3.73 | avg loss  3.35 | ppl 28.40 - 2025-03-27 11:24:59,403 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.628.pkl - 2025-03-27 11:25:00,265 - log
start to train the model................ 3 - 2025-03-27 11:25:02,078 - log
| epoch   3 step      700 |     72 batches | lr 0.000163 | ms/batch 23.36 | loss  2.60 | avg loss  3.10 | ppl 22.25 - 2025-03-27 11:25:04,415 - log
| epoch   3 step      800 |    172 batches | lr 0.000144 | ms/batch 31.88 | loss  3.29 | avg loss  3.20 | ppl 24.63 - 2025-03-27 11:25:07,603 - log
| epoch   3 step      900 |    272 batches | lr 0.000125 | ms/batch 31.87 | loss  3.21 | avg loss  3.20 | ppl 24.52 - 2025-03-27 11:25:10,790 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.942.pkl - 2025-03-27 11:25:12,096 - log
start to train the model................ 4 - 2025-03-27 11:25:13,831 - log
| epoch   4 step     1000 |     58 batches | lr 0.000107 | ms/batch 19.18 | loss  4.00 | avg loss  3.10 | ppl 22.11 - 2025-03-27 11:25:15,749 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.1000.ckpt - 2025-03-27 11:25:15,750 - log
| epoch   4 step     1100 |    158 batches | lr 8.79e-05 | ms/batch 32.08 | loss  3.49 | avg loss  3.13 | ppl 22.81 - 2025-03-27 11:25:18,957 - log
| epoch   4 step     1200 |    258 batches | lr 6.92e-05 | ms/batch 32.14 | loss  3.67 | avg loss  3.03 | ppl 20.76 - 2025-03-27 11:25:22,171 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.1256.pkl - 2025-03-27 11:25:23,943 - log
start to train the model................ 5 - 2025-03-27 11:25:25,675 - log
| epoch   5 step     1300 |     44 batches | lr 5.05e-05 | ms/batch 14.41 | loss  2.98 | avg loss  3.00 | ppl 20.00 - 2025-03-27 11:25:27,117 - log
| epoch   5 step     1400 |    144 batches | lr 3.18e-05 | ms/batch 32.26 | loss  3.28 | avg loss  3.08 | ppl 21.79 - 2025-03-27 11:25:30,343 - log
| epoch   5 step     1500 |    244 batches | lr 1.31e-05 | ms/batch 32.35 | loss  2.38 | avg loss  3.03 | ppl 20.70 - 2025-03-27 11:25:33,578 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.1570.pkl - 2025-03-27 11:25:35,816 - log
---------------------------------------------------------------------------------------------------- - 2025-03-27 11:25:37,588 - log
End of training - 2025-03-27 11:25:37,588 - log
ms/batch 29.21 - 2025-03-27 11:25:37,588 - log
==================================================================================================== - 2025-03-27 11:51:52,940 - log
        - random_seed : 2025 - 2025-03-27 11:51:52,941 - log
        - lr : 0.0002 - 2025-03-27 11:51:52,941 - log
        - weight_decay : 0.01 - 2025-03-27 11:51:52,941 - log
        - correct_bias : False - 2025-03-27 11:51:52,941 - log
        - adam_epislon : 1e-06 - 2025-03-27 11:51:52,941 - log
        - no_decay_bias : False - 2025-03-27 11:51:52,941 - log
        - adam_beta1 : 0.9 - 2025-03-27 11:51:52,941 - log
        - adam_beta2 : 0.999 - 2025-03-27 11:51:52,941 - log
        - scheduler : linear - 2025-03-27 11:51:52,941 - log
        - max_step : None - 2025-03-27 11:51:52,941 - log
        - max_epoch : 5 - 2025-03-27 11:51:52,941 - log
        - warmup_step : 500 - 2025-03-27 11:51:52,941 - log
        - i_steps : 0 - 2025-03-27 11:51:52,941 - log
        - i_lrs : 0.00025 - 2025-03-27 11:51:52,941 - log
        - train_data : ./data/dart/train.jsonl - 2025-03-27 11:51:52,941 - log
        - valid_data : ./data/dart/valid.jsonl - 2025-03-27 11:51:52,941 - log
        - train_batch_size : 2 - 2025-03-27 11:51:52,941 - log
        - valid_batch_size : 1 - 2025-03-27 11:51:52,941 - log
        - grad_acc : 2 - 2025-03-27 11:51:52,941 - log
        - clip : 0.0 - 2025-03-27 11:51:52,941 - log
        - seq_len : 64 - 2025-03-27 11:51:52,941 - log
        - model_card : gpt2.sm - 2025-03-27 11:51:52,941 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-27 11:51:52,941 - log
        - fp16 : False - 2025-03-27 11:51:52,941 - log
        - log_interval : 100 - 2025-03-27 11:51:52,941 - log
        - eval_interval : 2000 - 2025-03-27 11:51:52,941 - log
        - save_interval : 1000 - 2025-03-27 11:51:52,941 - log
        - work_dir : ./trained_models/GPT2_M/dart - 2025-03-27 11:51:52,941 - log
        - lora_dim : 4 - 2025-03-27 11:51:52,941 - log
        - lora_alpha : 32 - 2025-03-27 11:51:52,941 - log
        - obj : clm - 2025-03-27 11:51:52,941 - log
        - lora_dropout : 0.1 - 2025-03-27 11:51:52,941 - log
        - label_smooth : 0.1 - 2025-03-27 11:51:52,941 - log
        - roll_interval : -1 - 2025-03-27 11:51:52,941 - log
        - roll_lr : 1e-05 - 2025-03-27 11:51:52,941 - log
        - roll_step : 100 - 2025-03-27 11:51:52,941 - log
        - eval_epoch : 1 - 2025-03-27 11:51:52,941 - log
        - device : cuda - 2025-03-27 11:51:52,941 - log
==================================================================================================== - 2025-03-27 11:51:52,941 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-27 11:51:52,941 - log
loading model pretrained weight. - 2025-03-27 11:51:53,054 - log
set max_step: 1570 - 2025-03-27 11:51:54,057 - log
start to train the model................ 1 - 2025-03-27 11:51:54,057 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.22 | loss  6.12 | avg loss  5.84 | ppl 343.45 - 2025-03-27 11:51:57,479 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 31.43 | loss  4.45 | avg loss  5.04 | ppl 154.28 - 2025-03-27 11:52:00,623 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 31.56 | loss  2.77 | avg loss  3.73 | ppl 41.81 - 2025-03-27 11:52:03,779 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.314.pkl - 2025-03-27 11:52:04,192 - log
start to train the model................ 2 - 2025-03-27 11:52:05,871 - log
| epoch   2 step      400 |     86 batches | lr 0.00016 | ms/batch 27.88 | loss  3.41 | avg loss  3.39 | ppl 29.69 - 2025-03-27 11:52:08,659 - log
| epoch   2 step      500 |    186 batches | lr 0.0002 | ms/batch 31.81 | loss  3.32 | avg loss  3.39 | ppl 29.81 - 2025-03-27 11:52:11,839 - log
| epoch   2 step      600 |    286 batches | lr 0.000181 | ms/batch 31.89 | loss  3.73 | avg loss  3.35 | ppl 28.40 - 2025-03-27 11:52:15,029 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.628.pkl - 2025-03-27 11:52:15,932 - log
start to train the model................ 3 - 2025-03-27 11:52:17,707 - log
| epoch   3 step      700 |     72 batches | lr 0.000163 | ms/batch 23.57 | loss  2.60 | avg loss  3.10 | ppl 22.25 - 2025-03-27 11:52:20,065 - log
| epoch   3 step      800 |    172 batches | lr 0.000144 | ms/batch 32.03 | loss  3.29 | avg loss  3.20 | ppl 24.63 - 2025-03-27 11:52:23,268 - log
| epoch   3 step      900 |    272 batches | lr 0.000125 | ms/batch 32.15 | loss  3.21 | avg loss  3.20 | ppl 24.52 - 2025-03-27 11:52:26,483 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.942.pkl - 2025-03-27 11:52:27,805 - log
start to train the model................ 4 - 2025-03-27 11:52:29,540 - log
| epoch   4 step     1000 |     58 batches | lr 0.000107 | ms/batch 19.01 | loss  4.00 | avg loss  3.10 | ppl 22.11 - 2025-03-27 11:52:31,442 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.1000.ckpt - 2025-03-27 11:52:31,442 - log
| epoch   4 step     1100 |    158 batches | lr 8.79e-05 | ms/batch 32.27 | loss  3.49 | avg loss  3.13 | ppl 22.81 - 2025-03-27 11:52:34,669 - log
| epoch   4 step     1200 |    258 batches | lr 6.92e-05 | ms/batch 32.29 | loss  3.67 | avg loss  3.03 | ppl 20.76 - 2025-03-27 11:52:37,898 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.1256.pkl - 2025-03-27 11:52:39,678 - log
start to train the model................ 5 - 2025-03-27 11:52:41,739 - log
| epoch   5 step     1300 |     44 batches | lr 5.05e-05 | ms/batch 14.81 | loss  2.98 | avg loss  3.00 | ppl 20.00 - 2025-03-27 11:52:43,220 - log
| epoch   5 step     1400 |    144 batches | lr 3.18e-05 | ms/batch 32.40 | loss  3.28 | avg loss  3.08 | ppl 21.79 - 2025-03-27 11:52:46,461 - log
| epoch   5 step     1500 |    244 batches | lr 1.31e-05 | ms/batch 32.54 | loss  2.38 | avg loss  3.03 | ppl 20.70 - 2025-03-27 11:52:49,715 - log
saving checkpoint, ./trained_models/GPT2_M/dart/model.1570.pkl - 2025-03-27 11:52:51,960 - log
---------------------------------------------------------------------------------------------------- - 2025-03-27 11:52:53,801 - log
End of training - 2025-03-27 11:52:53,801 - log
ms/batch 29.32 - 2025-03-27 11:52:53,801 - log
