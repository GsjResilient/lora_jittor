==================================================================================================== - 2025-03-27 08:11:01,456 - log
        - random_seed : 2025 - 2025-03-27 08:11:01,456 - log
        - lr : 0.0002 - 2025-03-27 08:11:01,456 - log
        - weight_decay : 0.01 - 2025-03-27 08:11:01,456 - log
        - correct_bias : False - 2025-03-27 08:11:01,456 - log
        - adam_epislon : 1e-06 - 2025-03-27 08:11:01,456 - log
        - no_decay_bias : False - 2025-03-27 08:11:01,456 - log
        - adam_beta1 : 0.9 - 2025-03-27 08:11:01,456 - log
        - adam_beta2 : 0.999 - 2025-03-27 08:11:01,456 - log
        - scheduler : linear - 2025-03-27 08:11:01,456 - log
        - max_step : None - 2025-03-27 08:11:01,456 - log
        - max_epoch : 5 - 2025-03-27 08:11:01,456 - log
        - warmup_step : 500 - 2025-03-27 08:11:01,456 - log
        - i_steps : 0 - 2025-03-27 08:11:01,456 - log
        - i_lrs : 0.00025 - 2025-03-27 08:11:01,456 - log
        - train_data : ./data/webnlg_challenge_2017/train.jsonl - 2025-03-27 08:11:01,456 - log
        - valid_data : ./data/webnlg_challenge_2017/valid.jsonl - 2025-03-27 08:11:01,456 - log
        - train_batch_size : 2 - 2025-03-27 08:11:01,456 - log
        - valid_batch_size : 1 - 2025-03-27 08:11:01,456 - log
        - grad_acc : 2 - 2025-03-27 08:11:01,456 - log
        - clip : 0.0 - 2025-03-27 08:11:01,456 - log
        - seq_len : 64 - 2025-03-27 08:11:01,456 - log
        - model_card : gpt2.sm - 2025-03-27 08:11:01,456 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-27 08:11:01,456 - log
        - fp16 : False - 2025-03-27 08:11:01,456 - log
        - log_interval : 100 - 2025-03-27 08:11:01,456 - log
        - eval_interval : 2000 - 2025-03-27 08:11:01,456 - log
        - save_interval : 1000 - 2025-03-27 08:11:01,456 - log
        - work_dir : ./trained_models/GPT2_M/webnlg - 2025-03-27 08:11:01,456 - log
        - lora_dim : 4 - 2025-03-27 08:11:01,456 - log
        - lora_alpha : 32 - 2025-03-27 08:11:01,456 - log
        - obj : clm - 2025-03-27 08:11:01,456 - log
        - lora_dropout : 0.1 - 2025-03-27 08:11:01,456 - log
        - label_smooth : 0.1 - 2025-03-27 08:11:01,456 - log
        - roll_interval : -1 - 2025-03-27 08:11:01,456 - log
        - roll_lr : 1e-05 - 2025-03-27 08:11:01,456 - log
        - roll_step : 100 - 2025-03-27 08:11:01,456 - log
        - eval_epoch : 1 - 2025-03-27 08:11:01,456 - log
        - device : cuda - 2025-03-27 08:11:01,456 - log
==================================================================================================== - 2025-03-27 08:11:01,456 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-27 08:11:01,456 - log
loading model pretrained weight. - 2025-03-27 08:11:01,581 - log
set max_step: 455 - 2025-03-27 08:11:03,380 - log
start to train the model................ 1 - 2025-03-27 08:11:03,380 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.91.pkl - 2025-03-27 08:11:06,624 - log
start to train the model................ 2 - 2025-03-27 08:11:08,178 - log
| epoch   2 step      100 |      9 batches | lr 4e-05 | ms/batch  3.51 | loss  5.37 | avg loss  5.92 | ppl 372.28 - 2025-03-27 08:11:08,528 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.182.pkl - 2025-03-27 08:11:11,068 - log
start to train the model................ 3 - 2025-03-27 08:11:12,647 - log
| epoch   3 step      200 |     18 batches | lr 8e-05 | ms/batch  5.92 | loss  3.86 | avg loss  3.98 | ppl 53.78 - 2025-03-27 08:11:13,238 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.273.pkl - 2025-03-27 08:11:15,504 - log
start to train the model................ 4 - 2025-03-27 08:11:16,969 - log
| epoch   4 step      300 |     27 batches | lr 0.00012 | ms/batch  8.97 | loss  2.68 | avg loss  3.06 | ppl 21.26 - 2025-03-27 08:11:17,866 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.364.pkl - 2025-03-27 08:11:19,843 - log
start to train the model................ 5 - 2025-03-27 08:11:21,382 - log
| epoch   5 step      400 |     36 batches | lr 0.00016 | ms/batch 11.57 | loss  3.28 | avg loss  2.94 | ppl 18.88 - 2025-03-27 08:11:22,539 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.455.pkl - 2025-03-27 08:11:24,245 - log
---------------------------------------------------------------------------------------------------- - 2025-03-27 08:11:25,737 - log
End of training - 2025-03-27 08:11:25,738 - log
ms/batch  5.99 - 2025-03-27 08:11:25,738 - log
==================================================================================================== - 2025-03-27 09:03:48,447 - log
        - random_seed : 2025 - 2025-03-27 09:03:48,447 - log
        - lr : 0.0002 - 2025-03-27 09:03:48,447 - log
        - weight_decay : 0.01 - 2025-03-27 09:03:48,447 - log
        - correct_bias : False - 2025-03-27 09:03:48,447 - log
        - adam_epislon : 1e-06 - 2025-03-27 09:03:48,447 - log
        - no_decay_bias : False - 2025-03-27 09:03:48,447 - log
        - adam_beta1 : 0.9 - 2025-03-27 09:03:48,448 - log
        - adam_beta2 : 0.999 - 2025-03-27 09:03:48,448 - log
        - scheduler : linear - 2025-03-27 09:03:48,448 - log
        - max_step : None - 2025-03-27 09:03:48,448 - log
        - max_epoch : 5 - 2025-03-27 09:03:48,448 - log
        - warmup_step : 500 - 2025-03-27 09:03:48,448 - log
        - i_steps : 0 - 2025-03-27 09:03:48,448 - log
        - i_lrs : 0.00025 - 2025-03-27 09:03:48,448 - log
        - train_data : ./data/webnlg_challenge_2017/train.jsonl - 2025-03-27 09:03:48,448 - log
        - valid_data : ./data/webnlg_challenge_2017/valid.jsonl - 2025-03-27 09:03:48,448 - log
        - train_batch_size : 2 - 2025-03-27 09:03:48,448 - log
        - valid_batch_size : 1 - 2025-03-27 09:03:48,448 - log
        - grad_acc : 2 - 2025-03-27 09:03:48,448 - log
        - clip : 0.0 - 2025-03-27 09:03:48,448 - log
        - seq_len : 64 - 2025-03-27 09:03:48,448 - log
        - model_card : gpt2.sm - 2025-03-27 09:03:48,448 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-27 09:03:48,448 - log
        - fp16 : False - 2025-03-27 09:03:48,448 - log
        - log_interval : 100 - 2025-03-27 09:03:48,448 - log
        - eval_interval : 2000 - 2025-03-27 09:03:48,448 - log
        - save_interval : 1000 - 2025-03-27 09:03:48,448 - log
        - work_dir : ./trained_models/GPT2_M/webnlg - 2025-03-27 09:03:48,448 - log
        - lora_dim : 4 - 2025-03-27 09:03:48,448 - log
        - lora_alpha : 32 - 2025-03-27 09:03:48,448 - log
        - obj : clm - 2025-03-27 09:03:48,448 - log
        - lora_dropout : 0.1 - 2025-03-27 09:03:48,448 - log
        - label_smooth : 0.1 - 2025-03-27 09:03:48,448 - log
        - roll_interval : -1 - 2025-03-27 09:03:48,448 - log
        - roll_lr : 1e-05 - 2025-03-27 09:03:48,448 - log
        - roll_step : 100 - 2025-03-27 09:03:48,448 - log
        - eval_epoch : 1 - 2025-03-27 09:03:48,448 - log
        - device : cuda - 2025-03-27 09:03:48,448 - log
==================================================================================================== - 2025-03-27 09:03:48,448 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-27 09:03:48,448 - log
loading model pretrained weight. - 2025-03-27 09:03:48,548 - log
set max_step: 455 - 2025-03-27 09:03:50,296 - log
start to train the model................ 1 - 2025-03-27 09:03:50,296 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.91.pkl - 2025-03-27 09:03:53,455 - log
start to train the model................ 2 - 2025-03-27 09:03:55,198 - log
| epoch   2 step      100 |      9 batches | lr 4e-05 | ms/batch  3.60 | loss  5.37 | avg loss  5.92 | ppl 372.28 - 2025-03-27 09:03:55,558 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.182.pkl - 2025-03-27 09:03:58,082 - log
start to train the model................ 3 - 2025-03-27 09:03:59,795 - log
| epoch   3 step      200 |     18 batches | lr 8e-05 | ms/batch  5.90 | loss  3.86 | avg loss  3.98 | ppl 53.78 - 2025-03-27 09:04:00,385 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.273.pkl - 2025-03-27 09:04:02,649 - log
start to train the model................ 4 - 2025-03-27 09:04:04,281 - log
| epoch   4 step      300 |     27 batches | lr 0.00012 | ms/batch  9.04 | loss  2.68 | avg loss  3.06 | ppl 21.26 - 2025-03-27 09:04:05,185 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.364.pkl - 2025-03-27 09:04:07,154 - log
start to train the model................ 5 - 2025-03-27 09:04:08,803 - log
| epoch   5 step      400 |     36 batches | lr 0.00016 | ms/batch 11.61 | loss  3.28 | avg loss  2.94 | ppl 18.88 - 2025-03-27 09:04:09,964 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.455.pkl - 2025-03-27 09:04:11,678 - log
---------------------------------------------------------------------------------------------------- - 2025-03-27 09:04:13,374 - log
End of training - 2025-03-27 09:04:13,374 - log
ms/batch  6.03 - 2025-03-27 09:04:13,374 - log
==================================================================================================== - 2025-03-27 09:24:42,456 - log
        - random_seed : 2025 - 2025-03-27 09:24:42,457 - log
        - lr : 0.0002 - 2025-03-27 09:24:42,457 - log
        - weight_decay : 0.01 - 2025-03-27 09:24:42,457 - log
        - correct_bias : False - 2025-03-27 09:24:42,457 - log
        - adam_epislon : 1e-06 - 2025-03-27 09:24:42,457 - log
        - no_decay_bias : False - 2025-03-27 09:24:42,457 - log
        - adam_beta1 : 0.9 - 2025-03-27 09:24:42,457 - log
        - adam_beta2 : 0.999 - 2025-03-27 09:24:42,457 - log
        - scheduler : linear - 2025-03-27 09:24:42,457 - log
        - max_step : None - 2025-03-27 09:24:42,457 - log
        - max_epoch : 5 - 2025-03-27 09:24:42,457 - log
        - warmup_step : 500 - 2025-03-27 09:24:42,457 - log
        - i_steps : 0 - 2025-03-27 09:24:42,457 - log
        - i_lrs : 0.00025 - 2025-03-27 09:24:42,457 - log
        - train_data : ./data/webnlg_challenge_2017/train.jsonl - 2025-03-27 09:24:42,457 - log
        - valid_data : ./data/webnlg_challenge_2017/valid.jsonl - 2025-03-27 09:24:42,457 - log
        - train_batch_size : 2 - 2025-03-27 09:24:42,457 - log
        - valid_batch_size : 1 - 2025-03-27 09:24:42,457 - log
        - grad_acc : 2 - 2025-03-27 09:24:42,457 - log
        - clip : 0.0 - 2025-03-27 09:24:42,457 - log
        - seq_len : 64 - 2025-03-27 09:24:42,457 - log
        - model_card : gpt2.sm - 2025-03-27 09:24:42,457 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-27 09:24:42,457 - log
        - fp16 : False - 2025-03-27 09:24:42,457 - log
        - log_interval : 100 - 2025-03-27 09:24:42,457 - log
        - eval_interval : 2000 - 2025-03-27 09:24:42,457 - log
        - save_interval : 1000 - 2025-03-27 09:24:42,457 - log
        - work_dir : ./trained_models/GPT2_M/webnlg - 2025-03-27 09:24:42,457 - log
        - lora_dim : 4 - 2025-03-27 09:24:42,457 - log
        - lora_alpha : 32 - 2025-03-27 09:24:42,457 - log
        - obj : clm - 2025-03-27 09:24:42,457 - log
        - lora_dropout : 0.1 - 2025-03-27 09:24:42,457 - log
        - label_smooth : 0.1 - 2025-03-27 09:24:42,457 - log
        - roll_interval : -1 - 2025-03-27 09:24:42,457 - log
        - roll_lr : 1e-05 - 2025-03-27 09:24:42,457 - log
        - roll_step : 100 - 2025-03-27 09:24:42,457 - log
        - eval_epoch : 1 - 2025-03-27 09:24:42,457 - log
        - device : cuda - 2025-03-27 09:24:42,457 - log
==================================================================================================== - 2025-03-27 09:24:42,457 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-27 09:24:42,457 - log
loading model pretrained weight. - 2025-03-27 09:24:42,564 - log
set max_step: 455 - 2025-03-27 09:24:44,003 - log
start to train the model................ 1 - 2025-03-27 09:24:44,003 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.91.pkl - 2025-03-27 09:24:47,243 - log
start to train the model................ 2 - 2025-03-27 09:24:48,907 - log
| epoch   2 step      100 |      9 batches | lr 4e-05 | ms/batch  3.39 | loss  5.37 | avg loss  5.92 | ppl 372.28 - 2025-03-27 09:24:49,246 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.182.pkl - 2025-03-27 09:24:51,933 - log
start to train the model................ 3 - 2025-03-27 09:24:53,710 - log
| epoch   3 step      200 |     18 batches | lr 8e-05 | ms/batch  5.99 | loss  3.86 | avg loss  3.98 | ppl 53.78 - 2025-03-27 09:24:54,309 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.273.pkl - 2025-03-27 09:24:56,619 - log
start to train the model................ 4 - 2025-03-27 09:24:58,292 - log
| epoch   4 step      300 |     27 batches | lr 0.00012 | ms/batch  9.13 | loss  2.68 | avg loss  3.06 | ppl 21.26 - 2025-03-27 09:24:59,204 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.364.pkl - 2025-03-27 09:25:01,208 - log
start to train the model................ 5 - 2025-03-27 09:25:02,885 - log
| epoch   5 step      400 |     36 batches | lr 0.00016 | ms/batch 11.73 | loss  3.28 | avg loss  2.94 | ppl 18.88 - 2025-03-27 09:25:04,058 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.455.pkl - 2025-03-27 09:25:05,794 - log
---------------------------------------------------------------------------------------------------- - 2025-03-27 09:25:07,476 - log
End of training - 2025-03-27 09:25:07,476 - log
ms/batch  6.05 - 2025-03-27 09:25:07,476 - log
==================================================================================================== - 2025-03-27 11:34:19,286 - log
        - random_seed : 2025 - 2025-03-27 11:34:19,286 - log
        - lr : 0.0002 - 2025-03-27 11:34:19,286 - log
        - weight_decay : 0.01 - 2025-03-27 11:34:19,286 - log
        - correct_bias : False - 2025-03-27 11:34:19,286 - log
        - adam_epislon : 1e-06 - 2025-03-27 11:34:19,286 - log
        - no_decay_bias : False - 2025-03-27 11:34:19,286 - log
        - adam_beta1 : 0.9 - 2025-03-27 11:34:19,286 - log
        - adam_beta2 : 0.999 - 2025-03-27 11:34:19,286 - log
        - scheduler : linear - 2025-03-27 11:34:19,286 - log
        - max_step : None - 2025-03-27 11:34:19,286 - log
        - max_epoch : 5 - 2025-03-27 11:34:19,286 - log
        - warmup_step : 500 - 2025-03-27 11:34:19,286 - log
        - i_steps : 0 - 2025-03-27 11:34:19,286 - log
        - i_lrs : 0.00025 - 2025-03-27 11:34:19,286 - log
        - train_data : ./data/webnlg_challenge_2017/train.jsonl - 2025-03-27 11:34:19,286 - log
        - valid_data : ./data/webnlg_challenge_2017/valid.jsonl - 2025-03-27 11:34:19,286 - log
        - train_batch_size : 2 - 2025-03-27 11:34:19,287 - log
        - valid_batch_size : 1 - 2025-03-27 11:34:19,287 - log
        - grad_acc : 2 - 2025-03-27 11:34:19,287 - log
        - clip : 0.0 - 2025-03-27 11:34:19,287 - log
        - seq_len : 64 - 2025-03-27 11:34:19,287 - log
        - model_card : gpt2.sm - 2025-03-27 11:34:19,287 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-27 11:34:19,287 - log
        - fp16 : False - 2025-03-27 11:34:19,287 - log
        - log_interval : 100 - 2025-03-27 11:34:19,287 - log
        - eval_interval : 2000 - 2025-03-27 11:34:19,287 - log
        - save_interval : 1000 - 2025-03-27 11:34:19,287 - log
        - work_dir : ./trained_models/GPT2_M/webnlg - 2025-03-27 11:34:19,287 - log
        - lora_dim : 4 - 2025-03-27 11:34:19,287 - log
        - lora_alpha : 32 - 2025-03-27 11:34:19,287 - log
        - obj : clm - 2025-03-27 11:34:19,287 - log
        - lora_dropout : 0.1 - 2025-03-27 11:34:19,287 - log
        - label_smooth : 0.1 - 2025-03-27 11:34:19,287 - log
        - roll_interval : -1 - 2025-03-27 11:34:19,287 - log
        - roll_lr : 1e-05 - 2025-03-27 11:34:19,287 - log
        - roll_step : 100 - 2025-03-27 11:34:19,287 - log
        - eval_epoch : 1 - 2025-03-27 11:34:19,287 - log
        - device : cuda - 2025-03-27 11:34:19,287 - log
==================================================================================================== - 2025-03-27 11:34:19,287 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-27 11:34:19,287 - log
loading model pretrained weight. - 2025-03-27 11:34:19,400 - log
set max_step: 455 - 2025-03-27 11:34:20,702 - log
start to train the model................ 1 - 2025-03-27 11:34:20,702 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.91.pkl - 2025-03-27 11:34:23,895 - log
start to train the model................ 2 - 2025-03-27 11:34:25,544 - log
| epoch   2 step      100 |      9 batches | lr 4e-05 | ms/batch  3.38 | loss  5.37 | avg loss  5.92 | ppl 372.28 - 2025-03-27 11:34:25,882 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.182.pkl - 2025-03-27 11:34:28,431 - log
start to train the model................ 3 - 2025-03-27 11:34:30,158 - log
| epoch   3 step      200 |     18 batches | lr 8e-05 | ms/batch  5.96 | loss  3.86 | avg loss  3.98 | ppl 53.78 - 2025-03-27 11:34:30,754 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.273.pkl - 2025-03-27 11:34:33,041 - log
start to train the model................ 4 - 2025-03-27 11:34:34,728 - log
| epoch   4 step      300 |     27 batches | lr 0.00012 | ms/batch  9.05 | loss  2.68 | avg loss  3.06 | ppl 21.26 - 2025-03-27 11:34:35,633 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.364.pkl - 2025-03-27 11:34:37,630 - log
start to train the model................ 5 - 2025-03-27 11:34:39,335 - log
| epoch   5 step      400 |     36 batches | lr 0.00016 | ms/batch 11.68 | loss  3.28 | avg loss  2.94 | ppl 18.88 - 2025-03-27 11:34:40,504 - log
saving checkpoint, ./trained_models/GPT2_M/webnlg/model.455.pkl - 2025-03-27 11:34:42,232 - log
---------------------------------------------------------------------------------------------------- - 2025-03-27 11:34:43,997 - log
End of training - 2025-03-27 11:34:43,997 - log
ms/batch  6.01 - 2025-03-27 11:34:43,997 - log
