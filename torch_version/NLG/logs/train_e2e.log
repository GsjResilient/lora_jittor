==================================================================================================== - 2025-03-23 16:01:53,915 - log
        - platform : local - 2025-03-23 16:01:53,915 - log
        - local_rank : 0 - 2025-03-23 16:01:53,915 - log
        - rank : 0 - 2025-03-23 16:01:53,915 - log
        - device : cuda:0 - 2025-03-23 16:01:53,915 - log
        - world_size : 1 - 2025-03-23 16:01:53,915 - log
        - random_seed : 110 - 2025-03-23 16:01:53,915 - log
        - lr : 0.0002 - 2025-03-23 16:01:53,915 - log
        - weight_decay : 0.01 - 2025-03-23 16:01:53,915 - log
        - correct_bias : True - 2025-03-23 16:01:53,915 - log
        - adam_epislon : 1e-06 - 2025-03-23 16:01:53,915 - log
        - no_decay_bias : False - 2025-03-23 16:01:53,915 - log
        - adam_beta1 : 0.9 - 2025-03-23 16:01:53,915 - log
        - adam_beta2 : 0.999 - 2025-03-23 16:01:53,915 - log
        - scheduler : linear - 2025-03-23 16:01:53,915 - log
        - max_step : None - 2025-03-23 16:01:53,915 - log
        - max_epoch : 5 - 2025-03-23 16:01:53,915 - log
        - warmup_step : 500 - 2025-03-23 16:01:53,915 - log
        - i_steps : 0 - 2025-03-23 16:01:53,915 - log
        - i_lrs : 0.00025 - 2025-03-23 16:01:53,915 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-23 16:01:53,915 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-23 16:01:53,915 - log
        - train_batch_size : 4 - 2025-03-23 16:01:53,915 - log
        - valid_batch_size : 2 - 2025-03-23 16:01:53,915 - log
        - grad_acc : 2 - 2025-03-23 16:01:53,915 - log
        - clip : 0.0 - 2025-03-23 16:01:53,915 - log
        - seq_len : 256 - 2025-03-23 16:01:53,915 - log
        - model_card : gpt2.md - 2025-03-23 16:01:53,915 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-23 16:01:53,915 - log
        - fp16 : False - 2025-03-23 16:01:53,915 - log
        - log_interval : 100 - 2025-03-23 16:01:53,915 - log
        - eval_interval : 2000 - 2025-03-23 16:01:53,915 - log
        - save_interval : 1000 - 2025-03-23 16:01:53,915 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-23 16:01:53,915 - log
        - lora_dim : 4 - 2025-03-23 16:01:53,915 - log
        - lora_alpha : 32 - 2025-03-23 16:01:53,915 - log
        - obj : clm - 2025-03-23 16:01:53,915 - log
        - lora_dropout : 0.1 - 2025-03-23 16:01:53,916 - log
        - label_smooth : 0.1 - 2025-03-23 16:01:53,916 - log
        - roll_interval : -1 - 2025-03-23 16:01:53,916 - log
        - roll_lr : 1e-05 - 2025-03-23 16:01:53,916 - log
        - roll_step : 100 - 2025-03-23 16:01:53,916 - log
        - eval_epoch : 1 - 2025-03-23 16:01:53,916 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-23 16:01:53,916 - log
==================================================================================================== - 2025-03-23 16:01:53,916 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-23 16:01:53,916 - log
loading model pretrained weight. - 2025-03-23 16:01:56,450 - log
set max_step: 525 - 2025-03-23 16:01:59,793 - log
start to train the model................ 1 - 2025-03-23 16:01:59,943 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 431.16 | loss  5.20 | avg loss  5.73 | ppl 308.19 - 2025-03-23 16:02:43,060 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.105.pt - 2025-03-23 16:02:45,205 - log
start to train the model................ 2 - 2025-03-23 16:02:46,576 - log
| epoch   2 step      200 |     95 batches | lr 8e-05 | ms/batch 409.02 | loss  3.75 | avg loss  4.69 | ppl 109.10 - 2025-03-23 16:03:27,478 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-23 16:03:31,828 - log
start to train the model................ 3 - 2025-03-23 16:03:33,229 - log
| epoch   3 step      300 |     90 batches | lr 0.00012 | ms/batch 389.05 | loss  2.97 | avg loss  3.37 | ppl 29.00 - 2025-03-23 16:04:12,134 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.315.pt - 2025-03-23 16:04:18,659 - log
start to train the model................ 4 - 2025-03-23 16:04:20,104 - log
| epoch   4 step      400 |     85 batches | lr 0.00016 | ms/batch 366.09 | loss  2.93 | avg loss  3.01 | ppl 20.32 - 2025-03-23 16:04:56,713 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-23 16:05:05,366 - log
start to train the model................ 5 - 2025-03-23 16:05:06,752 - log
| epoch   5 step      500 |     80 batches | lr 0.0002 | ms/batch 344.34 | loss  2.92 | avg loss  2.89 | ppl 18.03 - 2025-03-23 16:05:41,185 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.525.pt - 2025-03-23 16:05:51,979 - log
---------------------------------------------------------------------------------------------------- - 2025-03-23 16:05:53,330 - log
End of training - 2025-03-23 16:05:53,330 - log
cleanup dist ... - 2025-03-23 16:06:22,907 - log
==================================================================================================== - 2025-03-24 07:53:42,798 - log
        - platform : local - 2025-03-24 07:53:42,799 - log
        - local_rank : 0 - 2025-03-24 07:53:42,799 - log
        - rank : 0 - 2025-03-24 07:53:42,799 - log
        - device : cuda:0 - 2025-03-24 07:53:42,799 - log
        - world_size : 1 - 2025-03-24 07:53:42,799 - log
        - random_seed : 110 - 2025-03-24 07:53:42,799 - log
        - lr : 0.0002 - 2025-03-24 07:53:42,799 - log
        - weight_decay : 0.01 - 2025-03-24 07:53:42,799 - log
        - correct_bias : True - 2025-03-24 07:53:42,799 - log
        - adam_epislon : 1e-06 - 2025-03-24 07:53:42,799 - log
        - no_decay_bias : False - 2025-03-24 07:53:42,799 - log
        - adam_beta1 : 0.9 - 2025-03-24 07:53:42,799 - log
        - adam_beta2 : 0.999 - 2025-03-24 07:53:42,799 - log
        - scheduler : linear - 2025-03-24 07:53:42,799 - log
        - max_step : None - 2025-03-24 07:53:42,799 - log
        - max_epoch : 5 - 2025-03-24 07:53:42,799 - log
        - warmup_step : 500 - 2025-03-24 07:53:42,799 - log
        - i_steps : 0 - 2025-03-24 07:53:42,799 - log
        - i_lrs : 0.00025 - 2025-03-24 07:53:42,799 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 07:53:42,799 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 07:53:42,799 - log
        - train_batch_size : 4 - 2025-03-24 07:53:42,799 - log
        - valid_batch_size : 2 - 2025-03-24 07:53:42,799 - log
        - grad_acc : 2 - 2025-03-24 07:53:42,799 - log
        - clip : 0.0 - 2025-03-24 07:53:42,799 - log
        - seq_len : 256 - 2025-03-24 07:53:42,799 - log
        - model_card : gpt2.md - 2025-03-24 07:53:42,799 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 07:53:42,799 - log
        - fp16 : False - 2025-03-24 07:53:42,799 - log
        - log_interval : 100 - 2025-03-24 07:53:42,799 - log
        - eval_interval : 2000 - 2025-03-24 07:53:42,799 - log
        - save_interval : 1000 - 2025-03-24 07:53:42,799 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 07:53:42,799 - log
        - lora_dim : 4 - 2025-03-24 07:53:42,799 - log
        - lora_alpha : 32 - 2025-03-24 07:53:42,799 - log
        - obj : clm - 2025-03-24 07:53:42,799 - log
        - lora_dropout : 0.1 - 2025-03-24 07:53:42,799 - log
        - label_smooth : 0.1 - 2025-03-24 07:53:42,799 - log
        - roll_interval : -1 - 2025-03-24 07:53:42,799 - log
        - roll_lr : 1e-05 - 2025-03-24 07:53:42,799 - log
        - roll_step : 100 - 2025-03-24 07:53:42,799 - log
        - eval_epoch : 1 - 2025-03-24 07:53:42,799 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 07:53:42,799 - log
==================================================================================================== - 2025-03-24 07:53:42,799 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 07:53:42,799 - log
loading model pretrained weight. - 2025-03-24 07:53:45,385 - log
set max_step: 525 - 2025-03-24 07:53:49,796 - log
start to train the model................ 1 - 2025-03-24 07:53:49,965 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 07:54:09,512 - log
Exiting from training early - 2025-03-24 07:54:09,512 - log
cleanup dist ... - 2025-03-24 07:54:09,736 - log
==================================================================================================== - 2025-03-24 07:59:37,544 - log
        - platform : local - 2025-03-24 07:59:37,545 - log
        - local_rank : 0 - 2025-03-24 07:59:37,545 - log
        - rank : 0 - 2025-03-24 07:59:37,545 - log
        - device : cuda:0 - 2025-03-24 07:59:37,545 - log
        - world_size : 1 - 2025-03-24 07:59:37,545 - log
        - random_seed : 110 - 2025-03-24 07:59:37,545 - log
        - lr : 0.0002 - 2025-03-24 07:59:37,545 - log
        - weight_decay : 0.01 - 2025-03-24 07:59:37,545 - log
        - correct_bias : True - 2025-03-24 07:59:37,545 - log
        - adam_epislon : 1e-06 - 2025-03-24 07:59:37,545 - log
        - no_decay_bias : False - 2025-03-24 07:59:37,545 - log
        - adam_beta1 : 0.9 - 2025-03-24 07:59:37,545 - log
        - adam_beta2 : 0.999 - 2025-03-24 07:59:37,545 - log
        - scheduler : linear - 2025-03-24 07:59:37,545 - log
        - max_step : None - 2025-03-24 07:59:37,545 - log
        - max_epoch : 5 - 2025-03-24 07:59:37,545 - log
        - warmup_step : 500 - 2025-03-24 07:59:37,545 - log
        - i_steps : 0 - 2025-03-24 07:59:37,545 - log
        - i_lrs : 0.00025 - 2025-03-24 07:59:37,545 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 07:59:37,545 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 07:59:37,545 - log
        - train_batch_size : 4 - 2025-03-24 07:59:37,545 - log
        - valid_batch_size : 2 - 2025-03-24 07:59:37,545 - log
        - grad_acc : 2 - 2025-03-24 07:59:37,545 - log
        - clip : 0.0 - 2025-03-24 07:59:37,545 - log
        - seq_len : 256 - 2025-03-24 07:59:37,545 - log
        - model_card : gpt2.md - 2025-03-24 07:59:37,545 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 07:59:37,545 - log
        - fp16 : False - 2025-03-24 07:59:37,545 - log
        - log_interval : 100 - 2025-03-24 07:59:37,545 - log
        - eval_interval : 2000 - 2025-03-24 07:59:37,545 - log
        - save_interval : 1000 - 2025-03-24 07:59:37,545 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 07:59:37,545 - log
        - lora_dim : 4 - 2025-03-24 07:59:37,545 - log
        - lora_alpha : 32 - 2025-03-24 07:59:37,545 - log
        - obj : clm - 2025-03-24 07:59:37,545 - log
        - lora_dropout : 0.1 - 2025-03-24 07:59:37,545 - log
        - label_smooth : 0.1 - 2025-03-24 07:59:37,545 - log
        - roll_interval : -1 - 2025-03-24 07:59:37,545 - log
        - roll_lr : 1e-05 - 2025-03-24 07:59:37,545 - log
        - roll_step : 100 - 2025-03-24 07:59:37,545 - log
        - eval_epoch : 1 - 2025-03-24 07:59:37,545 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 07:59:37,545 - log
==================================================================================================== - 2025-03-24 07:59:37,545 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 07:59:37,545 - log
loading model pretrained weight. - 2025-03-24 07:59:40,053 - log
set max_step: 525 - 2025-03-24 07:59:44,666 - log
start to train the model................ 1 - 2025-03-24 07:59:44,796 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 07:59:56,092 - log
Exiting from training early - 2025-03-24 07:59:56,092 - log
cleanup dist ... - 2025-03-24 07:59:56,274 - log
==================================================================================================== - 2025-03-24 08:03:39,858 - log
        - platform : local - 2025-03-24 08:03:39,859 - log
        - local_rank : 0 - 2025-03-24 08:03:39,859 - log
        - rank : 0 - 2025-03-24 08:03:39,859 - log
        - device : cuda:0 - 2025-03-24 08:03:39,859 - log
        - world_size : 1 - 2025-03-24 08:03:39,859 - log
        - random_seed : 110 - 2025-03-24 08:03:39,859 - log
        - lr : 0.0002 - 2025-03-24 08:03:39,859 - log
        - weight_decay : 0.01 - 2025-03-24 08:03:39,859 - log
        - correct_bias : True - 2025-03-24 08:03:39,859 - log
        - adam_epislon : 1e-06 - 2025-03-24 08:03:39,859 - log
        - no_decay_bias : False - 2025-03-24 08:03:39,859 - log
        - adam_beta1 : 0.9 - 2025-03-24 08:03:39,859 - log
        - adam_beta2 : 0.999 - 2025-03-24 08:03:39,859 - log
        - scheduler : linear - 2025-03-24 08:03:39,859 - log
        - max_step : None - 2025-03-24 08:03:39,859 - log
        - max_epoch : 5 - 2025-03-24 08:03:39,859 - log
        - warmup_step : 500 - 2025-03-24 08:03:39,859 - log
        - i_steps : 0 - 2025-03-24 08:03:39,859 - log
        - i_lrs : 0.00025 - 2025-03-24 08:03:39,859 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 08:03:39,859 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 08:03:39,859 - log
        - train_batch_size : 4 - 2025-03-24 08:03:39,859 - log
        - valid_batch_size : 2 - 2025-03-24 08:03:39,859 - log
        - grad_acc : 2 - 2025-03-24 08:03:39,859 - log
        - clip : 0.0 - 2025-03-24 08:03:39,859 - log
        - seq_len : 256 - 2025-03-24 08:03:39,859 - log
        - model_card : gpt2.md - 2025-03-24 08:03:39,859 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 08:03:39,859 - log
        - fp16 : False - 2025-03-24 08:03:39,859 - log
        - log_interval : 100 - 2025-03-24 08:03:39,859 - log
        - eval_interval : 2000 - 2025-03-24 08:03:39,859 - log
        - save_interval : 1000 - 2025-03-24 08:03:39,859 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 08:03:39,859 - log
        - lora_dim : 4 - 2025-03-24 08:03:39,859 - log
        - lora_alpha : 32 - 2025-03-24 08:03:39,859 - log
        - obj : clm - 2025-03-24 08:03:39,859 - log
        - lora_dropout : 0.1 - 2025-03-24 08:03:39,859 - log
        - label_smooth : 0.1 - 2025-03-24 08:03:39,859 - log
        - roll_interval : -1 - 2025-03-24 08:03:39,859 - log
        - roll_lr : 1e-05 - 2025-03-24 08:03:39,859 - log
        - roll_step : 100 - 2025-03-24 08:03:39,859 - log
        - eval_epoch : 1 - 2025-03-24 08:03:39,859 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 08:03:39,859 - log
==================================================================================================== - 2025-03-24 08:03:39,859 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 08:03:39,859 - log
loading model pretrained weight. - 2025-03-24 08:03:42,470 - log
set max_step: 525 - 2025-03-24 08:03:46,538 - log
start to train the model................ 1 - 2025-03-24 08:03:46,694 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 08:03:48,203 - log
Exiting from training early - 2025-03-24 08:03:48,203 - log
cleanup dist ... - 2025-03-24 08:03:48,409 - log
==================================================================================================== - 2025-03-24 08:30:34,278 - log
        - platform : local - 2025-03-24 08:30:34,278 - log
        - local_rank : 0 - 2025-03-24 08:30:34,278 - log
        - rank : 0 - 2025-03-24 08:30:34,278 - log
        - device : cuda:0 - 2025-03-24 08:30:34,278 - log
        - world_size : 1 - 2025-03-24 08:30:34,278 - log
        - random_seed : 110 - 2025-03-24 08:30:34,278 - log
        - lr : 0.0002 - 2025-03-24 08:30:34,278 - log
        - weight_decay : 0.01 - 2025-03-24 08:30:34,278 - log
        - correct_bias : True - 2025-03-24 08:30:34,278 - log
        - adam_epislon : 1e-06 - 2025-03-24 08:30:34,278 - log
        - no_decay_bias : False - 2025-03-24 08:30:34,278 - log
        - adam_beta1 : 0.9 - 2025-03-24 08:30:34,278 - log
        - adam_beta2 : 0.999 - 2025-03-24 08:30:34,278 - log
        - scheduler : linear - 2025-03-24 08:30:34,278 - log
        - max_step : None - 2025-03-24 08:30:34,278 - log
        - max_epoch : 5 - 2025-03-24 08:30:34,278 - log
        - warmup_step : 500 - 2025-03-24 08:30:34,278 - log
        - i_steps : 0 - 2025-03-24 08:30:34,278 - log
        - i_lrs : 0.00025 - 2025-03-24 08:30:34,278 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 08:30:34,278 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 08:30:34,278 - log
        - train_batch_size : 4 - 2025-03-24 08:30:34,278 - log
        - valid_batch_size : 2 - 2025-03-24 08:30:34,278 - log
        - grad_acc : 2 - 2025-03-24 08:30:34,279 - log
        - clip : 0.0 - 2025-03-24 08:30:34,279 - log
        - seq_len : 256 - 2025-03-24 08:30:34,279 - log
        - model_card : gpt2.md - 2025-03-24 08:30:34,279 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 08:30:34,279 - log
        - fp16 : False - 2025-03-24 08:30:34,279 - log
        - log_interval : 100 - 2025-03-24 08:30:34,279 - log
        - eval_interval : 2000 - 2025-03-24 08:30:34,279 - log
        - save_interval : 1000 - 2025-03-24 08:30:34,279 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 08:30:34,279 - log
        - lora_dim : 4 - 2025-03-24 08:30:34,279 - log
        - lora_alpha : 32 - 2025-03-24 08:30:34,279 - log
        - obj : clm - 2025-03-24 08:30:34,279 - log
        - lora_dropout : 0.1 - 2025-03-24 08:30:34,279 - log
        - label_smooth : 0.1 - 2025-03-24 08:30:34,279 - log
        - roll_interval : -1 - 2025-03-24 08:30:34,279 - log
        - roll_lr : 1e-05 - 2025-03-24 08:30:34,279 - log
        - roll_step : 100 - 2025-03-24 08:30:34,279 - log
        - eval_epoch : 1 - 2025-03-24 08:30:34,279 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 08:30:34,279 - log
==================================================================================================== - 2025-03-24 08:30:34,279 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 08:30:34,279 - log
loading model pretrained weight. - 2025-03-24 08:30:36,789 - log
set max_step: 525 - 2025-03-24 08:30:40,927 - log
start to train the model................ 1 - 2025-03-24 08:30:41,099 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 08:30:45,459 - log
Exiting from training early - 2025-03-24 08:30:45,459 - log
cleanup dist ... - 2025-03-24 08:30:45,683 - log
==================================================================================================== - 2025-03-24 08:49:28,078 - log
        - platform : local - 2025-03-24 08:49:28,078 - log
        - local_rank : 0 - 2025-03-24 08:49:28,078 - log
        - rank : 0 - 2025-03-24 08:49:28,078 - log
        - device : cuda:0 - 2025-03-24 08:49:28,078 - log
        - world_size : 1 - 2025-03-24 08:49:28,078 - log
        - random_seed : 110 - 2025-03-24 08:49:28,078 - log
        - lr : 0.0002 - 2025-03-24 08:49:28,078 - log
        - weight_decay : 0.01 - 2025-03-24 08:49:28,078 - log
        - correct_bias : True - 2025-03-24 08:49:28,078 - log
        - adam_epislon : 1e-06 - 2025-03-24 08:49:28,078 - log
        - no_decay_bias : False - 2025-03-24 08:49:28,078 - log
        - adam_beta1 : 0.9 - 2025-03-24 08:49:28,078 - log
        - adam_beta2 : 0.999 - 2025-03-24 08:49:28,078 - log
        - scheduler : linear - 2025-03-24 08:49:28,078 - log
        - max_step : None - 2025-03-24 08:49:28,078 - log
        - max_epoch : 5 - 2025-03-24 08:49:28,078 - log
        - warmup_step : 500 - 2025-03-24 08:49:28,078 - log
        - i_steps : 0 - 2025-03-24 08:49:28,078 - log
        - i_lrs : 0.00025 - 2025-03-24 08:49:28,078 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 08:49:28,078 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 08:49:28,078 - log
        - train_batch_size : 4 - 2025-03-24 08:49:28,078 - log
        - valid_batch_size : 2 - 2025-03-24 08:49:28,078 - log
        - grad_acc : 2 - 2025-03-24 08:49:28,078 - log
        - clip : 0.0 - 2025-03-24 08:49:28,078 - log
        - seq_len : 256 - 2025-03-24 08:49:28,078 - log
        - model_card : gpt2.md - 2025-03-24 08:49:28,078 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 08:49:28,078 - log
        - fp16 : False - 2025-03-24 08:49:28,078 - log
        - log_interval : 100 - 2025-03-24 08:49:28,078 - log
        - eval_interval : 2000 - 2025-03-24 08:49:28,078 - log
        - save_interval : 1000 - 2025-03-24 08:49:28,078 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 08:49:28,078 - log
        - lora_dim : 4 - 2025-03-24 08:49:28,078 - log
        - lora_alpha : 32 - 2025-03-24 08:49:28,079 - log
        - obj : clm - 2025-03-24 08:49:28,079 - log
        - lora_dropout : 0.1 - 2025-03-24 08:49:28,079 - log
        - label_smooth : 0.1 - 2025-03-24 08:49:28,079 - log
        - roll_interval : -1 - 2025-03-24 08:49:28,079 - log
        - roll_lr : 1e-05 - 2025-03-24 08:49:28,079 - log
        - roll_step : 100 - 2025-03-24 08:49:28,079 - log
        - eval_epoch : 1 - 2025-03-24 08:49:28,079 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 08:49:28,079 - log
==================================================================================================== - 2025-03-24 08:49:28,079 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 08:49:28,079 - log
loading model pretrained weight. - 2025-03-24 08:49:30,749 - log
set max_step: 525 - 2025-03-24 08:49:35,268 - log
start to train the model................ 1 - 2025-03-24 08:49:35,430 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 08:49:37,231 - log
Exiting from training early - 2025-03-24 08:49:37,231 - log
cleanup dist ... - 2025-03-24 08:49:37,456 - log
==================================================================================================== - 2025-03-24 09:02:38,868 - log
        - platform : local - 2025-03-24 09:02:38,869 - log
        - local_rank : 0 - 2025-03-24 09:02:38,869 - log
        - rank : 0 - 2025-03-24 09:02:38,869 - log
        - device : cuda:0 - 2025-03-24 09:02:38,869 - log
        - world_size : 1 - 2025-03-24 09:02:38,869 - log
        - random_seed : 110 - 2025-03-24 09:02:38,869 - log
        - lr : 0.0002 - 2025-03-24 09:02:38,869 - log
        - weight_decay : 0.01 - 2025-03-24 09:02:38,869 - log
        - correct_bias : True - 2025-03-24 09:02:38,869 - log
        - adam_epislon : 1e-06 - 2025-03-24 09:02:38,869 - log
        - no_decay_bias : False - 2025-03-24 09:02:38,869 - log
        - adam_beta1 : 0.9 - 2025-03-24 09:02:38,869 - log
        - adam_beta2 : 0.999 - 2025-03-24 09:02:38,869 - log
        - scheduler : linear - 2025-03-24 09:02:38,869 - log
        - max_step : None - 2025-03-24 09:02:38,869 - log
        - max_epoch : 5 - 2025-03-24 09:02:38,869 - log
        - warmup_step : 500 - 2025-03-24 09:02:38,869 - log
        - i_steps : 0 - 2025-03-24 09:02:38,869 - log
        - i_lrs : 0.00025 - 2025-03-24 09:02:38,869 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 09:02:38,869 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 09:02:38,869 - log
        - train_batch_size : 4 - 2025-03-24 09:02:38,869 - log
        - valid_batch_size : 2 - 2025-03-24 09:02:38,869 - log
        - grad_acc : 2 - 2025-03-24 09:02:38,870 - log
        - clip : 0.0 - 2025-03-24 09:02:38,870 - log
        - seq_len : 256 - 2025-03-24 09:02:38,870 - log
        - model_card : gpt2.md - 2025-03-24 09:02:38,870 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 09:02:38,870 - log
        - fp16 : False - 2025-03-24 09:02:38,870 - log
        - log_interval : 100 - 2025-03-24 09:02:38,870 - log
        - eval_interval : 2000 - 2025-03-24 09:02:38,870 - log
        - save_interval : 1000 - 2025-03-24 09:02:38,870 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 09:02:38,870 - log
        - lora_dim : 4 - 2025-03-24 09:02:38,870 - log
        - lora_alpha : 32 - 2025-03-24 09:02:38,870 - log
        - obj : clm - 2025-03-24 09:02:38,870 - log
        - lora_dropout : 0.1 - 2025-03-24 09:02:38,870 - log
        - label_smooth : 0.1 - 2025-03-24 09:02:38,870 - log
        - roll_interval : -1 - 2025-03-24 09:02:38,870 - log
        - roll_lr : 1e-05 - 2025-03-24 09:02:38,870 - log
        - roll_step : 100 - 2025-03-24 09:02:38,870 - log
        - eval_epoch : 1 - 2025-03-24 09:02:38,870 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 09:02:38,870 - log
==================================================================================================== - 2025-03-24 09:02:38,870 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 09:02:38,870 - log
loading model pretrained weight. - 2025-03-24 09:02:41,424 - log
set max_step: 525 - 2025-03-24 09:02:46,027 - log
start to train the model................ 1 - 2025-03-24 09:02:46,198 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 09:02:47,619 - log
Exiting from training early - 2025-03-24 09:02:47,619 - log
cleanup dist ... - 2025-03-24 09:02:47,847 - log
==================================================================================================== - 2025-03-24 09:04:05,423 - log
        - platform : local - 2025-03-24 09:04:05,423 - log
        - local_rank : 0 - 2025-03-24 09:04:05,423 - log
        - rank : 0 - 2025-03-24 09:04:05,423 - log
        - device : cuda:0 - 2025-03-24 09:04:05,423 - log
        - world_size : 1 - 2025-03-24 09:04:05,423 - log
        - random_seed : 110 - 2025-03-24 09:04:05,423 - log
        - lr : 0.0002 - 2025-03-24 09:04:05,423 - log
        - weight_decay : 0.01 - 2025-03-24 09:04:05,424 - log
        - correct_bias : True - 2025-03-24 09:04:05,424 - log
        - adam_epislon : 1e-06 - 2025-03-24 09:04:05,424 - log
        - no_decay_bias : False - 2025-03-24 09:04:05,424 - log
        - adam_beta1 : 0.9 - 2025-03-24 09:04:05,424 - log
        - adam_beta2 : 0.999 - 2025-03-24 09:04:05,424 - log
        - scheduler : linear - 2025-03-24 09:04:05,424 - log
        - max_step : None - 2025-03-24 09:04:05,424 - log
        - max_epoch : 5 - 2025-03-24 09:04:05,424 - log
        - warmup_step : 500 - 2025-03-24 09:04:05,424 - log
        - i_steps : 0 - 2025-03-24 09:04:05,424 - log
        - i_lrs : 0.00025 - 2025-03-24 09:04:05,424 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 09:04:05,424 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 09:04:05,424 - log
        - train_batch_size : 4 - 2025-03-24 09:04:05,424 - log
        - valid_batch_size : 2 - 2025-03-24 09:04:05,424 - log
        - grad_acc : 2 - 2025-03-24 09:04:05,424 - log
        - clip : 0.0 - 2025-03-24 09:04:05,424 - log
        - seq_len : 256 - 2025-03-24 09:04:05,424 - log
        - model_card : gpt2.md - 2025-03-24 09:04:05,424 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 09:04:05,424 - log
        - fp16 : False - 2025-03-24 09:04:05,424 - log
        - log_interval : 100 - 2025-03-24 09:04:05,424 - log
        - eval_interval : 2000 - 2025-03-24 09:04:05,424 - log
        - save_interval : 1000 - 2025-03-24 09:04:05,424 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 09:04:05,424 - log
        - lora_dim : 4 - 2025-03-24 09:04:05,424 - log
        - lora_alpha : 32 - 2025-03-24 09:04:05,424 - log
        - obj : clm - 2025-03-24 09:04:05,424 - log
        - lora_dropout : 0.1 - 2025-03-24 09:04:05,424 - log
        - label_smooth : 0.1 - 2025-03-24 09:04:05,424 - log
        - roll_interval : -1 - 2025-03-24 09:04:05,424 - log
        - roll_lr : 1e-05 - 2025-03-24 09:04:05,424 - log
        - roll_step : 100 - 2025-03-24 09:04:05,424 - log
        - eval_epoch : 1 - 2025-03-24 09:04:05,424 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 09:04:05,424 - log
==================================================================================================== - 2025-03-24 09:04:05,424 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 09:04:05,424 - log
loading model pretrained weight. - 2025-03-24 09:04:08,017 - log
set max_step: 525 - 2025-03-24 09:04:12,506 - log
start to train the model................ 1 - 2025-03-24 09:04:12,636 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 09:04:18,343 - log
Exiting from training early - 2025-03-24 09:04:18,343 - log
cleanup dist ... - 2025-03-24 09:04:18,532 - log
==================================================================================================== - 2025-03-24 09:12:30,754 - log
        - platform : local - 2025-03-24 09:12:30,755 - log
        - local_rank : 0 - 2025-03-24 09:12:30,755 - log
        - rank : 0 - 2025-03-24 09:12:30,755 - log
        - device : cuda:0 - 2025-03-24 09:12:30,755 - log
        - world_size : 1 - 2025-03-24 09:12:30,755 - log
        - random_seed : 110 - 2025-03-24 09:12:30,755 - log
        - lr : 0.0002 - 2025-03-24 09:12:30,755 - log
        - weight_decay : 0.01 - 2025-03-24 09:12:30,755 - log
        - correct_bias : True - 2025-03-24 09:12:30,755 - log
        - adam_epislon : 1e-06 - 2025-03-24 09:12:30,755 - log
        - no_decay_bias : False - 2025-03-24 09:12:30,755 - log
        - adam_beta1 : 0.9 - 2025-03-24 09:12:30,755 - log
        - adam_beta2 : 0.999 - 2025-03-24 09:12:30,755 - log
        - scheduler : linear - 2025-03-24 09:12:30,755 - log
        - max_step : None - 2025-03-24 09:12:30,755 - log
        - max_epoch : 5 - 2025-03-24 09:12:30,755 - log
        - warmup_step : 500 - 2025-03-24 09:12:30,755 - log
        - i_steps : 0 - 2025-03-24 09:12:30,755 - log
        - i_lrs : 0.00025 - 2025-03-24 09:12:30,755 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 09:12:30,755 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 09:12:30,755 - log
        - train_batch_size : 4 - 2025-03-24 09:12:30,755 - log
        - valid_batch_size : 2 - 2025-03-24 09:12:30,755 - log
        - grad_acc : 2 - 2025-03-24 09:12:30,755 - log
        - clip : 0.0 - 2025-03-24 09:12:30,755 - log
        - seq_len : 256 - 2025-03-24 09:12:30,755 - log
        - model_card : gpt2.md - 2025-03-24 09:12:30,755 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 09:12:30,755 - log
        - fp16 : False - 2025-03-24 09:12:30,755 - log
        - log_interval : 100 - 2025-03-24 09:12:30,755 - log
        - eval_interval : 2000 - 2025-03-24 09:12:30,755 - log
        - save_interval : 1000 - 2025-03-24 09:12:30,755 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 09:12:30,755 - log
        - lora_dim : 4 - 2025-03-24 09:12:30,755 - log
        - lora_alpha : 32 - 2025-03-24 09:12:30,755 - log
        - obj : clm - 2025-03-24 09:12:30,755 - log
        - lora_dropout : 0.1 - 2025-03-24 09:12:30,755 - log
        - label_smooth : 0.1 - 2025-03-24 09:12:30,755 - log
        - roll_interval : -1 - 2025-03-24 09:12:30,755 - log
        - roll_lr : 1e-05 - 2025-03-24 09:12:30,755 - log
        - roll_step : 100 - 2025-03-24 09:12:30,755 - log
        - eval_epoch : 1 - 2025-03-24 09:12:30,755 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 09:12:30,755 - log
==================================================================================================== - 2025-03-24 09:12:30,755 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 09:12:30,755 - log
loading model pretrained weight. - 2025-03-24 09:12:33,275 - log
==================================================================================================== - 2025-03-24 09:14:51,307 - log
        - platform : local - 2025-03-24 09:14:51,307 - log
        - local_rank : 0 - 2025-03-24 09:14:51,307 - log
        - rank : 0 - 2025-03-24 09:14:51,307 - log
        - device : cuda:0 - 2025-03-24 09:14:51,307 - log
        - world_size : 1 - 2025-03-24 09:14:51,307 - log
        - random_seed : 110 - 2025-03-24 09:14:51,307 - log
        - lr : 0.0002 - 2025-03-24 09:14:51,307 - log
        - weight_decay : 0.01 - 2025-03-24 09:14:51,307 - log
        - correct_bias : True - 2025-03-24 09:14:51,307 - log
        - adam_epislon : 1e-06 - 2025-03-24 09:14:51,307 - log
        - no_decay_bias : False - 2025-03-24 09:14:51,307 - log
        - adam_beta1 : 0.9 - 2025-03-24 09:14:51,307 - log
        - adam_beta2 : 0.999 - 2025-03-24 09:14:51,307 - log
        - scheduler : linear - 2025-03-24 09:14:51,307 - log
        - max_step : None - 2025-03-24 09:14:51,307 - log
        - max_epoch : 5 - 2025-03-24 09:14:51,307 - log
        - warmup_step : 500 - 2025-03-24 09:14:51,307 - log
        - i_steps : 0 - 2025-03-24 09:14:51,307 - log
        - i_lrs : 0.00025 - 2025-03-24 09:14:51,307 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 09:14:51,307 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 09:14:51,307 - log
        - train_batch_size : 4 - 2025-03-24 09:14:51,307 - log
        - valid_batch_size : 2 - 2025-03-24 09:14:51,307 - log
        - grad_acc : 2 - 2025-03-24 09:14:51,307 - log
        - clip : 0.0 - 2025-03-24 09:14:51,307 - log
        - seq_len : 256 - 2025-03-24 09:14:51,307 - log
        - model_card : gpt2.md - 2025-03-24 09:14:51,307 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 09:14:51,307 - log
        - fp16 : False - 2025-03-24 09:14:51,307 - log
        - log_interval : 100 - 2025-03-24 09:14:51,307 - log
        - eval_interval : 2000 - 2025-03-24 09:14:51,307 - log
        - save_interval : 1000 - 2025-03-24 09:14:51,307 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 09:14:51,307 - log
        - lora_dim : 4 - 2025-03-24 09:14:51,307 - log
        - lora_alpha : 32 - 2025-03-24 09:14:51,307 - log
        - obj : clm - 2025-03-24 09:14:51,307 - log
        - lora_dropout : 0.1 - 2025-03-24 09:14:51,307 - log
        - label_smooth : 0.1 - 2025-03-24 09:14:51,307 - log
        - roll_interval : -1 - 2025-03-24 09:14:51,307 - log
        - roll_lr : 1e-05 - 2025-03-24 09:14:51,307 - log
        - roll_step : 100 - 2025-03-24 09:14:51,307 - log
        - eval_epoch : 1 - 2025-03-24 09:14:51,307 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 09:14:51,307 - log
==================================================================================================== - 2025-03-24 09:14:51,307 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 09:14:51,307 - log
loading model pretrained weight. - 2025-03-24 09:14:53,896 - log
set max_step: 525 - 2025-03-24 09:14:57,218 - log
start to train the model................ 1 - 2025-03-24 09:14:57,380 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 09:15:01,684 - log
Exiting from training early - 2025-03-24 09:15:01,684 - log
cleanup dist ... - 2025-03-24 09:15:01,896 - log
==================================================================================================== - 2025-03-24 09:36:00,543 - log
        - platform : local - 2025-03-24 09:36:00,544 - log
        - local_rank : 0 - 2025-03-24 09:36:00,544 - log
        - rank : 0 - 2025-03-24 09:36:00,544 - log
        - device : cuda:0 - 2025-03-24 09:36:00,544 - log
        - world_size : 1 - 2025-03-24 09:36:00,544 - log
        - random_seed : 110 - 2025-03-24 09:36:00,544 - log
        - lr : 0.0002 - 2025-03-24 09:36:00,544 - log
        - weight_decay : 0.01 - 2025-03-24 09:36:00,544 - log
        - correct_bias : True - 2025-03-24 09:36:00,544 - log
        - adam_epislon : 1e-06 - 2025-03-24 09:36:00,544 - log
        - no_decay_bias : False - 2025-03-24 09:36:00,544 - log
        - adam_beta1 : 0.9 - 2025-03-24 09:36:00,544 - log
        - adam_beta2 : 0.999 - 2025-03-24 09:36:00,544 - log
        - scheduler : linear - 2025-03-24 09:36:00,544 - log
        - max_step : None - 2025-03-24 09:36:00,544 - log
        - max_epoch : 5 - 2025-03-24 09:36:00,544 - log
        - warmup_step : 500 - 2025-03-24 09:36:00,544 - log
        - i_steps : 0 - 2025-03-24 09:36:00,544 - log
        - i_lrs : 0.00025 - 2025-03-24 09:36:00,544 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 09:36:00,544 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 09:36:00,544 - log
        - train_batch_size : 4 - 2025-03-24 09:36:00,544 - log
        - valid_batch_size : 2 - 2025-03-24 09:36:00,544 - log
        - grad_acc : 2 - 2025-03-24 09:36:00,544 - log
        - clip : 0.0 - 2025-03-24 09:36:00,544 - log
        - seq_len : 256 - 2025-03-24 09:36:00,544 - log
        - model_card : gpt2.md - 2025-03-24 09:36:00,544 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 09:36:00,544 - log
        - fp16 : False - 2025-03-24 09:36:00,544 - log
        - log_interval : 100 - 2025-03-24 09:36:00,544 - log
        - eval_interval : 2000 - 2025-03-24 09:36:00,544 - log
        - save_interval : 1000 - 2025-03-24 09:36:00,544 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 09:36:00,544 - log
        - lora_dim : 4 - 2025-03-24 09:36:00,544 - log
        - lora_alpha : 32 - 2025-03-24 09:36:00,544 - log
        - obj : clm - 2025-03-24 09:36:00,544 - log
        - lora_dropout : 0.1 - 2025-03-24 09:36:00,544 - log
        - label_smooth : 0.1 - 2025-03-24 09:36:00,544 - log
        - roll_interval : -1 - 2025-03-24 09:36:00,544 - log
        - roll_lr : 1e-05 - 2025-03-24 09:36:00,544 - log
        - roll_step : 100 - 2025-03-24 09:36:00,544 - log
        - eval_epoch : 1 - 2025-03-24 09:36:00,544 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 09:36:00,544 - log
==================================================================================================== - 2025-03-24 09:36:00,544 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 09:36:00,544 - log
loading model pretrained weight. - 2025-03-24 09:36:03,130 - log
==================================================================================================== - 2025-03-24 09:37:41,122 - log
        - platform : local - 2025-03-24 09:37:41,123 - log
        - local_rank : 0 - 2025-03-24 09:37:41,123 - log
        - rank : 0 - 2025-03-24 09:37:41,123 - log
        - device : cuda:0 - 2025-03-24 09:37:41,123 - log
        - world_size : 1 - 2025-03-24 09:37:41,123 - log
        - random_seed : 110 - 2025-03-24 09:37:41,123 - log
        - lr : 0.0002 - 2025-03-24 09:37:41,123 - log
        - weight_decay : 0.01 - 2025-03-24 09:37:41,123 - log
        - correct_bias : True - 2025-03-24 09:37:41,123 - log
        - adam_epislon : 1e-06 - 2025-03-24 09:37:41,123 - log
        - no_decay_bias : False - 2025-03-24 09:37:41,123 - log
        - adam_beta1 : 0.9 - 2025-03-24 09:37:41,123 - log
        - adam_beta2 : 0.999 - 2025-03-24 09:37:41,123 - log
        - scheduler : linear - 2025-03-24 09:37:41,123 - log
        - max_step : None - 2025-03-24 09:37:41,123 - log
        - max_epoch : 5 - 2025-03-24 09:37:41,123 - log
        - warmup_step : 500 - 2025-03-24 09:37:41,123 - log
        - i_steps : 0 - 2025-03-24 09:37:41,123 - log
        - i_lrs : 0.00025 - 2025-03-24 09:37:41,123 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 09:37:41,123 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 09:37:41,123 - log
        - train_batch_size : 4 - 2025-03-24 09:37:41,123 - log
        - valid_batch_size : 2 - 2025-03-24 09:37:41,123 - log
        - grad_acc : 2 - 2025-03-24 09:37:41,123 - log
        - clip : 0.0 - 2025-03-24 09:37:41,123 - log
        - seq_len : 256 - 2025-03-24 09:37:41,124 - log
        - model_card : gpt2.md - 2025-03-24 09:37:41,124 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 09:37:41,124 - log
        - fp16 : False - 2025-03-24 09:37:41,124 - log
        - log_interval : 100 - 2025-03-24 09:37:41,124 - log
        - eval_interval : 2000 - 2025-03-24 09:37:41,124 - log
        - save_interval : 1000 - 2025-03-24 09:37:41,124 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 09:37:41,124 - log
        - lora_dim : 4 - 2025-03-24 09:37:41,124 - log
        - lora_alpha : 32 - 2025-03-24 09:37:41,124 - log
        - obj : clm - 2025-03-24 09:37:41,124 - log
        - lora_dropout : 0.1 - 2025-03-24 09:37:41,124 - log
        - label_smooth : 0.1 - 2025-03-24 09:37:41,124 - log
        - roll_interval : -1 - 2025-03-24 09:37:41,124 - log
        - roll_lr : 1e-05 - 2025-03-24 09:37:41,124 - log
        - roll_step : 100 - 2025-03-24 09:37:41,124 - log
        - eval_epoch : 1 - 2025-03-24 09:37:41,124 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 09:37:41,124 - log
==================================================================================================== - 2025-03-24 09:37:41,124 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 09:37:41,124 - log
loading model pretrained weight. - 2025-03-24 09:37:43,776 - log
set max_step: 525 - 2025-03-24 09:37:48,551 - log
start to train the model................ 1 - 2025-03-24 09:37:48,720 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 09:37:50,344 - log
Exiting from training early - 2025-03-24 09:37:50,344 - log
cleanup dist ... - 2025-03-24 09:37:50,586 - log
==================================================================================================== - 2025-03-24 09:42:01,285 - log
        - platform : local - 2025-03-24 09:42:01,286 - log
        - local_rank : 0 - 2025-03-24 09:42:01,286 - log
        - rank : 0 - 2025-03-24 09:42:01,286 - log
        - device : cuda:0 - 2025-03-24 09:42:01,286 - log
        - world_size : 1 - 2025-03-24 09:42:01,287 - log
        - random_seed : 110 - 2025-03-24 09:42:01,287 - log
        - lr : 0.0002 - 2025-03-24 09:42:01,287 - log
        - weight_decay : 0.01 - 2025-03-24 09:42:01,287 - log
        - correct_bias : True - 2025-03-24 09:42:01,287 - log
        - adam_epislon : 1e-06 - 2025-03-24 09:42:01,287 - log
        - no_decay_bias : False - 2025-03-24 09:42:01,287 - log
        - adam_beta1 : 0.9 - 2025-03-24 09:42:01,287 - log
        - adam_beta2 : 0.999 - 2025-03-24 09:42:01,287 - log
        - scheduler : linear - 2025-03-24 09:42:01,287 - log
        - max_step : None - 2025-03-24 09:42:01,287 - log
        - max_epoch : 5 - 2025-03-24 09:42:01,287 - log
        - warmup_step : 500 - 2025-03-24 09:42:01,287 - log
        - i_steps : 0 - 2025-03-24 09:42:01,287 - log
        - i_lrs : 0.00025 - 2025-03-24 09:42:01,287 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 09:42:01,287 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 09:42:01,287 - log
        - train_batch_size : 4 - 2025-03-24 09:42:01,287 - log
        - valid_batch_size : 2 - 2025-03-24 09:42:01,287 - log
        - grad_acc : 2 - 2025-03-24 09:42:01,287 - log
        - clip : 0.0 - 2025-03-24 09:42:01,287 - log
        - seq_len : 256 - 2025-03-24 09:42:01,287 - log
        - model_card : gpt2.md - 2025-03-24 09:42:01,287 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 09:42:01,287 - log
        - fp16 : False - 2025-03-24 09:42:01,287 - log
        - log_interval : 100 - 2025-03-24 09:42:01,287 - log
        - eval_interval : 2000 - 2025-03-24 09:42:01,287 - log
        - save_interval : 1000 - 2025-03-24 09:42:01,287 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 09:42:01,287 - log
        - lora_dim : 4 - 2025-03-24 09:42:01,287 - log
        - lora_alpha : 32 - 2025-03-24 09:42:01,287 - log
        - obj : clm - 2025-03-24 09:42:01,287 - log
        - lora_dropout : 0.1 - 2025-03-24 09:42:01,287 - log
        - label_smooth : 0.1 - 2025-03-24 09:42:01,287 - log
        - roll_interval : -1 - 2025-03-24 09:42:01,287 - log
        - roll_lr : 1e-05 - 2025-03-24 09:42:01,287 - log
        - roll_step : 100 - 2025-03-24 09:42:01,287 - log
        - eval_epoch : 1 - 2025-03-24 09:42:01,287 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 09:42:01,287 - log
==================================================================================================== - 2025-03-24 09:42:01,287 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 09:42:01,287 - log
loading model pretrained weight. - 2025-03-24 09:42:03,819 - log
set max_step: 525 - 2025-03-24 09:42:08,183 - log
start to train the model................ 1 - 2025-03-24 09:42:08,346 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 09:42:17,538 - log
Exiting from training early - 2025-03-24 09:42:17,539 - log
cleanup dist ... - 2025-03-24 09:42:17,770 - log
==================================================================================================== - 2025-03-24 09:43:46,398 - log
        - platform : local - 2025-03-24 09:43:46,398 - log
        - local_rank : 0 - 2025-03-24 09:43:46,398 - log
        - rank : 0 - 2025-03-24 09:43:46,398 - log
        - device : cuda:0 - 2025-03-24 09:43:46,398 - log
        - world_size : 1 - 2025-03-24 09:43:46,398 - log
        - random_seed : 110 - 2025-03-24 09:43:46,398 - log
        - lr : 0.0002 - 2025-03-24 09:43:46,398 - log
        - weight_decay : 0.01 - 2025-03-24 09:43:46,398 - log
        - correct_bias : True - 2025-03-24 09:43:46,398 - log
        - adam_epislon : 1e-06 - 2025-03-24 09:43:46,398 - log
        - no_decay_bias : False - 2025-03-24 09:43:46,398 - log
        - adam_beta1 : 0.9 - 2025-03-24 09:43:46,398 - log
        - adam_beta2 : 0.999 - 2025-03-24 09:43:46,398 - log
        - scheduler : linear - 2025-03-24 09:43:46,398 - log
        - max_step : None - 2025-03-24 09:43:46,398 - log
        - max_epoch : 5 - 2025-03-24 09:43:46,398 - log
        - warmup_step : 500 - 2025-03-24 09:43:46,398 - log
        - i_steps : 0 - 2025-03-24 09:43:46,398 - log
        - i_lrs : 0.00025 - 2025-03-24 09:43:46,398 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 09:43:46,398 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 09:43:46,398 - log
        - train_batch_size : 4 - 2025-03-24 09:43:46,398 - log
        - valid_batch_size : 2 - 2025-03-24 09:43:46,398 - log
        - grad_acc : 2 - 2025-03-24 09:43:46,399 - log
        - clip : 0.0 - 2025-03-24 09:43:46,399 - log
        - seq_len : 256 - 2025-03-24 09:43:46,399 - log
        - model_card : gpt2.md - 2025-03-24 09:43:46,399 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 09:43:46,399 - log
        - fp16 : False - 2025-03-24 09:43:46,399 - log
        - log_interval : 100 - 2025-03-24 09:43:46,399 - log
        - eval_interval : 2000 - 2025-03-24 09:43:46,399 - log
        - save_interval : 1000 - 2025-03-24 09:43:46,399 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 09:43:46,399 - log
        - lora_dim : 4 - 2025-03-24 09:43:46,399 - log
        - lora_alpha : 32 - 2025-03-24 09:43:46,399 - log
        - obj : clm - 2025-03-24 09:43:46,399 - log
        - lora_dropout : 0.1 - 2025-03-24 09:43:46,399 - log
        - label_smooth : 0.1 - 2025-03-24 09:43:46,399 - log
        - roll_interval : -1 - 2025-03-24 09:43:46,399 - log
        - roll_lr : 1e-05 - 2025-03-24 09:43:46,399 - log
        - roll_step : 100 - 2025-03-24 09:43:46,399 - log
        - eval_epoch : 1 - 2025-03-24 09:43:46,399 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 09:43:46,399 - log
==================================================================================================== - 2025-03-24 09:43:46,399 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 09:43:46,399 - log
loading model pretrained weight. - 2025-03-24 09:43:49,038 - log
==================================================================================================== - 2025-03-24 10:15:27,749 - log
        - platform : local - 2025-03-24 10:15:27,749 - log
        - local_rank : 0 - 2025-03-24 10:15:27,749 - log
        - rank : 0 - 2025-03-24 10:15:27,749 - log
        - device : cuda:0 - 2025-03-24 10:15:27,749 - log
        - world_size : 1 - 2025-03-24 10:15:27,749 - log
        - random_seed : 110 - 2025-03-24 10:15:27,749 - log
        - lr : 0.0002 - 2025-03-24 10:15:27,749 - log
        - weight_decay : 0.01 - 2025-03-24 10:15:27,749 - log
        - correct_bias : True - 2025-03-24 10:15:27,749 - log
        - adam_epislon : 1e-06 - 2025-03-24 10:15:27,749 - log
        - no_decay_bias : False - 2025-03-24 10:15:27,749 - log
        - adam_beta1 : 0.9 - 2025-03-24 10:15:27,749 - log
        - adam_beta2 : 0.999 - 2025-03-24 10:15:27,749 - log
        - scheduler : linear - 2025-03-24 10:15:27,749 - log
        - max_step : None - 2025-03-24 10:15:27,749 - log
        - max_epoch : 5 - 2025-03-24 10:15:27,749 - log
        - warmup_step : 500 - 2025-03-24 10:15:27,749 - log
        - i_steps : 0 - 2025-03-24 10:15:27,749 - log
        - i_lrs : 0.00025 - 2025-03-24 10:15:27,749 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 10:15:27,749 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 10:15:27,749 - log
        - train_batch_size : 4 - 2025-03-24 10:15:27,749 - log
        - valid_batch_size : 2 - 2025-03-24 10:15:27,749 - log
        - grad_acc : 2 - 2025-03-24 10:15:27,749 - log
        - clip : 0.0 - 2025-03-24 10:15:27,749 - log
        - seq_len : 256 - 2025-03-24 10:15:27,749 - log
        - model_card : gpt2.md - 2025-03-24 10:15:27,749 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 10:15:27,749 - log
        - fp16 : False - 2025-03-24 10:15:27,749 - log
        - log_interval : 100 - 2025-03-24 10:15:27,749 - log
        - eval_interval : 2000 - 2025-03-24 10:15:27,749 - log
        - save_interval : 1000 - 2025-03-24 10:15:27,749 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 10:15:27,749 - log
        - lora_dim : 4 - 2025-03-24 10:15:27,750 - log
        - lora_alpha : 32 - 2025-03-24 10:15:27,750 - log
        - obj : clm - 2025-03-24 10:15:27,750 - log
        - lora_dropout : 0.1 - 2025-03-24 10:15:27,750 - log
        - label_smooth : 0.1 - 2025-03-24 10:15:27,750 - log
        - roll_interval : -1 - 2025-03-24 10:15:27,750 - log
        - roll_lr : 1e-05 - 2025-03-24 10:15:27,750 - log
        - roll_step : 100 - 2025-03-24 10:15:27,750 - log
        - eval_epoch : 1 - 2025-03-24 10:15:27,750 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 10:15:27,750 - log
==================================================================================================== - 2025-03-24 10:15:27,750 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 10:15:27,750 - log
loading model pretrained weight. - 2025-03-24 10:15:30,212 - log
set max_step: 525 - 2025-03-24 10:15:35,685 - log
start to train the model................ 1 - 2025-03-24 10:15:35,858 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 10:15:37,193 - log
Exiting from training early - 2025-03-24 10:15:37,193 - log
cleanup dist ... - 2025-03-24 10:15:37,404 - log
==================================================================================================== - 2025-03-24 10:16:13,705 - log
        - platform : local - 2025-03-24 10:16:13,705 - log
        - local_rank : 0 - 2025-03-24 10:16:13,705 - log
        - rank : 0 - 2025-03-24 10:16:13,705 - log
        - device : cuda:0 - 2025-03-24 10:16:13,705 - log
        - world_size : 1 - 2025-03-24 10:16:13,705 - log
        - random_seed : 110 - 2025-03-24 10:16:13,705 - log
        - lr : 0.0002 - 2025-03-24 10:16:13,705 - log
        - weight_decay : 0.01 - 2025-03-24 10:16:13,705 - log
        - correct_bias : True - 2025-03-24 10:16:13,705 - log
        - adam_epislon : 1e-06 - 2025-03-24 10:16:13,705 - log
        - no_decay_bias : False - 2025-03-24 10:16:13,705 - log
        - adam_beta1 : 0.9 - 2025-03-24 10:16:13,705 - log
        - adam_beta2 : 0.999 - 2025-03-24 10:16:13,705 - log
        - scheduler : linear - 2025-03-24 10:16:13,705 - log
        - max_step : None - 2025-03-24 10:16:13,705 - log
        - max_epoch : 5 - 2025-03-24 10:16:13,705 - log
        - warmup_step : 500 - 2025-03-24 10:16:13,705 - log
        - i_steps : 0 - 2025-03-24 10:16:13,705 - log
        - i_lrs : 0.00025 - 2025-03-24 10:16:13,705 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 10:16:13,705 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 10:16:13,705 - log
        - train_batch_size : 4 - 2025-03-24 10:16:13,705 - log
        - valid_batch_size : 2 - 2025-03-24 10:16:13,705 - log
        - grad_acc : 2 - 2025-03-24 10:16:13,705 - log
        - clip : 0.0 - 2025-03-24 10:16:13,705 - log
        - seq_len : 256 - 2025-03-24 10:16:13,705 - log
        - model_card : gpt2.md - 2025-03-24 10:16:13,705 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 10:16:13,705 - log
        - fp16 : False - 2025-03-24 10:16:13,705 - log
        - log_interval : 100 - 2025-03-24 10:16:13,705 - log
        - eval_interval : 2000 - 2025-03-24 10:16:13,705 - log
        - save_interval : 1000 - 2025-03-24 10:16:13,705 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 10:16:13,705 - log
        - lora_dim : 4 - 2025-03-24 10:16:13,705 - log
        - lora_alpha : 32 - 2025-03-24 10:16:13,705 - log
        - obj : clm - 2025-03-24 10:16:13,705 - log
        - lora_dropout : 0.1 - 2025-03-24 10:16:13,705 - log
        - label_smooth : 0.1 - 2025-03-24 10:16:13,705 - log
        - roll_interval : -1 - 2025-03-24 10:16:13,705 - log
        - roll_lr : 1e-05 - 2025-03-24 10:16:13,705 - log
        - roll_step : 100 - 2025-03-24 10:16:13,705 - log
        - eval_epoch : 1 - 2025-03-24 10:16:13,705 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 10:16:13,705 - log
==================================================================================================== - 2025-03-24 10:16:13,705 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 10:16:13,705 - log
loading model pretrained weight. - 2025-03-24 10:16:16,207 - log
==================================================================================================== - 2025-03-24 10:19:32,129 - log
        - platform : local - 2025-03-24 10:19:32,129 - log
        - local_rank : 0 - 2025-03-24 10:19:32,129 - log
        - rank : 0 - 2025-03-24 10:19:32,129 - log
        - device : cuda:0 - 2025-03-24 10:19:32,129 - log
        - world_size : 1 - 2025-03-24 10:19:32,129 - log
        - random_seed : 110 - 2025-03-24 10:19:32,129 - log
        - lr : 0.0002 - 2025-03-24 10:19:32,129 - log
        - weight_decay : 0.01 - 2025-03-24 10:19:32,129 - log
        - correct_bias : True - 2025-03-24 10:19:32,129 - log
        - adam_epislon : 1e-06 - 2025-03-24 10:19:32,129 - log
        - no_decay_bias : False - 2025-03-24 10:19:32,129 - log
        - adam_beta1 : 0.9 - 2025-03-24 10:19:32,129 - log
        - adam_beta2 : 0.999 - 2025-03-24 10:19:32,129 - log
        - scheduler : linear - 2025-03-24 10:19:32,129 - log
        - max_step : None - 2025-03-24 10:19:32,129 - log
        - max_epoch : 5 - 2025-03-24 10:19:32,129 - log
        - warmup_step : 500 - 2025-03-24 10:19:32,129 - log
        - i_steps : 0 - 2025-03-24 10:19:32,129 - log
        - i_lrs : 0.00025 - 2025-03-24 10:19:32,129 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 10:19:32,129 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 10:19:32,129 - log
        - train_batch_size : 4 - 2025-03-24 10:19:32,129 - log
        - valid_batch_size : 2 - 2025-03-24 10:19:32,129 - log
        - grad_acc : 2 - 2025-03-24 10:19:32,129 - log
        - clip : 0.0 - 2025-03-24 10:19:32,129 - log
        - seq_len : 256 - 2025-03-24 10:19:32,129 - log
        - model_card : gpt2.md - 2025-03-24 10:19:32,129 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 10:19:32,129 - log
        - fp16 : False - 2025-03-24 10:19:32,129 - log
        - log_interval : 100 - 2025-03-24 10:19:32,129 - log
        - eval_interval : 2000 - 2025-03-24 10:19:32,129 - log
        - save_interval : 1000 - 2025-03-24 10:19:32,129 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 10:19:32,129 - log
        - lora_dim : 4 - 2025-03-24 10:19:32,129 - log
        - lora_alpha : 32 - 2025-03-24 10:19:32,129 - log
        - obj : clm - 2025-03-24 10:19:32,129 - log
        - lora_dropout : 0.1 - 2025-03-24 10:19:32,129 - log
        - label_smooth : 0.1 - 2025-03-24 10:19:32,129 - log
        - roll_interval : -1 - 2025-03-24 10:19:32,129 - log
        - roll_lr : 1e-05 - 2025-03-24 10:19:32,129 - log
        - roll_step : 100 - 2025-03-24 10:19:32,129 - log
        - eval_epoch : 1 - 2025-03-24 10:19:32,129 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 10:19:32,129 - log
==================================================================================================== - 2025-03-24 10:19:32,129 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 10:19:32,129 - log
loading model pretrained weight. - 2025-03-24 10:19:34,625 - log
==================================================================================================== - 2025-03-24 10:20:29,820 - log
        - platform : local - 2025-03-24 10:20:29,820 - log
        - local_rank : 0 - 2025-03-24 10:20:29,820 - log
        - rank : 0 - 2025-03-24 10:20:29,820 - log
        - device : cuda:0 - 2025-03-24 10:20:29,820 - log
        - world_size : 1 - 2025-03-24 10:20:29,820 - log
        - random_seed : 110 - 2025-03-24 10:20:29,820 - log
        - lr : 0.0002 - 2025-03-24 10:20:29,820 - log
        - weight_decay : 0.01 - 2025-03-24 10:20:29,820 - log
        - correct_bias : True - 2025-03-24 10:20:29,820 - log
        - adam_epislon : 1e-06 - 2025-03-24 10:20:29,821 - log
        - no_decay_bias : False - 2025-03-24 10:20:29,821 - log
        - adam_beta1 : 0.9 - 2025-03-24 10:20:29,821 - log
        - adam_beta2 : 0.999 - 2025-03-24 10:20:29,821 - log
        - scheduler : linear - 2025-03-24 10:20:29,821 - log
        - max_step : None - 2025-03-24 10:20:29,821 - log
        - max_epoch : 5 - 2025-03-24 10:20:29,821 - log
        - warmup_step : 500 - 2025-03-24 10:20:29,821 - log
        - i_steps : 0 - 2025-03-24 10:20:29,821 - log
        - i_lrs : 0.00025 - 2025-03-24 10:20:29,821 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 10:20:29,821 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 10:20:29,821 - log
        - train_batch_size : 4 - 2025-03-24 10:20:29,821 - log
        - valid_batch_size : 2 - 2025-03-24 10:20:29,821 - log
        - grad_acc : 2 - 2025-03-24 10:20:29,821 - log
        - clip : 0.0 - 2025-03-24 10:20:29,821 - log
        - seq_len : 256 - 2025-03-24 10:20:29,821 - log
        - model_card : gpt2.md - 2025-03-24 10:20:29,821 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 10:20:29,821 - log
        - fp16 : False - 2025-03-24 10:20:29,821 - log
        - log_interval : 100 - 2025-03-24 10:20:29,821 - log
        - eval_interval : 2000 - 2025-03-24 10:20:29,821 - log
        - save_interval : 1000 - 2025-03-24 10:20:29,821 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 10:20:29,821 - log
        - lora_dim : 4 - 2025-03-24 10:20:29,821 - log
        - lora_alpha : 32 - 2025-03-24 10:20:29,821 - log
        - obj : clm - 2025-03-24 10:20:29,821 - log
        - lora_dropout : 0.1 - 2025-03-24 10:20:29,821 - log
        - label_smooth : 0.1 - 2025-03-24 10:20:29,821 - log
        - roll_interval : -1 - 2025-03-24 10:20:29,821 - log
        - roll_lr : 1e-05 - 2025-03-24 10:20:29,821 - log
        - roll_step : 100 - 2025-03-24 10:20:29,821 - log
        - eval_epoch : 1 - 2025-03-24 10:20:29,821 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 10:20:29,821 - log
==================================================================================================== - 2025-03-24 10:20:29,821 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 10:20:29,821 - log
loading model pretrained weight. - 2025-03-24 10:20:32,390 - log
==================================================================================================== - 2025-03-24 10:21:35,598 - log
        - platform : local - 2025-03-24 10:21:35,598 - log
        - local_rank : 0 - 2025-03-24 10:21:35,598 - log
        - rank : 0 - 2025-03-24 10:21:35,598 - log
        - device : cuda:0 - 2025-03-24 10:21:35,598 - log
        - world_size : 1 - 2025-03-24 10:21:35,598 - log
        - random_seed : 110 - 2025-03-24 10:21:35,598 - log
        - lr : 0.0002 - 2025-03-24 10:21:35,598 - log
        - weight_decay : 0.01 - 2025-03-24 10:21:35,598 - log
        - correct_bias : True - 2025-03-24 10:21:35,598 - log
        - adam_epislon : 1e-06 - 2025-03-24 10:21:35,598 - log
        - no_decay_bias : False - 2025-03-24 10:21:35,598 - log
        - adam_beta1 : 0.9 - 2025-03-24 10:21:35,598 - log
        - adam_beta2 : 0.999 - 2025-03-24 10:21:35,598 - log
        - scheduler : linear - 2025-03-24 10:21:35,598 - log
        - max_step : None - 2025-03-24 10:21:35,598 - log
        - max_epoch : 5 - 2025-03-24 10:21:35,598 - log
        - warmup_step : 500 - 2025-03-24 10:21:35,598 - log
        - i_steps : 0 - 2025-03-24 10:21:35,598 - log
        - i_lrs : 0.00025 - 2025-03-24 10:21:35,598 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 10:21:35,598 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 10:21:35,598 - log
        - train_batch_size : 4 - 2025-03-24 10:21:35,598 - log
        - valid_batch_size : 2 - 2025-03-24 10:21:35,598 - log
        - grad_acc : 2 - 2025-03-24 10:21:35,598 - log
        - clip : 0.0 - 2025-03-24 10:21:35,598 - log
        - seq_len : 256 - 2025-03-24 10:21:35,598 - log
        - model_card : gpt2.md - 2025-03-24 10:21:35,598 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 10:21:35,598 - log
        - fp16 : False - 2025-03-24 10:21:35,598 - log
        - log_interval : 100 - 2025-03-24 10:21:35,598 - log
        - eval_interval : 2000 - 2025-03-24 10:21:35,598 - log
        - save_interval : 1000 - 2025-03-24 10:21:35,598 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 10:21:35,598 - log
        - lora_dim : 4 - 2025-03-24 10:21:35,598 - log
        - lora_alpha : 32 - 2025-03-24 10:21:35,598 - log
        - obj : clm - 2025-03-24 10:21:35,598 - log
        - lora_dropout : 0.1 - 2025-03-24 10:21:35,598 - log
        - label_smooth : 0.1 - 2025-03-24 10:21:35,598 - log
        - roll_interval : -1 - 2025-03-24 10:21:35,598 - log
        - roll_lr : 1e-05 - 2025-03-24 10:21:35,598 - log
        - roll_step : 100 - 2025-03-24 10:21:35,598 - log
        - eval_epoch : 1 - 2025-03-24 10:21:35,598 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 10:21:35,598 - log
==================================================================================================== - 2025-03-24 10:21:35,598 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 10:21:35,598 - log
loading model pretrained weight. - 2025-03-24 10:21:38,068 - log
set max_step: 525 - 2025-03-24 10:21:40,427 - log
start to train the model................ 1 - 2025-03-24 10:21:40,547 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 10:21:41,548 - log
Exiting from training early - 2025-03-24 10:21:41,548 - log
==================================================================================================== - 2025-03-24 10:22:53,536 - log
        - platform : local - 2025-03-24 10:22:53,537 - log
        - local_rank : 0 - 2025-03-24 10:22:53,537 - log
        - rank : 0 - 2025-03-24 10:22:53,537 - log
        - device : cuda:0 - 2025-03-24 10:22:53,537 - log
        - world_size : 1 - 2025-03-24 10:22:53,537 - log
        - random_seed : 110 - 2025-03-24 10:22:53,537 - log
        - lr : 0.0002 - 2025-03-24 10:22:53,537 - log
        - weight_decay : 0.01 - 2025-03-24 10:22:53,537 - log
        - correct_bias : True - 2025-03-24 10:22:53,537 - log
        - adam_epislon : 1e-06 - 2025-03-24 10:22:53,537 - log
        - no_decay_bias : False - 2025-03-24 10:22:53,537 - log
        - adam_beta1 : 0.9 - 2025-03-24 10:22:53,537 - log
        - adam_beta2 : 0.999 - 2025-03-24 10:22:53,537 - log
        - scheduler : linear - 2025-03-24 10:22:53,537 - log
        - max_step : None - 2025-03-24 10:22:53,537 - log
        - max_epoch : 5 - 2025-03-24 10:22:53,537 - log
        - warmup_step : 500 - 2025-03-24 10:22:53,537 - log
        - i_steps : 0 - 2025-03-24 10:22:53,537 - log
        - i_lrs : 0.00025 - 2025-03-24 10:22:53,537 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 10:22:53,537 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 10:22:53,537 - log
        - train_batch_size : 4 - 2025-03-24 10:22:53,537 - log
        - valid_batch_size : 2 - 2025-03-24 10:22:53,537 - log
        - grad_acc : 2 - 2025-03-24 10:22:53,537 - log
        - clip : 0.0 - 2025-03-24 10:22:53,537 - log
        - seq_len : 256 - 2025-03-24 10:22:53,537 - log
        - model_card : gpt2.md - 2025-03-24 10:22:53,537 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 10:22:53,537 - log
        - fp16 : False - 2025-03-24 10:22:53,537 - log
        - log_interval : 100 - 2025-03-24 10:22:53,537 - log
        - eval_interval : 2000 - 2025-03-24 10:22:53,537 - log
        - save_interval : 1000 - 2025-03-24 10:22:53,537 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 10:22:53,537 - log
        - lora_dim : 4 - 2025-03-24 10:22:53,537 - log
        - lora_alpha : 32 - 2025-03-24 10:22:53,537 - log
        - obj : clm - 2025-03-24 10:22:53,537 - log
        - lora_dropout : 0.1 - 2025-03-24 10:22:53,537 - log
        - label_smooth : 0.1 - 2025-03-24 10:22:53,537 - log
        - roll_interval : -1 - 2025-03-24 10:22:53,537 - log
        - roll_lr : 1e-05 - 2025-03-24 10:22:53,537 - log
        - roll_step : 100 - 2025-03-24 10:22:53,537 - log
        - eval_epoch : 1 - 2025-03-24 10:22:53,537 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 10:22:53,537 - log
==================================================================================================== - 2025-03-24 10:22:53,537 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 10:22:53,537 - log
loading model pretrained weight. - 2025-03-24 10:22:56,101 - log
set max_step: 525 - 2025-03-24 10:22:58,367 - log
start to train the model................ 1 - 2025-03-24 10:22:58,484 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 10:23:10,206 - log
Exiting from training early - 2025-03-24 10:23:10,206 - log
==================================================================================================== - 2025-03-24 10:27:47,220 - log
        - platform : local - 2025-03-24 10:27:47,220 - log
        - local_rank : 0 - 2025-03-24 10:27:47,220 - log
        - rank : 0 - 2025-03-24 10:27:47,220 - log
        - device : cuda:0 - 2025-03-24 10:27:47,220 - log
        - world_size : 1 - 2025-03-24 10:27:47,220 - log
        - random_seed : 110 - 2025-03-24 10:27:47,220 - log
        - lr : 0.0002 - 2025-03-24 10:27:47,220 - log
        - weight_decay : 0.01 - 2025-03-24 10:27:47,220 - log
        - correct_bias : True - 2025-03-24 10:27:47,220 - log
        - adam_epislon : 1e-06 - 2025-03-24 10:27:47,220 - log
        - no_decay_bias : False - 2025-03-24 10:27:47,220 - log
        - adam_beta1 : 0.9 - 2025-03-24 10:27:47,220 - log
        - adam_beta2 : 0.999 - 2025-03-24 10:27:47,220 - log
        - scheduler : linear - 2025-03-24 10:27:47,220 - log
        - max_step : None - 2025-03-24 10:27:47,220 - log
        - max_epoch : 5 - 2025-03-24 10:27:47,220 - log
        - warmup_step : 500 - 2025-03-24 10:27:47,220 - log
        - i_steps : 0 - 2025-03-24 10:27:47,220 - log
        - i_lrs : 0.00025 - 2025-03-24 10:27:47,220 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 10:27:47,220 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 10:27:47,220 - log
        - train_batch_size : 4 - 2025-03-24 10:27:47,220 - log
        - valid_batch_size : 2 - 2025-03-24 10:27:47,220 - log
        - grad_acc : 2 - 2025-03-24 10:27:47,220 - log
        - clip : 0.0 - 2025-03-24 10:27:47,220 - log
        - seq_len : 256 - 2025-03-24 10:27:47,220 - log
        - model_card : gpt2.md - 2025-03-24 10:27:47,220 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 10:27:47,220 - log
        - fp16 : False - 2025-03-24 10:27:47,220 - log
        - log_interval : 100 - 2025-03-24 10:27:47,220 - log
        - eval_interval : 2000 - 2025-03-24 10:27:47,220 - log
        - save_interval : 1000 - 2025-03-24 10:27:47,220 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 10:27:47,220 - log
        - lora_dim : 4 - 2025-03-24 10:27:47,220 - log
        - lora_alpha : 32 - 2025-03-24 10:27:47,220 - log
        - obj : clm - 2025-03-24 10:27:47,220 - log
        - lora_dropout : 0.1 - 2025-03-24 10:27:47,220 - log
        - label_smooth : 0.1 - 2025-03-24 10:27:47,220 - log
        - roll_interval : -1 - 2025-03-24 10:27:47,220 - log
        - roll_lr : 1e-05 - 2025-03-24 10:27:47,220 - log
        - roll_step : 100 - 2025-03-24 10:27:47,220 - log
        - eval_epoch : 1 - 2025-03-24 10:27:47,220 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 10:27:47,220 - log
==================================================================================================== - 2025-03-24 10:27:47,220 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 10:27:47,220 - log
loading model pretrained weight. - 2025-03-24 10:27:49,766 - log
set max_step: 525 - 2025-03-24 10:27:51,848 - log
start to train the model................ 1 - 2025-03-24 10:27:51,970 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 10:27:54,003 - log
Exiting from training early - 2025-03-24 10:27:54,003 - log
cleanup dist ... - 2025-03-24 10:27:54,182 - log
==================================================================================================== - 2025-03-24 10:29:30,154 - log
        - platform : local - 2025-03-24 10:29:30,155 - log
        - local_rank : 0 - 2025-03-24 10:29:30,155 - log
        - rank : 0 - 2025-03-24 10:29:30,155 - log
        - device : cuda:0 - 2025-03-24 10:29:30,155 - log
        - world_size : 1 - 2025-03-24 10:29:30,155 - log
        - random_seed : 110 - 2025-03-24 10:29:30,155 - log
        - lr : 0.0002 - 2025-03-24 10:29:30,155 - log
        - weight_decay : 0.01 - 2025-03-24 10:29:30,155 - log
        - correct_bias : True - 2025-03-24 10:29:30,155 - log
        - adam_epislon : 1e-06 - 2025-03-24 10:29:30,155 - log
        - no_decay_bias : False - 2025-03-24 10:29:30,155 - log
        - adam_beta1 : 0.9 - 2025-03-24 10:29:30,155 - log
        - adam_beta2 : 0.999 - 2025-03-24 10:29:30,155 - log
        - scheduler : linear - 2025-03-24 10:29:30,155 - log
        - max_step : None - 2025-03-24 10:29:30,155 - log
        - max_epoch : 5 - 2025-03-24 10:29:30,155 - log
        - warmup_step : 500 - 2025-03-24 10:29:30,155 - log
        - i_steps : 0 - 2025-03-24 10:29:30,155 - log
        - i_lrs : 0.00025 - 2025-03-24 10:29:30,155 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 10:29:30,155 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 10:29:30,155 - log
        - train_batch_size : 4 - 2025-03-24 10:29:30,155 - log
        - valid_batch_size : 2 - 2025-03-24 10:29:30,155 - log
        - grad_acc : 2 - 2025-03-24 10:29:30,155 - log
        - clip : 0.0 - 2025-03-24 10:29:30,155 - log
        - seq_len : 256 - 2025-03-24 10:29:30,155 - log
        - model_card : gpt2.md - 2025-03-24 10:29:30,155 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 10:29:30,155 - log
        - fp16 : False - 2025-03-24 10:29:30,155 - log
        - log_interval : 100 - 2025-03-24 10:29:30,155 - log
        - eval_interval : 2000 - 2025-03-24 10:29:30,155 - log
        - save_interval : 1000 - 2025-03-24 10:29:30,155 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 10:29:30,155 - log
        - lora_dim : 4 - 2025-03-24 10:29:30,155 - log
        - lora_alpha : 32 - 2025-03-24 10:29:30,155 - log
        - obj : clm - 2025-03-24 10:29:30,155 - log
        - lora_dropout : 0.1 - 2025-03-24 10:29:30,155 - log
        - label_smooth : 0.1 - 2025-03-24 10:29:30,155 - log
        - roll_interval : -1 - 2025-03-24 10:29:30,155 - log
        - roll_lr : 1e-05 - 2025-03-24 10:29:30,155 - log
        - roll_step : 100 - 2025-03-24 10:29:30,155 - log
        - eval_epoch : 1 - 2025-03-24 10:29:30,155 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 10:29:30,155 - log
==================================================================================================== - 2025-03-24 10:29:30,155 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 10:29:30,155 - log
loading model pretrained weight. - 2025-03-24 10:29:32,702 - log
set max_step: 525 - 2025-03-24 10:29:34,760 - log
start to train the model................ 1 - 2025-03-24 10:29:34,872 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 10:29:57,059 - log
Exiting from training early - 2025-03-24 10:29:57,060 - log
cleanup dist ... - 2025-03-24 10:29:57,224 - log
==================================================================================================== - 2025-03-24 10:30:01,531 - log
        - platform : local - 2025-03-24 10:30:01,531 - log
        - local_rank : 0 - 2025-03-24 10:30:01,531 - log
        - rank : 0 - 2025-03-24 10:30:01,531 - log
        - device : cuda:0 - 2025-03-24 10:30:01,531 - log
        - world_size : 1 - 2025-03-24 10:30:01,531 - log
        - random_seed : 110 - 2025-03-24 10:30:01,531 - log
        - lr : 0.0002 - 2025-03-24 10:30:01,531 - log
        - weight_decay : 0.01 - 2025-03-24 10:30:01,531 - log
        - correct_bias : True - 2025-03-24 10:30:01,531 - log
        - adam_epislon : 1e-06 - 2025-03-24 10:30:01,531 - log
        - no_decay_bias : False - 2025-03-24 10:30:01,531 - log
        - adam_beta1 : 0.9 - 2025-03-24 10:30:01,531 - log
        - adam_beta2 : 0.999 - 2025-03-24 10:30:01,531 - log
        - scheduler : linear - 2025-03-24 10:30:01,531 - log
        - max_step : None - 2025-03-24 10:30:01,531 - log
        - max_epoch : 5 - 2025-03-24 10:30:01,531 - log
        - warmup_step : 500 - 2025-03-24 10:30:01,531 - log
        - i_steps : 0 - 2025-03-24 10:30:01,531 - log
        - i_lrs : 0.00025 - 2025-03-24 10:30:01,531 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 10:30:01,531 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 10:30:01,531 - log
        - train_batch_size : 4 - 2025-03-24 10:30:01,531 - log
        - valid_batch_size : 2 - 2025-03-24 10:30:01,531 - log
        - grad_acc : 2 - 2025-03-24 10:30:01,531 - log
        - clip : 0.0 - 2025-03-24 10:30:01,531 - log
        - seq_len : 256 - 2025-03-24 10:30:01,531 - log
        - model_card : gpt2.md - 2025-03-24 10:30:01,531 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 10:30:01,531 - log
        - fp16 : False - 2025-03-24 10:30:01,531 - log
        - log_interval : 100 - 2025-03-24 10:30:01,531 - log
        - eval_interval : 2000 - 2025-03-24 10:30:01,531 - log
        - save_interval : 1000 - 2025-03-24 10:30:01,531 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 10:30:01,531 - log
        - lora_dim : 4 - 2025-03-24 10:30:01,531 - log
        - lora_alpha : 32 - 2025-03-24 10:30:01,531 - log
        - obj : clm - 2025-03-24 10:30:01,531 - log
        - lora_dropout : 0.1 - 2025-03-24 10:30:01,531 - log
        - label_smooth : 0.1 - 2025-03-24 10:30:01,531 - log
        - roll_interval : -1 - 2025-03-24 10:30:01,531 - log
        - roll_lr : 1e-05 - 2025-03-24 10:30:01,531 - log
        - roll_step : 100 - 2025-03-24 10:30:01,531 - log
        - eval_epoch : 1 - 2025-03-24 10:30:01,531 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 10:30:01,532 - log
==================================================================================================== - 2025-03-24 10:30:01,532 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 10:30:01,532 - log
loading model pretrained weight. - 2025-03-24 10:30:04,189 - log
set max_step: 525 - 2025-03-24 10:30:06,332 - log
start to train the model................ 1 - 2025-03-24 10:30:06,447 - log
==================================================================================================== - 2025-03-24 10:31:47,999 - log
        - platform : local - 2025-03-24 10:31:47,999 - log
        - local_rank : 0 - 2025-03-24 10:31:47,999 - log
        - rank : 0 - 2025-03-24 10:31:47,999 - log
        - device : cuda:0 - 2025-03-24 10:31:47,999 - log
        - world_size : 1 - 2025-03-24 10:31:47,999 - log
        - random_seed : 110 - 2025-03-24 10:31:47,999 - log
        - lr : 0.0002 - 2025-03-24 10:31:47,999 - log
        - weight_decay : 0.01 - 2025-03-24 10:31:47,999 - log
        - correct_bias : True - 2025-03-24 10:31:47,999 - log
        - adam_epislon : 1e-06 - 2025-03-24 10:31:47,999 - log
        - no_decay_bias : False - 2025-03-24 10:31:47,999 - log
        - adam_beta1 : 0.9 - 2025-03-24 10:31:48,000 - log
        - adam_beta2 : 0.999 - 2025-03-24 10:31:48,000 - log
        - scheduler : linear - 2025-03-24 10:31:48,000 - log
        - max_step : None - 2025-03-24 10:31:48,000 - log
        - max_epoch : 5 - 2025-03-24 10:31:48,000 - log
        - warmup_step : 500 - 2025-03-24 10:31:48,000 - log
        - i_steps : 0 - 2025-03-24 10:31:48,000 - log
        - i_lrs : 0.00025 - 2025-03-24 10:31:48,000 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 10:31:48,000 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 10:31:48,000 - log
        - train_batch_size : 4 - 2025-03-24 10:31:48,000 - log
        - valid_batch_size : 2 - 2025-03-24 10:31:48,000 - log
        - grad_acc : 2 - 2025-03-24 10:31:48,000 - log
        - clip : 0.0 - 2025-03-24 10:31:48,000 - log
        - seq_len : 256 - 2025-03-24 10:31:48,000 - log
        - model_card : gpt2.md - 2025-03-24 10:31:48,000 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 10:31:48,000 - log
        - fp16 : False - 2025-03-24 10:31:48,000 - log
        - log_interval : 100 - 2025-03-24 10:31:48,000 - log
        - eval_interval : 2000 - 2025-03-24 10:31:48,000 - log
        - save_interval : 1000 - 2025-03-24 10:31:48,000 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 10:31:48,000 - log
        - lora_dim : 4 - 2025-03-24 10:31:48,000 - log
        - lora_alpha : 32 - 2025-03-24 10:31:48,000 - log
        - obj : clm - 2025-03-24 10:31:48,000 - log
        - lora_dropout : 0.1 - 2025-03-24 10:31:48,000 - log
        - label_smooth : 0.1 - 2025-03-24 10:31:48,000 - log
        - roll_interval : -1 - 2025-03-24 10:31:48,000 - log
        - roll_lr : 1e-05 - 2025-03-24 10:31:48,000 - log
        - roll_step : 100 - 2025-03-24 10:31:48,000 - log
        - eval_epoch : 1 - 2025-03-24 10:31:48,000 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 10:31:48,000 - log
==================================================================================================== - 2025-03-24 10:31:48,000 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 10:31:48,000 - log
loading model pretrained weight. - 2025-03-24 10:31:50,554 - log
set max_step: 525 - 2025-03-24 10:31:52,523 - log
start to train the model................ 1 - 2025-03-24 10:31:52,635 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 10:32:00,936 - log
Exiting from training early - 2025-03-24 10:32:00,936 - log
cleanup dist ... - 2025-03-24 10:32:01,175 - log
==================================================================================================== - 2025-03-24 10:37:33,660 - log
        - platform : local - 2025-03-24 10:37:33,660 - log
        - local_rank : 0 - 2025-03-24 10:37:33,660 - log
        - rank : 0 - 2025-03-24 10:37:33,660 - log
        - device : cuda:0 - 2025-03-24 10:37:33,660 - log
        - world_size : 1 - 2025-03-24 10:37:33,660 - log
        - random_seed : 110 - 2025-03-24 10:37:33,660 - log
        - lr : 0.0002 - 2025-03-24 10:37:33,660 - log
        - weight_decay : 0.01 - 2025-03-24 10:37:33,660 - log
        - correct_bias : True - 2025-03-24 10:37:33,660 - log
        - adam_epislon : 1e-06 - 2025-03-24 10:37:33,661 - log
        - no_decay_bias : False - 2025-03-24 10:37:33,661 - log
        - adam_beta1 : 0.9 - 2025-03-24 10:37:33,661 - log
        - adam_beta2 : 0.999 - 2025-03-24 10:37:33,661 - log
        - scheduler : linear - 2025-03-24 10:37:33,661 - log
        - max_step : None - 2025-03-24 10:37:33,661 - log
        - max_epoch : 5 - 2025-03-24 10:37:33,661 - log
        - warmup_step : 500 - 2025-03-24 10:37:33,661 - log
        - i_steps : 0 - 2025-03-24 10:37:33,661 - log
        - i_lrs : 0.00025 - 2025-03-24 10:37:33,661 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 10:37:33,661 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 10:37:33,661 - log
        - train_batch_size : 4 - 2025-03-24 10:37:33,661 - log
        - valid_batch_size : 2 - 2025-03-24 10:37:33,661 - log
        - grad_acc : 2 - 2025-03-24 10:37:33,661 - log
        - clip : 0.0 - 2025-03-24 10:37:33,661 - log
        - seq_len : 256 - 2025-03-24 10:37:33,661 - log
        - model_card : gpt2.md - 2025-03-24 10:37:33,661 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 10:37:33,661 - log
        - fp16 : False - 2025-03-24 10:37:33,661 - log
        - log_interval : 100 - 2025-03-24 10:37:33,661 - log
        - eval_interval : 2000 - 2025-03-24 10:37:33,661 - log
        - save_interval : 1000 - 2025-03-24 10:37:33,661 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 10:37:33,661 - log
        - lora_dim : 4 - 2025-03-24 10:37:33,661 - log
        - lora_alpha : 32 - 2025-03-24 10:37:33,661 - log
        - obj : clm - 2025-03-24 10:37:33,661 - log
        - lora_dropout : 0.1 - 2025-03-24 10:37:33,661 - log
        - label_smooth : 0.1 - 2025-03-24 10:37:33,661 - log
        - roll_interval : -1 - 2025-03-24 10:37:33,661 - log
        - roll_lr : 1e-05 - 2025-03-24 10:37:33,661 - log
        - roll_step : 100 - 2025-03-24 10:37:33,661 - log
        - eval_epoch : 1 - 2025-03-24 10:37:33,661 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 10:37:33,661 - log
==================================================================================================== - 2025-03-24 10:37:33,661 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 10:37:33,661 - log
loading model pretrained weight. - 2025-03-24 10:37:36,233 - log
set max_step: 525 - 2025-03-24 10:37:38,413 - log
start to train the model................ 1 - 2025-03-24 10:37:38,531 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 10:38:00,069 - log
Exiting from training early - 2025-03-24 10:38:00,070 - log
==================================================================================================== - 2025-03-24 10:42:12,673 - log
        - platform : local - 2025-03-24 10:42:12,673 - log
        - local_rank : 0 - 2025-03-24 10:42:12,673 - log
        - rank : 0 - 2025-03-24 10:42:12,673 - log
        - device : cuda:0 - 2025-03-24 10:42:12,673 - log
        - world_size : 1 - 2025-03-24 10:42:12,673 - log
        - random_seed : 110 - 2025-03-24 10:42:12,673 - log
        - lr : 0.0002 - 2025-03-24 10:42:12,673 - log
        - weight_decay : 0.01 - 2025-03-24 10:42:12,673 - log
        - correct_bias : True - 2025-03-24 10:42:12,673 - log
        - adam_epislon : 1e-06 - 2025-03-24 10:42:12,673 - log
        - no_decay_bias : False - 2025-03-24 10:42:12,673 - log
        - adam_beta1 : 0.9 - 2025-03-24 10:42:12,673 - log
        - adam_beta2 : 0.999 - 2025-03-24 10:42:12,673 - log
        - scheduler : linear - 2025-03-24 10:42:12,673 - log
        - max_step : None - 2025-03-24 10:42:12,673 - log
        - max_epoch : 5 - 2025-03-24 10:42:12,673 - log
        - warmup_step : 500 - 2025-03-24 10:42:12,673 - log
        - i_steps : 0 - 2025-03-24 10:42:12,673 - log
        - i_lrs : 0.00025 - 2025-03-24 10:42:12,673 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 10:42:12,673 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 10:42:12,673 - log
        - train_batch_size : 4 - 2025-03-24 10:42:12,673 - log
        - valid_batch_size : 2 - 2025-03-24 10:42:12,673 - log
        - grad_acc : 2 - 2025-03-24 10:42:12,673 - log
        - clip : 0.0 - 2025-03-24 10:42:12,673 - log
        - seq_len : 256 - 2025-03-24 10:42:12,673 - log
        - model_card : gpt2.md - 2025-03-24 10:42:12,673 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 10:42:12,673 - log
        - fp16 : False - 2025-03-24 10:42:12,673 - log
        - log_interval : 100 - 2025-03-24 10:42:12,673 - log
        - eval_interval : 2000 - 2025-03-24 10:42:12,673 - log
        - save_interval : 1000 - 2025-03-24 10:42:12,674 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 10:42:12,674 - log
        - lora_dim : 4 - 2025-03-24 10:42:12,674 - log
        - lora_alpha : 32 - 2025-03-24 10:42:12,674 - log
        - obj : clm - 2025-03-24 10:42:12,674 - log
        - lora_dropout : 0.1 - 2025-03-24 10:42:12,674 - log
        - label_smooth : 0.1 - 2025-03-24 10:42:12,674 - log
        - roll_interval : -1 - 2025-03-24 10:42:12,674 - log
        - roll_lr : 1e-05 - 2025-03-24 10:42:12,674 - log
        - roll_step : 100 - 2025-03-24 10:42:12,674 - log
        - eval_epoch : 1 - 2025-03-24 10:42:12,674 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 10:42:12,674 - log
==================================================================================================== - 2025-03-24 10:42:12,674 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 10:42:12,674 - log
loading model pretrained weight. - 2025-03-24 10:42:15,277 - log
set max_step: 525 - 2025-03-24 10:42:17,688 - log
start to train the model................ 1 - 2025-03-24 10:42:17,826 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 10:42:18,005 - log
Exiting from training early - 2025-03-24 10:42:18,005 - log
cleanup dist ... - 2025-03-24 10:42:18,177 - log
==================================================================================================== - 2025-03-24 10:58:56,619 - log
        - platform : local - 2025-03-24 10:58:56,619 - log
        - local_rank : 0 - 2025-03-24 10:58:56,619 - log
        - rank : 0 - 2025-03-24 10:58:56,619 - log
        - device : cuda:0 - 2025-03-24 10:58:56,619 - log
        - world_size : 1 - 2025-03-24 10:58:56,619 - log
        - random_seed : 110 - 2025-03-24 10:58:56,619 - log
        - lr : 0.0002 - 2025-03-24 10:58:56,619 - log
        - weight_decay : 0.01 - 2025-03-24 10:58:56,619 - log
        - correct_bias : True - 2025-03-24 10:58:56,619 - log
        - adam_epislon : 1e-06 - 2025-03-24 10:58:56,619 - log
        - no_decay_bias : False - 2025-03-24 10:58:56,619 - log
        - adam_beta1 : 0.9 - 2025-03-24 10:58:56,619 - log
        - adam_beta2 : 0.999 - 2025-03-24 10:58:56,619 - log
        - scheduler : linear - 2025-03-24 10:58:56,619 - log
        - max_step : None - 2025-03-24 10:58:56,619 - log
        - max_epoch : 5 - 2025-03-24 10:58:56,619 - log
        - warmup_step : 500 - 2025-03-24 10:58:56,619 - log
        - i_steps : 0 - 2025-03-24 10:58:56,619 - log
        - i_lrs : 0.00025 - 2025-03-24 10:58:56,619 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 10:58:56,619 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 10:58:56,619 - log
        - train_batch_size : 4 - 2025-03-24 10:58:56,619 - log
        - valid_batch_size : 2 - 2025-03-24 10:58:56,619 - log
        - grad_acc : 2 - 2025-03-24 10:58:56,619 - log
        - clip : 0.0 - 2025-03-24 10:58:56,619 - log
        - seq_len : 256 - 2025-03-24 10:58:56,619 - log
        - model_card : gpt2.md - 2025-03-24 10:58:56,619 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 10:58:56,619 - log
        - fp16 : False - 2025-03-24 10:58:56,619 - log
        - log_interval : 100 - 2025-03-24 10:58:56,619 - log
        - eval_interval : 2000 - 2025-03-24 10:58:56,619 - log
        - save_interval : 1000 - 2025-03-24 10:58:56,619 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 10:58:56,619 - log
        - lora_dim : 4 - 2025-03-24 10:58:56,619 - log
        - lora_alpha : 32 - 2025-03-24 10:58:56,619 - log
        - obj : clm - 2025-03-24 10:58:56,619 - log
        - lora_dropout : 0.1 - 2025-03-24 10:58:56,619 - log
        - label_smooth : 0.1 - 2025-03-24 10:58:56,619 - log
        - roll_interval : -1 - 2025-03-24 10:58:56,619 - log
        - roll_lr : 1e-05 - 2025-03-24 10:58:56,619 - log
        - roll_step : 100 - 2025-03-24 10:58:56,619 - log
        - eval_epoch : 1 - 2025-03-24 10:58:56,619 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 10:58:56,620 - log
==================================================================================================== - 2025-03-24 10:58:56,620 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 10:58:56,620 - log
loading model pretrained weight. - 2025-03-24 10:58:59,165 - log
set max_step: 525 - 2025-03-24 10:59:02,034 - log
start to train the model................ 1 - 2025-03-24 10:59:02,155 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 10:59:02,757 - log
Exiting from training early - 2025-03-24 10:59:02,757 - log
==================================================================================================== - 2025-03-24 11:17:35,267 - log
        - platform : local - 2025-03-24 11:17:35,267 - log
        - local_rank : 0 - 2025-03-24 11:17:35,267 - log
        - rank : 0 - 2025-03-24 11:17:35,267 - log
        - device : cuda:0 - 2025-03-24 11:17:35,267 - log
        - world_size : 1 - 2025-03-24 11:17:35,267 - log
        - random_seed : 110 - 2025-03-24 11:17:35,267 - log
        - lr : 0.0002 - 2025-03-24 11:17:35,267 - log
        - weight_decay : 0.01 - 2025-03-24 11:17:35,267 - log
        - correct_bias : True - 2025-03-24 11:17:35,267 - log
        - adam_epislon : 1e-06 - 2025-03-24 11:17:35,267 - log
        - no_decay_bias : False - 2025-03-24 11:17:35,267 - log
        - adam_beta1 : 0.9 - 2025-03-24 11:17:35,267 - log
        - adam_beta2 : 0.999 - 2025-03-24 11:17:35,268 - log
        - scheduler : linear - 2025-03-24 11:17:35,268 - log
        - max_step : None - 2025-03-24 11:17:35,268 - log
        - max_epoch : 5 - 2025-03-24 11:17:35,268 - log
        - warmup_step : 500 - 2025-03-24 11:17:35,268 - log
        - i_steps : 0 - 2025-03-24 11:17:35,268 - log
        - i_lrs : 0.00025 - 2025-03-24 11:17:35,268 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 11:17:35,268 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 11:17:35,268 - log
        - train_batch_size : 4 - 2025-03-24 11:17:35,268 - log
        - valid_batch_size : 2 - 2025-03-24 11:17:35,268 - log
        - grad_acc : 2 - 2025-03-24 11:17:35,268 - log
        - clip : 0.0 - 2025-03-24 11:17:35,268 - log
        - seq_len : 256 - 2025-03-24 11:17:35,268 - log
        - model_card : gpt2.md - 2025-03-24 11:17:35,268 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 11:17:35,268 - log
        - fp16 : False - 2025-03-24 11:17:35,268 - log
        - log_interval : 100 - 2025-03-24 11:17:35,268 - log
        - eval_interval : 2000 - 2025-03-24 11:17:35,268 - log
        - save_interval : 1000 - 2025-03-24 11:17:35,268 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 11:17:35,268 - log
        - lora_dim : 4 - 2025-03-24 11:17:35,268 - log
        - lora_alpha : 32 - 2025-03-24 11:17:35,268 - log
        - obj : clm - 2025-03-24 11:17:35,268 - log
        - lora_dropout : 0.1 - 2025-03-24 11:17:35,268 - log
        - label_smooth : 0.1 - 2025-03-24 11:17:35,268 - log
        - roll_interval : -1 - 2025-03-24 11:17:35,268 - log
        - roll_lr : 1e-05 - 2025-03-24 11:17:35,268 - log
        - roll_step : 100 - 2025-03-24 11:17:35,268 - log
        - eval_epoch : 1 - 2025-03-24 11:17:35,268 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 11:17:35,268 - log
==================================================================================================== - 2025-03-24 11:17:35,268 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 11:17:35,268 - log
loading model pretrained weight. - 2025-03-24 11:17:37,821 - log
set max_step: 525 - 2025-03-24 11:17:41,032 - log
start to train the model................ 1 - 2025-03-24 11:17:41,188 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 11:17:42,914 - log
Exiting from training early - 2025-03-24 11:17:42,914 - log
==================================================================================================== - 2025-03-24 11:18:21,157 - log
        - platform : local - 2025-03-24 11:18:21,157 - log
        - local_rank : 0 - 2025-03-24 11:18:21,157 - log
        - rank : 0 - 2025-03-24 11:18:21,157 - log
        - device : cuda:0 - 2025-03-24 11:18:21,157 - log
        - world_size : 1 - 2025-03-24 11:18:21,157 - log
        - random_seed : 110 - 2025-03-24 11:18:21,157 - log
        - lr : 0.0002 - 2025-03-24 11:18:21,157 - log
        - weight_decay : 0.01 - 2025-03-24 11:18:21,157 - log
        - correct_bias : True - 2025-03-24 11:18:21,157 - log
        - adam_epislon : 1e-06 - 2025-03-24 11:18:21,157 - log
        - no_decay_bias : False - 2025-03-24 11:18:21,157 - log
        - adam_beta1 : 0.9 - 2025-03-24 11:18:21,157 - log
        - adam_beta2 : 0.999 - 2025-03-24 11:18:21,157 - log
        - scheduler : linear - 2025-03-24 11:18:21,157 - log
        - max_step : None - 2025-03-24 11:18:21,157 - log
        - max_epoch : 5 - 2025-03-24 11:18:21,157 - log
        - warmup_step : 500 - 2025-03-24 11:18:21,157 - log
        - i_steps : 0 - 2025-03-24 11:18:21,157 - log
        - i_lrs : 0.00025 - 2025-03-24 11:18:21,157 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 11:18:21,157 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 11:18:21,157 - log
        - train_batch_size : 4 - 2025-03-24 11:18:21,157 - log
        - valid_batch_size : 2 - 2025-03-24 11:18:21,157 - log
        - grad_acc : 2 - 2025-03-24 11:18:21,157 - log
        - clip : 0.0 - 2025-03-24 11:18:21,157 - log
        - seq_len : 256 - 2025-03-24 11:18:21,157 - log
        - model_card : gpt2.md - 2025-03-24 11:18:21,157 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 11:18:21,157 - log
        - fp16 : False - 2025-03-24 11:18:21,157 - log
        - log_interval : 100 - 2025-03-24 11:18:21,157 - log
        - eval_interval : 2000 - 2025-03-24 11:18:21,157 - log
        - save_interval : 1000 - 2025-03-24 11:18:21,157 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 11:18:21,157 - log
        - lora_dim : 4 - 2025-03-24 11:18:21,157 - log
        - lora_alpha : 32 - 2025-03-24 11:18:21,157 - log
        - obj : clm - 2025-03-24 11:18:21,157 - log
        - lora_dropout : 0.1 - 2025-03-24 11:18:21,157 - log
        - label_smooth : 0.1 - 2025-03-24 11:18:21,157 - log
        - roll_interval : -1 - 2025-03-24 11:18:21,157 - log
        - roll_lr : 1e-05 - 2025-03-24 11:18:21,158 - log
        - roll_step : 100 - 2025-03-24 11:18:21,158 - log
        - eval_epoch : 1 - 2025-03-24 11:18:21,158 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 11:18:21,158 - log
==================================================================================================== - 2025-03-24 11:18:21,158 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 11:18:21,158 - log
loading model pretrained weight. - 2025-03-24 11:18:23,615 - log
set max_step: 525 - 2025-03-24 11:18:26,118 - log
start to train the model................ 1 - 2025-03-24 11:18:26,238 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 11:18:27,330 - log
Exiting from training early - 2025-03-24 11:18:27,331 - log
cleanup dist ... - 2025-03-24 11:18:27,497 - log
==================================================================================================== - 2025-03-24 12:04:14,255 - log
        - platform : local - 2025-03-24 12:04:14,256 - log
        - local_rank : 0 - 2025-03-24 12:04:14,256 - log
        - rank : 0 - 2025-03-24 12:04:14,256 - log
        - device : cuda:0 - 2025-03-24 12:04:14,256 - log
        - world_size : 1 - 2025-03-24 12:04:14,256 - log
        - random_seed : 110 - 2025-03-24 12:04:14,256 - log
        - lr : 0.0002 - 2025-03-24 12:04:14,256 - log
        - weight_decay : 0.01 - 2025-03-24 12:04:14,256 - log
        - correct_bias : True - 2025-03-24 12:04:14,256 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:04:14,256 - log
        - no_decay_bias : False - 2025-03-24 12:04:14,256 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:04:14,256 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:04:14,256 - log
        - scheduler : linear - 2025-03-24 12:04:14,256 - log
        - max_step : None - 2025-03-24 12:04:14,256 - log
        - max_epoch : 5 - 2025-03-24 12:04:14,256 - log
        - warmup_step : 500 - 2025-03-24 12:04:14,256 - log
        - i_steps : 0 - 2025-03-24 12:04:14,256 - log
        - i_lrs : 0.00025 - 2025-03-24 12:04:14,256 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:04:14,256 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:04:14,256 - log
        - train_batch_size : 4 - 2025-03-24 12:04:14,256 - log
        - valid_batch_size : 2 - 2025-03-24 12:04:14,256 - log
        - grad_acc : 2 - 2025-03-24 12:04:14,256 - log
        - clip : 0.0 - 2025-03-24 12:04:14,256 - log
        - seq_len : 256 - 2025-03-24 12:04:14,256 - log
        - model_card : gpt2.md - 2025-03-24 12:04:14,256 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:04:14,256 - log
        - fp16 : False - 2025-03-24 12:04:14,256 - log
        - log_interval : 100 - 2025-03-24 12:04:14,256 - log
        - eval_interval : 2000 - 2025-03-24 12:04:14,256 - log
        - save_interval : 1000 - 2025-03-24 12:04:14,256 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:04:14,256 - log
        - lora_dim : 4 - 2025-03-24 12:04:14,256 - log
        - lora_alpha : 32 - 2025-03-24 12:04:14,256 - log
        - obj : clm - 2025-03-24 12:04:14,256 - log
        - lora_dropout : 0.1 - 2025-03-24 12:04:14,256 - log
        - label_smooth : 0.1 - 2025-03-24 12:04:14,256 - log
        - roll_interval : -1 - 2025-03-24 12:04:14,256 - log
        - roll_lr : 1e-05 - 2025-03-24 12:04:14,256 - log
        - roll_step : 100 - 2025-03-24 12:04:14,256 - log
        - eval_epoch : 1 - 2025-03-24 12:04:14,256 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:04:14,256 - log
==================================================================================================== - 2025-03-24 12:04:14,256 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:04:14,256 - log
loading model pretrained weight. - 2025-03-24 12:04:16,839 - log
set max_step: 525 - 2025-03-24 12:04:20,144 - log
start to train the model................ 1 - 2025-03-24 12:04:20,286 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 430.28 | loss  5.20 | avg loss  5.73 | ppl 308.19 - 2025-03-24 12:05:03,314 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.105.pt - 2025-03-24 12:05:05,475 - log
start to train the model................ 2 - 2025-03-24 12:05:06,958 - log
| epoch   2 step      200 |     95 batches | lr 8e-05 | ms/batch 408.09 | loss  3.75 | avg loss  4.69 | ppl 109.10 - 2025-03-24 12:05:47,767 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-24 12:05:52,060 - log
start to train the model................ 3 - 2025-03-24 12:05:53,576 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 12:06:22,638 - log
Exiting from training early - 2025-03-24 12:06:22,638 - log
==================================================================================================== - 2025-03-24 12:06:27,036 - log
        - platform : local - 2025-03-24 12:06:27,036 - log
        - local_rank : 0 - 2025-03-24 12:06:27,036 - log
        - rank : 0 - 2025-03-24 12:06:27,036 - log
        - device : cuda:0 - 2025-03-24 12:06:27,036 - log
        - world_size : 1 - 2025-03-24 12:06:27,037 - log
        - random_seed : 110 - 2025-03-24 12:06:27,037 - log
        - lr : 0.0002 - 2025-03-24 12:06:27,037 - log
        - weight_decay : 0.01 - 2025-03-24 12:06:27,037 - log
        - correct_bias : True - 2025-03-24 12:06:27,037 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:06:27,037 - log
        - no_decay_bias : False - 2025-03-24 12:06:27,037 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:06:27,037 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:06:27,037 - log
        - scheduler : linear - 2025-03-24 12:06:27,037 - log
        - max_step : None - 2025-03-24 12:06:27,037 - log
        - max_epoch : 5 - 2025-03-24 12:06:27,037 - log
        - warmup_step : 500 - 2025-03-24 12:06:27,037 - log
        - i_steps : 0 - 2025-03-24 12:06:27,037 - log
        - i_lrs : 0.00025 - 2025-03-24 12:06:27,037 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:06:27,037 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:06:27,037 - log
        - train_batch_size : 4 - 2025-03-24 12:06:27,037 - log
        - valid_batch_size : 2 - 2025-03-24 12:06:27,037 - log
        - grad_acc : 2 - 2025-03-24 12:06:27,037 - log
        - clip : 0.0 - 2025-03-24 12:06:27,037 - log
        - seq_len : 256 - 2025-03-24 12:06:27,037 - log
        - model_card : gpt2.md - 2025-03-24 12:06:27,037 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:06:27,037 - log
        - fp16 : False - 2025-03-24 12:06:27,037 - log
        - log_interval : 100 - 2025-03-24 12:06:27,037 - log
        - eval_interval : 2000 - 2025-03-24 12:06:27,037 - log
        - save_interval : 1000 - 2025-03-24 12:06:27,037 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:06:27,037 - log
        - lora_dim : 4 - 2025-03-24 12:06:27,037 - log
        - lora_alpha : 32 - 2025-03-24 12:06:27,037 - log
        - obj : clm - 2025-03-24 12:06:27,037 - log
        - lora_dropout : 0.1 - 2025-03-24 12:06:27,037 - log
        - label_smooth : 0.1 - 2025-03-24 12:06:27,037 - log
        - roll_interval : -1 - 2025-03-24 12:06:27,037 - log
        - roll_lr : 1e-05 - 2025-03-24 12:06:27,037 - log
        - roll_step : 100 - 2025-03-24 12:06:27,037 - log
        - eval_epoch : 1 - 2025-03-24 12:06:27,037 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:06:27,037 - log
==================================================================================================== - 2025-03-24 12:06:27,037 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:06:27,037 - log
loading model pretrained weight. - 2025-03-24 12:06:29,820 - log
set max_step: 525 - 2025-03-24 12:06:33,882 - log
start to train the model................ 1 - 2025-03-24 12:06:34,001 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 12:06:43,751 - log
Exiting from training early - 2025-03-24 12:06:43,751 - log
cleanup dist ... - 2025-03-24 12:06:43,922 - log
==================================================================================================== - 2025-03-24 12:14:35,571 - log
        - platform : local - 2025-03-24 12:14:35,571 - log
        - local_rank : 0 - 2025-03-24 12:14:35,571 - log
        - rank : 0 - 2025-03-24 12:14:35,571 - log
        - device : cuda:0 - 2025-03-24 12:14:35,571 - log
        - world_size : 1 - 2025-03-24 12:14:35,571 - log
        - random_seed : 110 - 2025-03-24 12:14:35,571 - log
        - lr : 0.0002 - 2025-03-24 12:14:35,571 - log
        - weight_decay : 0.01 - 2025-03-24 12:14:35,571 - log
        - correct_bias : True - 2025-03-24 12:14:35,571 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:14:35,571 - log
        - no_decay_bias : False - 2025-03-24 12:14:35,571 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:14:35,572 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:14:35,572 - log
        - scheduler : linear - 2025-03-24 12:14:35,572 - log
        - max_step : None - 2025-03-24 12:14:35,572 - log
        - max_epoch : 5 - 2025-03-24 12:14:35,572 - log
        - warmup_step : 500 - 2025-03-24 12:14:35,572 - log
        - i_steps : 0 - 2025-03-24 12:14:35,572 - log
        - i_lrs : 0.00025 - 2025-03-24 12:14:35,572 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:14:35,572 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:14:35,572 - log
        - train_batch_size : 4 - 2025-03-24 12:14:35,572 - log
        - valid_batch_size : 2 - 2025-03-24 12:14:35,572 - log
        - grad_acc : 2 - 2025-03-24 12:14:35,572 - log
        - clip : 0.0 - 2025-03-24 12:14:35,572 - log
        - seq_len : 256 - 2025-03-24 12:14:35,572 - log
        - model_card : gpt2.md - 2025-03-24 12:14:35,572 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:14:35,572 - log
        - fp16 : False - 2025-03-24 12:14:35,572 - log
        - log_interval : 100 - 2025-03-24 12:14:35,572 - log
        - eval_interval : 2000 - 2025-03-24 12:14:35,572 - log
        - save_interval : 1000 - 2025-03-24 12:14:35,572 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:14:35,572 - log
        - lora_dim : 4 - 2025-03-24 12:14:35,572 - log
        - lora_alpha : 32 - 2025-03-24 12:14:35,572 - log
        - obj : clm - 2025-03-24 12:14:35,572 - log
        - lora_dropout : 0.1 - 2025-03-24 12:14:35,572 - log
        - label_smooth : 0.1 - 2025-03-24 12:14:35,572 - log
        - roll_interval : -1 - 2025-03-24 12:14:35,572 - log
        - roll_lr : 1e-05 - 2025-03-24 12:14:35,572 - log
        - roll_step : 100 - 2025-03-24 12:14:35,572 - log
        - eval_epoch : 1 - 2025-03-24 12:14:35,572 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:14:35,572 - log
==================================================================================================== - 2025-03-24 12:14:35,572 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:14:35,572 - log
loading model pretrained weight. - 2025-03-24 12:14:38,032 - log
set max_step: 525 - 2025-03-24 12:14:42,015 - log
start to train the model................ 1 - 2025-03-24 12:14:42,132 - log
==================================================================================================== - 2025-03-24 12:15:56,294 - log
        - platform : local - 2025-03-24 12:15:56,294 - log
        - local_rank : 0 - 2025-03-24 12:15:56,294 - log
        - rank : 0 - 2025-03-24 12:15:56,294 - log
        - device : cuda:0 - 2025-03-24 12:15:56,294 - log
        - world_size : 1 - 2025-03-24 12:15:56,294 - log
        - random_seed : 110 - 2025-03-24 12:15:56,294 - log
        - lr : 0.0002 - 2025-03-24 12:15:56,294 - log
        - weight_decay : 0.01 - 2025-03-24 12:15:56,294 - log
        - correct_bias : True - 2025-03-24 12:15:56,294 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:15:56,294 - log
        - no_decay_bias : False - 2025-03-24 12:15:56,294 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:15:56,294 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:15:56,294 - log
        - scheduler : linear - 2025-03-24 12:15:56,294 - log
        - max_step : None - 2025-03-24 12:15:56,294 - log
        - max_epoch : 5 - 2025-03-24 12:15:56,294 - log
        - warmup_step : 500 - 2025-03-24 12:15:56,294 - log
        - i_steps : 0 - 2025-03-24 12:15:56,294 - log
        - i_lrs : 0.00025 - 2025-03-24 12:15:56,294 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:15:56,294 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:15:56,294 - log
        - train_batch_size : 4 - 2025-03-24 12:15:56,294 - log
        - valid_batch_size : 2 - 2025-03-24 12:15:56,294 - log
        - grad_acc : 2 - 2025-03-24 12:15:56,294 - log
        - clip : 0.0 - 2025-03-24 12:15:56,294 - log
        - seq_len : 256 - 2025-03-24 12:15:56,294 - log
        - model_card : gpt2.md - 2025-03-24 12:15:56,294 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:15:56,294 - log
        - fp16 : False - 2025-03-24 12:15:56,294 - log
        - log_interval : 100 - 2025-03-24 12:15:56,294 - log
        - eval_interval : 2000 - 2025-03-24 12:15:56,294 - log
        - save_interval : 1000 - 2025-03-24 12:15:56,295 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:15:56,295 - log
        - lora_dim : 4 - 2025-03-24 12:15:56,295 - log
        - lora_alpha : 32 - 2025-03-24 12:15:56,295 - log
        - obj : clm - 2025-03-24 12:15:56,295 - log
        - lora_dropout : 0.1 - 2025-03-24 12:15:56,295 - log
        - label_smooth : 0.1 - 2025-03-24 12:15:56,295 - log
        - roll_interval : -1 - 2025-03-24 12:15:56,295 - log
        - roll_lr : 1e-05 - 2025-03-24 12:15:56,295 - log
        - roll_step : 100 - 2025-03-24 12:15:56,295 - log
        - eval_epoch : 1 - 2025-03-24 12:15:56,295 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:15:56,295 - log
==================================================================================================== - 2025-03-24 12:15:56,295 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:15:56,295 - log
loading model pretrained weight. - 2025-03-24 12:15:58,921 - log
set max_step: 525 - 2025-03-24 12:16:01,069 - log
start to train the model................ 1 - 2025-03-24 12:16:01,187 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 12:16:01,389 - log
Exiting from training early - 2025-03-24 12:16:01,389 - log
cleanup dist ... - 2025-03-24 12:16:01,586 - log
==================================================================================================== - 2025-03-24 12:20:57,261 - log
        - platform : local - 2025-03-24 12:20:57,261 - log
        - local_rank : 0 - 2025-03-24 12:20:57,261 - log
        - rank : 0 - 2025-03-24 12:20:57,261 - log
        - device : cuda:0 - 2025-03-24 12:20:57,261 - log
        - world_size : 1 - 2025-03-24 12:20:57,261 - log
        - random_seed : 110 - 2025-03-24 12:20:57,261 - log
        - lr : 0.0002 - 2025-03-24 12:20:57,261 - log
        - weight_decay : 0.01 - 2025-03-24 12:20:57,261 - log
        - correct_bias : True - 2025-03-24 12:20:57,261 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:20:57,261 - log
        - no_decay_bias : False - 2025-03-24 12:20:57,261 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:20:57,261 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:20:57,262 - log
        - scheduler : linear - 2025-03-24 12:20:57,262 - log
        - max_step : None - 2025-03-24 12:20:57,262 - log
        - max_epoch : 5 - 2025-03-24 12:20:57,262 - log
        - warmup_step : 500 - 2025-03-24 12:20:57,262 - log
        - i_steps : 0 - 2025-03-24 12:20:57,262 - log
        - i_lrs : 0.00025 - 2025-03-24 12:20:57,262 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:20:57,262 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:20:57,262 - log
        - train_batch_size : 4 - 2025-03-24 12:20:57,262 - log
        - valid_batch_size : 2 - 2025-03-24 12:20:57,262 - log
        - grad_acc : 2 - 2025-03-24 12:20:57,262 - log
        - clip : 0.0 - 2025-03-24 12:20:57,262 - log
        - seq_len : 256 - 2025-03-24 12:20:57,262 - log
        - model_card : gpt2.md - 2025-03-24 12:20:57,262 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:20:57,262 - log
        - fp16 : False - 2025-03-24 12:20:57,262 - log
        - log_interval : 100 - 2025-03-24 12:20:57,262 - log
        - eval_interval : 2000 - 2025-03-24 12:20:57,262 - log
        - save_interval : 1000 - 2025-03-24 12:20:57,262 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:20:57,262 - log
        - lora_dim : 4 - 2025-03-24 12:20:57,262 - log
        - lora_alpha : 32 - 2025-03-24 12:20:57,262 - log
        - obj : clm - 2025-03-24 12:20:57,262 - log
        - lora_dropout : 0.1 - 2025-03-24 12:20:57,262 - log
        - label_smooth : 0.1 - 2025-03-24 12:20:57,262 - log
        - roll_interval : -1 - 2025-03-24 12:20:57,262 - log
        - roll_lr : 1e-05 - 2025-03-24 12:20:57,262 - log
        - roll_step : 100 - 2025-03-24 12:20:57,262 - log
        - eval_epoch : 1 - 2025-03-24 12:20:57,262 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:20:57,262 - log
==================================================================================================== - 2025-03-24 12:20:57,262 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:20:57,262 - log
loading model pretrained weight. - 2025-03-24 12:21:00,083 - log
set max_step: 525 - 2025-03-24 12:21:02,025 - log
start to train the model................ 1 - 2025-03-24 12:21:02,141 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 12:21:02,852 - log
Exiting from training early - 2025-03-24 12:21:02,852 - log
cleanup dist ... - 2025-03-24 12:21:03,012 - log
==================================================================================================== - 2025-03-24 12:27:32,658 - log
        - platform : local - 2025-03-24 12:27:32,658 - log
        - local_rank : 0 - 2025-03-24 12:27:32,659 - log
        - rank : 0 - 2025-03-24 12:27:32,659 - log
        - device : cuda:0 - 2025-03-24 12:27:32,659 - log
        - world_size : 1 - 2025-03-24 12:27:32,659 - log
        - random_seed : 110 - 2025-03-24 12:27:32,659 - log
        - lr : 0.0002 - 2025-03-24 12:27:32,659 - log
        - weight_decay : 0.01 - 2025-03-24 12:27:32,659 - log
        - correct_bias : True - 2025-03-24 12:27:32,659 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:27:32,659 - log
        - no_decay_bias : False - 2025-03-24 12:27:32,659 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:27:32,659 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:27:32,659 - log
        - scheduler : linear - 2025-03-24 12:27:32,659 - log
        - max_step : None - 2025-03-24 12:27:32,659 - log
        - max_epoch : 5 - 2025-03-24 12:27:32,659 - log
        - warmup_step : 500 - 2025-03-24 12:27:32,659 - log
        - i_steps : 0 - 2025-03-24 12:27:32,659 - log
        - i_lrs : 0.00025 - 2025-03-24 12:27:32,659 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:27:32,659 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:27:32,659 - log
        - train_batch_size : 4 - 2025-03-24 12:27:32,659 - log
        - valid_batch_size : 2 - 2025-03-24 12:27:32,659 - log
        - grad_acc : 2 - 2025-03-24 12:27:32,659 - log
        - clip : 0.0 - 2025-03-24 12:27:32,659 - log
        - seq_len : 256 - 2025-03-24 12:27:32,659 - log
        - model_card : gpt2.md - 2025-03-24 12:27:32,659 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:27:32,659 - log
        - fp16 : False - 2025-03-24 12:27:32,659 - log
        - log_interval : 100 - 2025-03-24 12:27:32,659 - log
        - eval_interval : 2000 - 2025-03-24 12:27:32,659 - log
        - save_interval : 1000 - 2025-03-24 12:27:32,659 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:27:32,659 - log
        - lora_dim : 4 - 2025-03-24 12:27:32,659 - log
        - lora_alpha : 32 - 2025-03-24 12:27:32,659 - log
        - obj : clm - 2025-03-24 12:27:32,659 - log
        - lora_dropout : 0.1 - 2025-03-24 12:27:32,659 - log
        - label_smooth : 0.1 - 2025-03-24 12:27:32,659 - log
        - roll_interval : -1 - 2025-03-24 12:27:32,659 - log
        - roll_lr : 1e-05 - 2025-03-24 12:27:32,659 - log
        - roll_step : 100 - 2025-03-24 12:27:32,659 - log
        - eval_epoch : 1 - 2025-03-24 12:27:32,659 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:27:32,659 - log
==================================================================================================== - 2025-03-24 12:27:32,659 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:27:32,659 - log
loading model pretrained weight. - 2025-03-24 12:27:35,399 - log
set max_step: 525 - 2025-03-24 12:27:39,263 - log
start to train the model................ 1 - 2025-03-24 12:27:39,386 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 12:27:41,133 - log
Exiting from training early - 2025-03-24 12:27:41,133 - log
cleanup dist ... - 2025-03-24 12:27:41,323 - log
==================================================================================================== - 2025-03-24 12:28:04,621 - log
        - platform : local - 2025-03-24 12:28:04,621 - log
        - local_rank : 0 - 2025-03-24 12:28:04,621 - log
        - rank : 0 - 2025-03-24 12:28:04,621 - log
        - device : cuda:0 - 2025-03-24 12:28:04,621 - log
        - world_size : 1 - 2025-03-24 12:28:04,621 - log
        - random_seed : 110 - 2025-03-24 12:28:04,621 - log
        - lr : 0.0002 - 2025-03-24 12:28:04,621 - log
        - weight_decay : 0.01 - 2025-03-24 12:28:04,621 - log
        - correct_bias : True - 2025-03-24 12:28:04,621 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:28:04,621 - log
        - no_decay_bias : False - 2025-03-24 12:28:04,621 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:28:04,621 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:28:04,621 - log
        - scheduler : linear - 2025-03-24 12:28:04,621 - log
        - max_step : None - 2025-03-24 12:28:04,621 - log
        - max_epoch : 5 - 2025-03-24 12:28:04,621 - log
        - warmup_step : 500 - 2025-03-24 12:28:04,621 - log
        - i_steps : 0 - 2025-03-24 12:28:04,621 - log
        - i_lrs : 0.00025 - 2025-03-24 12:28:04,621 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:28:04,621 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:28:04,621 - log
        - train_batch_size : 4 - 2025-03-24 12:28:04,621 - log
        - valid_batch_size : 2 - 2025-03-24 12:28:04,621 - log
        - grad_acc : 2 - 2025-03-24 12:28:04,621 - log
        - clip : 0.0 - 2025-03-24 12:28:04,622 - log
        - seq_len : 256 - 2025-03-24 12:28:04,622 - log
        - model_card : gpt2.md - 2025-03-24 12:28:04,622 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:28:04,622 - log
        - fp16 : False - 2025-03-24 12:28:04,622 - log
        - log_interval : 100 - 2025-03-24 12:28:04,622 - log
        - eval_interval : 2000 - 2025-03-24 12:28:04,622 - log
        - save_interval : 1000 - 2025-03-24 12:28:04,622 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:28:04,622 - log
        - lora_dim : 4 - 2025-03-24 12:28:04,622 - log
        - lora_alpha : 32 - 2025-03-24 12:28:04,622 - log
        - obj : clm - 2025-03-24 12:28:04,622 - log
        - lora_dropout : 0.1 - 2025-03-24 12:28:04,622 - log
        - label_smooth : 0.1 - 2025-03-24 12:28:04,622 - log
        - roll_interval : -1 - 2025-03-24 12:28:04,622 - log
        - roll_lr : 1e-05 - 2025-03-24 12:28:04,622 - log
        - roll_step : 100 - 2025-03-24 12:28:04,622 - log
        - eval_epoch : 1 - 2025-03-24 12:28:04,622 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:28:04,622 - log
==================================================================================================== - 2025-03-24 12:28:04,622 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:28:04,622 - log
loading model pretrained weight. - 2025-03-24 12:28:07,195 - log
set max_step: 525 - 2025-03-24 12:28:09,138 - log
start to train the model................ 1 - 2025-03-24 12:28:09,257 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 12:28:10,213 - log
Exiting from training early - 2025-03-24 12:28:10,213 - log
cleanup dist ... - 2025-03-24 12:28:10,372 - log
==================================================================================================== - 2025-03-24 12:28:41,532 - log
        - platform : local - 2025-03-24 12:28:41,532 - log
        - local_rank : 0 - 2025-03-24 12:28:41,532 - log
        - rank : 0 - 2025-03-24 12:28:41,532 - log
        - device : cuda:0 - 2025-03-24 12:28:41,532 - log
        - world_size : 1 - 2025-03-24 12:28:41,532 - log
        - random_seed : 110 - 2025-03-24 12:28:41,532 - log
        - lr : 0.0002 - 2025-03-24 12:28:41,532 - log
        - weight_decay : 0.01 - 2025-03-24 12:28:41,532 - log
        - correct_bias : True - 2025-03-24 12:28:41,532 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:28:41,532 - log
        - no_decay_bias : False - 2025-03-24 12:28:41,532 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:28:41,532 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:28:41,532 - log
        - scheduler : linear - 2025-03-24 12:28:41,532 - log
        - max_step : None - 2025-03-24 12:28:41,532 - log
        - max_epoch : 5 - 2025-03-24 12:28:41,532 - log
        - warmup_step : 500 - 2025-03-24 12:28:41,532 - log
        - i_steps : 0 - 2025-03-24 12:28:41,532 - log
        - i_lrs : 0.00025 - 2025-03-24 12:28:41,532 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:28:41,532 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:28:41,532 - log
        - train_batch_size : 4 - 2025-03-24 12:28:41,532 - log
        - valid_batch_size : 2 - 2025-03-24 12:28:41,532 - log
        - grad_acc : 2 - 2025-03-24 12:28:41,533 - log
        - clip : 0.0 - 2025-03-24 12:28:41,533 - log
        - seq_len : 256 - 2025-03-24 12:28:41,533 - log
        - model_card : gpt2.md - 2025-03-24 12:28:41,533 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:28:41,533 - log
        - fp16 : False - 2025-03-24 12:28:41,533 - log
        - log_interval : 100 - 2025-03-24 12:28:41,533 - log
        - eval_interval : 2000 - 2025-03-24 12:28:41,533 - log
        - save_interval : 1000 - 2025-03-24 12:28:41,533 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:28:41,533 - log
        - lora_dim : 4 - 2025-03-24 12:28:41,533 - log
        - lora_alpha : 32 - 2025-03-24 12:28:41,533 - log
        - obj : clm - 2025-03-24 12:28:41,533 - log
        - lora_dropout : 0.1 - 2025-03-24 12:28:41,533 - log
        - label_smooth : 0.1 - 2025-03-24 12:28:41,533 - log
        - roll_interval : -1 - 2025-03-24 12:28:41,533 - log
        - roll_lr : 1e-05 - 2025-03-24 12:28:41,533 - log
        - roll_step : 100 - 2025-03-24 12:28:41,533 - log
        - eval_epoch : 1 - 2025-03-24 12:28:41,533 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:28:41,533 - log
==================================================================================================== - 2025-03-24 12:28:41,533 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:28:41,533 - log
loading model pretrained weight. - 2025-03-24 12:28:44,113 - log
set max_step: 525 - 2025-03-24 12:28:46,134 - log
start to train the model................ 1 - 2025-03-24 12:28:46,252 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 12:28:49,986 - log
Exiting from training early - 2025-03-24 12:28:49,986 - log
cleanup dist ... - 2025-03-24 12:28:50,151 - log
==================================================================================================== - 2025-03-24 12:29:22,608 - log
        - platform : local - 2025-03-24 12:29:22,608 - log
        - local_rank : 0 - 2025-03-24 12:29:22,608 - log
        - rank : 0 - 2025-03-24 12:29:22,608 - log
        - device : cuda:0 - 2025-03-24 12:29:22,608 - log
        - world_size : 1 - 2025-03-24 12:29:22,608 - log
        - random_seed : 110 - 2025-03-24 12:29:22,608 - log
        - lr : 0.0002 - 2025-03-24 12:29:22,608 - log
        - weight_decay : 0.01 - 2025-03-24 12:29:22,608 - log
        - correct_bias : True - 2025-03-24 12:29:22,608 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:29:22,608 - log
        - no_decay_bias : False - 2025-03-24 12:29:22,608 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:29:22,608 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:29:22,608 - log
        - scheduler : linear - 2025-03-24 12:29:22,608 - log
        - max_step : None - 2025-03-24 12:29:22,608 - log
        - max_epoch : 5 - 2025-03-24 12:29:22,609 - log
        - warmup_step : 500 - 2025-03-24 12:29:22,609 - log
        - i_steps : 0 - 2025-03-24 12:29:22,609 - log
        - i_lrs : 0.00025 - 2025-03-24 12:29:22,609 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:29:22,609 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:29:22,609 - log
        - train_batch_size : 4 - 2025-03-24 12:29:22,609 - log
        - valid_batch_size : 2 - 2025-03-24 12:29:22,609 - log
        - grad_acc : 2 - 2025-03-24 12:29:22,609 - log
        - clip : 0.0 - 2025-03-24 12:29:22,609 - log
        - seq_len : 256 - 2025-03-24 12:29:22,609 - log
        - model_card : gpt2.md - 2025-03-24 12:29:22,609 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:29:22,609 - log
        - fp16 : False - 2025-03-24 12:29:22,609 - log
        - log_interval : 100 - 2025-03-24 12:29:22,609 - log
        - eval_interval : 2000 - 2025-03-24 12:29:22,609 - log
        - save_interval : 1000 - 2025-03-24 12:29:22,609 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:29:22,609 - log
        - lora_dim : 4 - 2025-03-24 12:29:22,609 - log
        - lora_alpha : 32 - 2025-03-24 12:29:22,609 - log
        - obj : clm - 2025-03-24 12:29:22,609 - log
        - lora_dropout : 0.1 - 2025-03-24 12:29:22,609 - log
        - label_smooth : 0.1 - 2025-03-24 12:29:22,609 - log
        - roll_interval : -1 - 2025-03-24 12:29:22,609 - log
        - roll_lr : 1e-05 - 2025-03-24 12:29:22,609 - log
        - roll_step : 100 - 2025-03-24 12:29:22,609 - log
        - eval_epoch : 1 - 2025-03-24 12:29:22,609 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:29:22,609 - log
==================================================================================================== - 2025-03-24 12:29:22,609 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:29:22,609 - log
loading model pretrained weight. - 2025-03-24 12:29:25,284 - log
==================================================================================================== - 2025-03-24 12:29:50,925 - log
        - platform : local - 2025-03-24 12:29:50,925 - log
        - local_rank : 0 - 2025-03-24 12:29:50,925 - log
        - rank : 0 - 2025-03-24 12:29:50,925 - log
        - device : cuda:0 - 2025-03-24 12:29:50,925 - log
        - world_size : 1 - 2025-03-24 12:29:50,925 - log
        - random_seed : 110 - 2025-03-24 12:29:50,925 - log
        - lr : 0.0002 - 2025-03-24 12:29:50,925 - log
        - weight_decay : 0.01 - 2025-03-24 12:29:50,925 - log
        - correct_bias : True - 2025-03-24 12:29:50,925 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:29:50,925 - log
        - no_decay_bias : False - 2025-03-24 12:29:50,925 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:29:50,925 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:29:50,925 - log
        - scheduler : linear - 2025-03-24 12:29:50,925 - log
        - max_step : None - 2025-03-24 12:29:50,925 - log
        - max_epoch : 5 - 2025-03-24 12:29:50,925 - log
        - warmup_step : 500 - 2025-03-24 12:29:50,925 - log
        - i_steps : 0 - 2025-03-24 12:29:50,926 - log
        - i_lrs : 0.00025 - 2025-03-24 12:29:50,926 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:29:50,926 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:29:50,926 - log
        - train_batch_size : 4 - 2025-03-24 12:29:50,926 - log
        - valid_batch_size : 2 - 2025-03-24 12:29:50,926 - log
        - grad_acc : 2 - 2025-03-24 12:29:50,926 - log
        - clip : 0.0 - 2025-03-24 12:29:50,926 - log
        - seq_len : 256 - 2025-03-24 12:29:50,926 - log
        - model_card : gpt2.md - 2025-03-24 12:29:50,926 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:29:50,926 - log
        - fp16 : False - 2025-03-24 12:29:50,926 - log
        - log_interval : 100 - 2025-03-24 12:29:50,926 - log
        - eval_interval : 2000 - 2025-03-24 12:29:50,926 - log
        - save_interval : 1000 - 2025-03-24 12:29:50,926 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:29:50,926 - log
        - lora_dim : 4 - 2025-03-24 12:29:50,926 - log
        - lora_alpha : 32 - 2025-03-24 12:29:50,926 - log
        - obj : clm - 2025-03-24 12:29:50,926 - log
        - lora_dropout : 0.1 - 2025-03-24 12:29:50,926 - log
        - label_smooth : 0.1 - 2025-03-24 12:29:50,926 - log
        - roll_interval : -1 - 2025-03-24 12:29:50,926 - log
        - roll_lr : 1e-05 - 2025-03-24 12:29:50,926 - log
        - roll_step : 100 - 2025-03-24 12:29:50,926 - log
        - eval_epoch : 1 - 2025-03-24 12:29:50,926 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:29:50,926 - log
==================================================================================================== - 2025-03-24 12:29:50,926 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:29:50,926 - log
loading model pretrained weight. - 2025-03-24 12:29:53,551 - log
==================================================================================================== - 2025-03-24 12:30:10,420 - log
        - platform : local - 2025-03-24 12:30:10,420 - log
        - local_rank : 0 - 2025-03-24 12:30:10,420 - log
        - rank : 0 - 2025-03-24 12:30:10,420 - log
        - device : cuda:0 - 2025-03-24 12:30:10,420 - log
        - world_size : 1 - 2025-03-24 12:30:10,420 - log
        - random_seed : 110 - 2025-03-24 12:30:10,420 - log
        - lr : 0.0002 - 2025-03-24 12:30:10,420 - log
        - weight_decay : 0.01 - 2025-03-24 12:30:10,420 - log
        - correct_bias : True - 2025-03-24 12:30:10,420 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:30:10,420 - log
        - no_decay_bias : False - 2025-03-24 12:30:10,420 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:30:10,420 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:30:10,420 - log
        - scheduler : linear - 2025-03-24 12:30:10,420 - log
        - max_step : None - 2025-03-24 12:30:10,420 - log
        - max_epoch : 5 - 2025-03-24 12:30:10,420 - log
        - warmup_step : 500 - 2025-03-24 12:30:10,420 - log
        - i_steps : 0 - 2025-03-24 12:30:10,421 - log
        - i_lrs : 0.00025 - 2025-03-24 12:30:10,421 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:30:10,421 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:30:10,421 - log
        - train_batch_size : 4 - 2025-03-24 12:30:10,421 - log
        - valid_batch_size : 2 - 2025-03-24 12:30:10,421 - log
        - grad_acc : 2 - 2025-03-24 12:30:10,421 - log
        - clip : 0.0 - 2025-03-24 12:30:10,421 - log
        - seq_len : 256 - 2025-03-24 12:30:10,421 - log
        - model_card : gpt2.md - 2025-03-24 12:30:10,421 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:30:10,421 - log
        - fp16 : False - 2025-03-24 12:30:10,421 - log
        - log_interval : 100 - 2025-03-24 12:30:10,421 - log
        - eval_interval : 2000 - 2025-03-24 12:30:10,421 - log
        - save_interval : 1000 - 2025-03-24 12:30:10,421 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:30:10,421 - log
        - lora_dim : 4 - 2025-03-24 12:30:10,421 - log
        - lora_alpha : 32 - 2025-03-24 12:30:10,421 - log
        - obj : clm - 2025-03-24 12:30:10,421 - log
        - lora_dropout : 0.1 - 2025-03-24 12:30:10,421 - log
        - label_smooth : 0.1 - 2025-03-24 12:30:10,421 - log
        - roll_interval : -1 - 2025-03-24 12:30:10,421 - log
        - roll_lr : 1e-05 - 2025-03-24 12:30:10,421 - log
        - roll_step : 100 - 2025-03-24 12:30:10,421 - log
        - eval_epoch : 1 - 2025-03-24 12:30:10,421 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:30:10,421 - log
==================================================================================================== - 2025-03-24 12:30:10,421 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:30:10,421 - log
loading model pretrained weight. - 2025-03-24 12:30:13,095 - log
set max_step: 525 - 2025-03-24 12:30:15,291 - log
start to train the model................ 1 - 2025-03-24 12:30:15,410 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 12:30:16,052 - log
Exiting from training early - 2025-03-24 12:30:16,052 - log
cleanup dist ... - 2025-03-24 12:30:16,219 - log
==================================================================================================== - 2025-03-24 12:30:32,088 - log
        - platform : local - 2025-03-24 12:30:32,088 - log
        - local_rank : 0 - 2025-03-24 12:30:32,088 - log
        - rank : 0 - 2025-03-24 12:30:32,088 - log
        - device : cuda:0 - 2025-03-24 12:30:32,088 - log
        - world_size : 1 - 2025-03-24 12:30:32,088 - log
        - random_seed : 110 - 2025-03-24 12:30:32,088 - log
        - lr : 0.0002 - 2025-03-24 12:30:32,088 - log
        - weight_decay : 0.01 - 2025-03-24 12:30:32,088 - log
        - correct_bias : True - 2025-03-24 12:30:32,088 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:30:32,088 - log
        - no_decay_bias : False - 2025-03-24 12:30:32,088 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:30:32,088 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:30:32,088 - log
        - scheduler : linear - 2025-03-24 12:30:32,088 - log
        - max_step : None - 2025-03-24 12:30:32,088 - log
        - max_epoch : 5 - 2025-03-24 12:30:32,088 - log
        - warmup_step : 500 - 2025-03-24 12:30:32,088 - log
        - i_steps : 0 - 2025-03-24 12:30:32,088 - log
        - i_lrs : 0.00025 - 2025-03-24 12:30:32,088 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:30:32,088 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:30:32,088 - log
        - train_batch_size : 4 - 2025-03-24 12:30:32,088 - log
        - valid_batch_size : 2 - 2025-03-24 12:30:32,088 - log
        - grad_acc : 2 - 2025-03-24 12:30:32,088 - log
        - clip : 0.0 - 2025-03-24 12:30:32,088 - log
        - seq_len : 256 - 2025-03-24 12:30:32,088 - log
        - model_card : gpt2.md - 2025-03-24 12:30:32,088 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:30:32,088 - log
        - fp16 : False - 2025-03-24 12:30:32,088 - log
        - log_interval : 100 - 2025-03-24 12:30:32,088 - log
        - eval_interval : 2000 - 2025-03-24 12:30:32,088 - log
        - save_interval : 1000 - 2025-03-24 12:30:32,088 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:30:32,088 - log
        - lora_dim : 4 - 2025-03-24 12:30:32,088 - log
        - lora_alpha : 32 - 2025-03-24 12:30:32,088 - log
        - obj : clm - 2025-03-24 12:30:32,088 - log
        - lora_dropout : 0.1 - 2025-03-24 12:30:32,088 - log
        - label_smooth : 0.1 - 2025-03-24 12:30:32,088 - log
        - roll_interval : -1 - 2025-03-24 12:30:32,088 - log
        - roll_lr : 1e-05 - 2025-03-24 12:30:32,088 - log
        - roll_step : 100 - 2025-03-24 12:30:32,089 - log
        - eval_epoch : 1 - 2025-03-24 12:30:32,089 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:30:32,089 - log
==================================================================================================== - 2025-03-24 12:30:32,089 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:30:32,089 - log
loading model pretrained weight. - 2025-03-24 12:30:34,690 - log
set max_step: 525 - 2025-03-24 12:30:36,732 - log
start to train the model................ 1 - 2025-03-24 12:30:36,855 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 12:30:37,904 - log
Exiting from training early - 2025-03-24 12:30:37,904 - log
cleanup dist ... - 2025-03-24 12:30:38,069 - log
==================================================================================================== - 2025-03-24 12:30:57,528 - log
        - platform : local - 2025-03-24 12:30:57,528 - log
        - local_rank : 0 - 2025-03-24 12:30:57,528 - log
        - rank : 0 - 2025-03-24 12:30:57,528 - log
        - device : cuda:0 - 2025-03-24 12:30:57,528 - log
        - world_size : 1 - 2025-03-24 12:30:57,528 - log
        - random_seed : 110 - 2025-03-24 12:30:57,528 - log
        - lr : 0.0002 - 2025-03-24 12:30:57,528 - log
        - weight_decay : 0.01 - 2025-03-24 12:30:57,528 - log
        - correct_bias : True - 2025-03-24 12:30:57,528 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:30:57,528 - log
        - no_decay_bias : False - 2025-03-24 12:30:57,528 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:30:57,528 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:30:57,528 - log
        - scheduler : linear - 2025-03-24 12:30:57,528 - log
        - max_step : None - 2025-03-24 12:30:57,528 - log
        - max_epoch : 5 - 2025-03-24 12:30:57,528 - log
        - warmup_step : 500 - 2025-03-24 12:30:57,528 - log
        - i_steps : 0 - 2025-03-24 12:30:57,528 - log
        - i_lrs : 0.00025 - 2025-03-24 12:30:57,528 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:30:57,528 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:30:57,528 - log
        - train_batch_size : 4 - 2025-03-24 12:30:57,528 - log
        - valid_batch_size : 2 - 2025-03-24 12:30:57,528 - log
        - grad_acc : 2 - 2025-03-24 12:30:57,528 - log
        - clip : 0.0 - 2025-03-24 12:30:57,528 - log
        - seq_len : 256 - 2025-03-24 12:30:57,528 - log
        - model_card : gpt2.md - 2025-03-24 12:30:57,528 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:30:57,528 - log
        - fp16 : False - 2025-03-24 12:30:57,528 - log
        - log_interval : 100 - 2025-03-24 12:30:57,528 - log
        - eval_interval : 2000 - 2025-03-24 12:30:57,528 - log
        - save_interval : 1000 - 2025-03-24 12:30:57,528 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:30:57,528 - log
        - lora_dim : 4 - 2025-03-24 12:30:57,528 - log
        - lora_alpha : 32 - 2025-03-24 12:30:57,528 - log
        - obj : clm - 2025-03-24 12:30:57,528 - log
        - lora_dropout : 0.1 - 2025-03-24 12:30:57,528 - log
        - label_smooth : 0.1 - 2025-03-24 12:30:57,528 - log
        - roll_interval : -1 - 2025-03-24 12:30:57,528 - log
        - roll_lr : 1e-05 - 2025-03-24 12:30:57,528 - log
        - roll_step : 100 - 2025-03-24 12:30:57,529 - log
        - eval_epoch : 1 - 2025-03-24 12:30:57,529 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:30:57,529 - log
==================================================================================================== - 2025-03-24 12:30:57,529 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:30:57,529 - log
loading model pretrained weight. - 2025-03-24 12:31:00,127 - log
set max_step: 525 - 2025-03-24 12:31:02,164 - log
start to train the model................ 1 - 2025-03-24 12:31:02,282 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 12:31:03,302 - log
Exiting from training early - 2025-03-24 12:31:03,303 - log
==================================================================================================== - 2025-03-24 12:34:30,443 - log
        - platform : local - 2025-03-24 12:34:30,443 - log
        - local_rank : 0 - 2025-03-24 12:34:30,443 - log
        - rank : 0 - 2025-03-24 12:34:30,443 - log
        - device : cuda:0 - 2025-03-24 12:34:30,443 - log
        - world_size : 1 - 2025-03-24 12:34:30,443 - log
        - random_seed : 110 - 2025-03-24 12:34:30,443 - log
        - lr : 0.0002 - 2025-03-24 12:34:30,443 - log
        - weight_decay : 0.01 - 2025-03-24 12:34:30,443 - log
        - correct_bias : True - 2025-03-24 12:34:30,443 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:34:30,443 - log
        - no_decay_bias : False - 2025-03-24 12:34:30,443 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:34:30,443 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:34:30,443 - log
        - scheduler : linear - 2025-03-24 12:34:30,443 - log
        - max_step : None - 2025-03-24 12:34:30,443 - log
        - max_epoch : 5 - 2025-03-24 12:34:30,443 - log
        - warmup_step : 500 - 2025-03-24 12:34:30,443 - log
        - i_steps : 0 - 2025-03-24 12:34:30,443 - log
        - i_lrs : 0.00025 - 2025-03-24 12:34:30,443 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:34:30,443 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:34:30,443 - log
        - train_batch_size : 4 - 2025-03-24 12:34:30,443 - log
        - valid_batch_size : 2 - 2025-03-24 12:34:30,443 - log
        - grad_acc : 2 - 2025-03-24 12:34:30,443 - log
        - clip : 0.0 - 2025-03-24 12:34:30,443 - log
        - seq_len : 256 - 2025-03-24 12:34:30,443 - log
        - model_card : gpt2.md - 2025-03-24 12:34:30,444 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:34:30,444 - log
        - fp16 : False - 2025-03-24 12:34:30,444 - log
        - log_interval : 100 - 2025-03-24 12:34:30,444 - log
        - eval_interval : 2000 - 2025-03-24 12:34:30,444 - log
        - save_interval : 1000 - 2025-03-24 12:34:30,444 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:34:30,444 - log
        - lora_dim : 4 - 2025-03-24 12:34:30,444 - log
        - lora_alpha : 32 - 2025-03-24 12:34:30,444 - log
        - obj : clm - 2025-03-24 12:34:30,444 - log
        - lora_dropout : 0.1 - 2025-03-24 12:34:30,444 - log
        - label_smooth : 0.1 - 2025-03-24 12:34:30,444 - log
        - roll_interval : -1 - 2025-03-24 12:34:30,444 - log
        - roll_lr : 1e-05 - 2025-03-24 12:34:30,444 - log
        - roll_step : 100 - 2025-03-24 12:34:30,444 - log
        - eval_epoch : 1 - 2025-03-24 12:34:30,444 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:34:30,444 - log
==================================================================================================== - 2025-03-24 12:34:30,444 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:34:30,444 - log
loading model pretrained weight. - 2025-03-24 12:34:32,993 - log
set max_step: 525 - 2025-03-24 12:34:35,126 - log
==================================================================================================== - 2025-03-24 12:42:10,327 - log
        - platform : local - 2025-03-24 12:42:10,328 - log
        - local_rank : 0 - 2025-03-24 12:42:10,328 - log
        - rank : 0 - 2025-03-24 12:42:10,328 - log
        - device : cuda:0 - 2025-03-24 12:42:10,328 - log
        - world_size : 1 - 2025-03-24 12:42:10,328 - log
        - random_seed : 110 - 2025-03-24 12:42:10,328 - log
        - lr : 0.0002 - 2025-03-24 12:42:10,328 - log
        - weight_decay : 0.01 - 2025-03-24 12:42:10,328 - log
        - correct_bias : True - 2025-03-24 12:42:10,328 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:42:10,328 - log
        - no_decay_bias : False - 2025-03-24 12:42:10,328 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:42:10,328 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:42:10,328 - log
        - scheduler : linear - 2025-03-24 12:42:10,328 - log
        - max_step : None - 2025-03-24 12:42:10,328 - log
        - max_epoch : 5 - 2025-03-24 12:42:10,328 - log
        - warmup_step : 500 - 2025-03-24 12:42:10,328 - log
        - i_steps : 0 - 2025-03-24 12:42:10,328 - log
        - i_lrs : 0.00025 - 2025-03-24 12:42:10,328 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:42:10,328 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:42:10,328 - log
        - train_batch_size : 4 - 2025-03-24 12:42:10,328 - log
        - valid_batch_size : 2 - 2025-03-24 12:42:10,328 - log
        - grad_acc : 2 - 2025-03-24 12:42:10,328 - log
        - clip : 0.0 - 2025-03-24 12:42:10,328 - log
        - seq_len : 256 - 2025-03-24 12:42:10,328 - log
        - model_card : gpt2.md - 2025-03-24 12:42:10,328 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:42:10,328 - log
        - fp16 : False - 2025-03-24 12:42:10,328 - log
        - log_interval : 100 - 2025-03-24 12:42:10,328 - log
        - eval_interval : 2000 - 2025-03-24 12:42:10,328 - log
        - save_interval : 1000 - 2025-03-24 12:42:10,328 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:42:10,328 - log
        - lora_dim : 4 - 2025-03-24 12:42:10,328 - log
        - lora_alpha : 32 - 2025-03-24 12:42:10,328 - log
        - obj : clm - 2025-03-24 12:42:10,328 - log
        - lora_dropout : 0.1 - 2025-03-24 12:42:10,328 - log
        - label_smooth : 0.1 - 2025-03-24 12:42:10,328 - log
        - roll_interval : -1 - 2025-03-24 12:42:10,328 - log
        - roll_lr : 1e-05 - 2025-03-24 12:42:10,328 - log
        - roll_step : 100 - 2025-03-24 12:42:10,328 - log
        - eval_epoch : 1 - 2025-03-24 12:42:10,328 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:42:10,328 - log
==================================================================================================== - 2025-03-24 12:42:10,328 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:42:10,328 - log
loading model pretrained weight. - 2025-03-24 12:42:12,920 - log
set max_step: 525 - 2025-03-24 12:42:14,968 - log
start to train the model................ 1 - 2025-03-24 12:42:15,083 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 12:42:16,085 - log
Exiting from training early - 2025-03-24 12:42:16,085 - log
==================================================================================================== - 2025-03-24 12:43:40,610 - log
        - platform : local - 2025-03-24 12:43:40,610 - log
        - local_rank : 0 - 2025-03-24 12:43:40,610 - log
        - rank : 0 - 2025-03-24 12:43:40,610 - log
        - device : cuda:0 - 2025-03-24 12:43:40,610 - log
        - world_size : 1 - 2025-03-24 12:43:40,610 - log
        - random_seed : 110 - 2025-03-24 12:43:40,610 - log
        - lr : 0.0002 - 2025-03-24 12:43:40,610 - log
        - weight_decay : 0.01 - 2025-03-24 12:43:40,610 - log
        - correct_bias : True - 2025-03-24 12:43:40,610 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:43:40,610 - log
        - no_decay_bias : False - 2025-03-24 12:43:40,610 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:43:40,610 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:43:40,610 - log
        - scheduler : linear - 2025-03-24 12:43:40,610 - log
        - max_step : None - 2025-03-24 12:43:40,610 - log
        - max_epoch : 5 - 2025-03-24 12:43:40,610 - log
        - warmup_step : 500 - 2025-03-24 12:43:40,610 - log
        - i_steps : 0 - 2025-03-24 12:43:40,610 - log
        - i_lrs : 0.00025 - 2025-03-24 12:43:40,610 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:43:40,610 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:43:40,610 - log
        - train_batch_size : 4 - 2025-03-24 12:43:40,610 - log
        - valid_batch_size : 2 - 2025-03-24 12:43:40,610 - log
        - grad_acc : 2 - 2025-03-24 12:43:40,610 - log
        - clip : 0.0 - 2025-03-24 12:43:40,610 - log
        - seq_len : 256 - 2025-03-24 12:43:40,610 - log
        - model_card : gpt2.md - 2025-03-24 12:43:40,610 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:43:40,610 - log
        - fp16 : False - 2025-03-24 12:43:40,610 - log
        - log_interval : 100 - 2025-03-24 12:43:40,610 - log
        - eval_interval : 2000 - 2025-03-24 12:43:40,610 - log
        - save_interval : 1000 - 2025-03-24 12:43:40,610 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:43:40,610 - log
        - lora_dim : 4 - 2025-03-24 12:43:40,610 - log
        - lora_alpha : 32 - 2025-03-24 12:43:40,610 - log
        - obj : clm - 2025-03-24 12:43:40,610 - log
        - lora_dropout : 0.1 - 2025-03-24 12:43:40,610 - log
        - label_smooth : 0.1 - 2025-03-24 12:43:40,610 - log
        - roll_interval : -1 - 2025-03-24 12:43:40,610 - log
        - roll_lr : 1e-05 - 2025-03-24 12:43:40,610 - log
        - roll_step : 100 - 2025-03-24 12:43:40,610 - log
        - eval_epoch : 1 - 2025-03-24 12:43:40,610 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:43:40,610 - log
==================================================================================================== - 2025-03-24 12:43:40,610 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:43:40,610 - log
loading model pretrained weight. - 2025-03-24 12:43:43,195 - log
set max_step: 525 - 2025-03-24 12:43:45,226 - log
start to train the model................ 1 - 2025-03-24 12:43:45,371 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 12:43:46,334 - log
Exiting from training early - 2025-03-24 12:43:46,335 - log
==================================================================================================== - 2025-03-24 12:51:00,300 - log
        - platform : local - 2025-03-24 12:51:00,300 - log
        - local_rank : 0 - 2025-03-24 12:51:00,300 - log
        - rank : 0 - 2025-03-24 12:51:00,300 - log
        - device : cuda:0 - 2025-03-24 12:51:00,300 - log
        - world_size : 1 - 2025-03-24 12:51:00,300 - log
        - random_seed : 110 - 2025-03-24 12:51:00,300 - log
        - lr : 0.0002 - 2025-03-24 12:51:00,300 - log
        - weight_decay : 0.01 - 2025-03-24 12:51:00,300 - log
        - correct_bias : True - 2025-03-24 12:51:00,300 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:51:00,300 - log
        - no_decay_bias : False - 2025-03-24 12:51:00,300 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:51:00,300 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:51:00,300 - log
        - scheduler : linear - 2025-03-24 12:51:00,300 - log
        - max_step : None - 2025-03-24 12:51:00,300 - log
        - max_epoch : 5 - 2025-03-24 12:51:00,300 - log
        - warmup_step : 500 - 2025-03-24 12:51:00,300 - log
        - i_steps : 0 - 2025-03-24 12:51:00,300 - log
        - i_lrs : 0.00025 - 2025-03-24 12:51:00,300 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:51:00,300 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:51:00,300 - log
        - train_batch_size : 4 - 2025-03-24 12:51:00,300 - log
        - valid_batch_size : 2 - 2025-03-24 12:51:00,300 - log
        - grad_acc : 2 - 2025-03-24 12:51:00,301 - log
        - clip : 0.0 - 2025-03-24 12:51:00,301 - log
        - seq_len : 256 - 2025-03-24 12:51:00,301 - log
        - model_card : gpt2.md - 2025-03-24 12:51:00,301 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:51:00,301 - log
        - fp16 : False - 2025-03-24 12:51:00,301 - log
        - log_interval : 100 - 2025-03-24 12:51:00,301 - log
        - eval_interval : 2000 - 2025-03-24 12:51:00,301 - log
        - save_interval : 1000 - 2025-03-24 12:51:00,301 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:51:00,301 - log
        - lora_dim : 4 - 2025-03-24 12:51:00,301 - log
        - lora_alpha : 32 - 2025-03-24 12:51:00,301 - log
        - obj : clm - 2025-03-24 12:51:00,301 - log
        - lora_dropout : 0.1 - 2025-03-24 12:51:00,301 - log
        - label_smooth : 0.1 - 2025-03-24 12:51:00,301 - log
        - roll_interval : -1 - 2025-03-24 12:51:00,301 - log
        - roll_lr : 1e-05 - 2025-03-24 12:51:00,301 - log
        - roll_step : 100 - 2025-03-24 12:51:00,301 - log
        - eval_epoch : 1 - 2025-03-24 12:51:00,301 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:51:00,301 - log
==================================================================================================== - 2025-03-24 12:51:00,301 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:51:00,301 - log
loading model pretrained weight. - 2025-03-24 12:51:02,792 - log
==================================================================================================== - 2025-03-24 12:53:02,186 - log
        - platform : local - 2025-03-24 12:53:02,186 - log
        - local_rank : 0 - 2025-03-24 12:53:02,186 - log
        - rank : 0 - 2025-03-24 12:53:02,186 - log
        - device : cuda:0 - 2025-03-24 12:53:02,186 - log
        - world_size : 1 - 2025-03-24 12:53:02,186 - log
        - random_seed : 110 - 2025-03-24 12:53:02,186 - log
        - lr : 0.0002 - 2025-03-24 12:53:02,186 - log
        - weight_decay : 0.01 - 2025-03-24 12:53:02,186 - log
        - correct_bias : True - 2025-03-24 12:53:02,186 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:53:02,186 - log
        - no_decay_bias : False - 2025-03-24 12:53:02,186 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:53:02,186 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:53:02,186 - log
        - scheduler : linear - 2025-03-24 12:53:02,186 - log
        - max_step : None - 2025-03-24 12:53:02,186 - log
        - max_epoch : 5 - 2025-03-24 12:53:02,186 - log
        - warmup_step : 500 - 2025-03-24 12:53:02,186 - log
        - i_steps : 0 - 2025-03-24 12:53:02,186 - log
        - i_lrs : 0.00025 - 2025-03-24 12:53:02,186 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:53:02,186 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:53:02,187 - log
        - train_batch_size : 4 - 2025-03-24 12:53:02,187 - log
        - valid_batch_size : 2 - 2025-03-24 12:53:02,187 - log
        - grad_acc : 2 - 2025-03-24 12:53:02,187 - log
        - clip : 0.0 - 2025-03-24 12:53:02,187 - log
        - seq_len : 256 - 2025-03-24 12:53:02,187 - log
        - model_card : gpt2.md - 2025-03-24 12:53:02,187 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:53:02,187 - log
        - fp16 : False - 2025-03-24 12:53:02,187 - log
        - log_interval : 100 - 2025-03-24 12:53:02,187 - log
        - eval_interval : 2000 - 2025-03-24 12:53:02,187 - log
        - save_interval : 1000 - 2025-03-24 12:53:02,187 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:53:02,187 - log
        - lora_dim : 4 - 2025-03-24 12:53:02,187 - log
        - lora_alpha : 32 - 2025-03-24 12:53:02,187 - log
        - obj : clm - 2025-03-24 12:53:02,187 - log
        - lora_dropout : 0.1 - 2025-03-24 12:53:02,187 - log
        - label_smooth : 0.1 - 2025-03-24 12:53:02,187 - log
        - roll_interval : -1 - 2025-03-24 12:53:02,187 - log
        - roll_lr : 1e-05 - 2025-03-24 12:53:02,187 - log
        - roll_step : 100 - 2025-03-24 12:53:02,187 - log
        - eval_epoch : 1 - 2025-03-24 12:53:02,187 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:53:02,187 - log
==================================================================================================== - 2025-03-24 12:53:02,187 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:53:02,187 - log
==================================================================================================== - 2025-03-24 12:53:37,948 - log
        - platform : local - 2025-03-24 12:53:37,949 - log
        - local_rank : 0 - 2025-03-24 12:53:37,949 - log
        - rank : 0 - 2025-03-24 12:53:37,949 - log
        - device : cuda:0 - 2025-03-24 12:53:37,949 - log
        - world_size : 1 - 2025-03-24 12:53:37,949 - log
        - random_seed : 110 - 2025-03-24 12:53:37,949 - log
        - lr : 0.0002 - 2025-03-24 12:53:37,949 - log
        - weight_decay : 0.01 - 2025-03-24 12:53:37,949 - log
        - correct_bias : True - 2025-03-24 12:53:37,949 - log
        - adam_epislon : 1e-06 - 2025-03-24 12:53:37,949 - log
        - no_decay_bias : False - 2025-03-24 12:53:37,949 - log
        - adam_beta1 : 0.9 - 2025-03-24 12:53:37,949 - log
        - adam_beta2 : 0.999 - 2025-03-24 12:53:37,949 - log
        - scheduler : linear - 2025-03-24 12:53:37,949 - log
        - max_step : None - 2025-03-24 12:53:37,949 - log
        - max_epoch : 5 - 2025-03-24 12:53:37,949 - log
        - warmup_step : 500 - 2025-03-24 12:53:37,949 - log
        - i_steps : 0 - 2025-03-24 12:53:37,949 - log
        - i_lrs : 0.00025 - 2025-03-24 12:53:37,949 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 12:53:37,949 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 12:53:37,949 - log
        - train_batch_size : 4 - 2025-03-24 12:53:37,949 - log
        - valid_batch_size : 2 - 2025-03-24 12:53:37,949 - log
        - grad_acc : 2 - 2025-03-24 12:53:37,949 - log
        - clip : 0.0 - 2025-03-24 12:53:37,949 - log
        - seq_len : 256 - 2025-03-24 12:53:37,949 - log
        - model_card : gpt2.md - 2025-03-24 12:53:37,949 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 12:53:37,949 - log
        - fp16 : False - 2025-03-24 12:53:37,949 - log
        - log_interval : 100 - 2025-03-24 12:53:37,949 - log
        - eval_interval : 2000 - 2025-03-24 12:53:37,949 - log
        - save_interval : 1000 - 2025-03-24 12:53:37,949 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 12:53:37,949 - log
        - lora_dim : 4 - 2025-03-24 12:53:37,949 - log
        - lora_alpha : 32 - 2025-03-24 12:53:37,949 - log
        - obj : clm - 2025-03-24 12:53:37,949 - log
        - lora_dropout : 0.1 - 2025-03-24 12:53:37,949 - log
        - label_smooth : 0.1 - 2025-03-24 12:53:37,949 - log
        - roll_interval : -1 - 2025-03-24 12:53:37,949 - log
        - roll_lr : 1e-05 - 2025-03-24 12:53:37,949 - log
        - roll_step : 100 - 2025-03-24 12:53:37,949 - log
        - eval_epoch : 1 - 2025-03-24 12:53:37,949 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 12:53:37,949 - log
==================================================================================================== - 2025-03-24 12:53:37,949 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 12:53:37,949 - log
loading model pretrained weight. - 2025-03-24 12:53:40,483 - log
==================================================================================================== - 2025-03-24 13:29:46,089 - log
        - platform : local - 2025-03-24 13:29:46,089 - log
        - local_rank : 0 - 2025-03-24 13:29:46,089 - log
        - rank : 0 - 2025-03-24 13:29:46,089 - log
        - device : cuda:0 - 2025-03-24 13:29:46,089 - log
        - world_size : 1 - 2025-03-24 13:29:46,089 - log
        - random_seed : 110 - 2025-03-24 13:29:46,089 - log
        - lr : 0.0002 - 2025-03-24 13:29:46,089 - log
        - weight_decay : 0.01 - 2025-03-24 13:29:46,089 - log
        - correct_bias : True - 2025-03-24 13:29:46,089 - log
        - adam_epislon : 1e-06 - 2025-03-24 13:29:46,089 - log
        - no_decay_bias : False - 2025-03-24 13:29:46,089 - log
        - adam_beta1 : 0.9 - 2025-03-24 13:29:46,089 - log
        - adam_beta2 : 0.999 - 2025-03-24 13:29:46,089 - log
        - scheduler : linear - 2025-03-24 13:29:46,089 - log
        - max_step : None - 2025-03-24 13:29:46,089 - log
        - max_epoch : 5 - 2025-03-24 13:29:46,089 - log
        - warmup_step : 500 - 2025-03-24 13:29:46,089 - log
        - i_steps : 0 - 2025-03-24 13:29:46,089 - log
        - i_lrs : 0.00025 - 2025-03-24 13:29:46,089 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 13:29:46,089 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 13:29:46,089 - log
        - train_batch_size : 4 - 2025-03-24 13:29:46,089 - log
        - valid_batch_size : 2 - 2025-03-24 13:29:46,089 - log
        - grad_acc : 2 - 2025-03-24 13:29:46,089 - log
        - clip : 0.0 - 2025-03-24 13:29:46,089 - log
        - seq_len : 256 - 2025-03-24 13:29:46,089 - log
        - model_card : gpt2.md - 2025-03-24 13:29:46,089 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 13:29:46,089 - log
        - fp16 : False - 2025-03-24 13:29:46,089 - log
        - log_interval : 100 - 2025-03-24 13:29:46,090 - log
        - eval_interval : 2000 - 2025-03-24 13:29:46,090 - log
        - save_interval : 1000 - 2025-03-24 13:29:46,090 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 13:29:46,090 - log
        - lora_dim : 4 - 2025-03-24 13:29:46,090 - log
        - lora_alpha : 32 - 2025-03-24 13:29:46,090 - log
        - obj : clm - 2025-03-24 13:29:46,090 - log
        - lora_dropout : 0.1 - 2025-03-24 13:29:46,090 - log
        - label_smooth : 0.1 - 2025-03-24 13:29:46,090 - log
        - roll_interval : -1 - 2025-03-24 13:29:46,090 - log
        - roll_lr : 1e-05 - 2025-03-24 13:29:46,090 - log
        - roll_step : 100 - 2025-03-24 13:29:46,090 - log
        - eval_epoch : 1 - 2025-03-24 13:29:46,090 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 13:29:46,090 - log
==================================================================================================== - 2025-03-24 13:29:46,090 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 13:29:46,090 - log
loading model pretrained weight. - 2025-03-24 13:29:48,616 - log
set max_step: 525 - 2025-03-24 13:29:50,644 - log
start to train the model................ 1 - 2025-03-24 13:29:50,758 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 13:29:57,276 - log
Exiting from training early - 2025-03-24 13:29:57,277 - log
==================================================================================================== - 2025-03-24 14:05:46,498 - log
        - platform : local - 2025-03-24 14:05:46,498 - log
        - local_rank : 0 - 2025-03-24 14:05:46,498 - log
        - rank : 0 - 2025-03-24 14:05:46,499 - log
        - device : cuda:0 - 2025-03-24 14:05:46,499 - log
        - world_size : 1 - 2025-03-24 14:05:46,499 - log
        - random_seed : 110 - 2025-03-24 14:05:46,499 - log
        - lr : 0.0002 - 2025-03-24 14:05:46,499 - log
        - weight_decay : 0.01 - 2025-03-24 14:05:46,499 - log
        - correct_bias : True - 2025-03-24 14:05:46,499 - log
        - adam_epislon : 1e-06 - 2025-03-24 14:05:46,499 - log
        - no_decay_bias : False - 2025-03-24 14:05:46,499 - log
        - adam_beta1 : 0.9 - 2025-03-24 14:05:46,499 - log
        - adam_beta2 : 0.999 - 2025-03-24 14:05:46,499 - log
        - scheduler : linear - 2025-03-24 14:05:46,499 - log
        - max_step : None - 2025-03-24 14:05:46,499 - log
        - max_epoch : 5 - 2025-03-24 14:05:46,499 - log
        - warmup_step : 500 - 2025-03-24 14:05:46,499 - log
        - i_steps : 0 - 2025-03-24 14:05:46,499 - log
        - i_lrs : 0.00025 - 2025-03-24 14:05:46,499 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 14:05:46,499 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 14:05:46,499 - log
        - train_batch_size : 4 - 2025-03-24 14:05:46,499 - log
        - valid_batch_size : 2 - 2025-03-24 14:05:46,499 - log
        - grad_acc : 2 - 2025-03-24 14:05:46,499 - log
        - clip : 0.0 - 2025-03-24 14:05:46,499 - log
        - seq_len : 256 - 2025-03-24 14:05:46,499 - log
        - model_card : gpt2.md - 2025-03-24 14:05:46,499 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 14:05:46,499 - log
        - fp16 : False - 2025-03-24 14:05:46,499 - log
        - log_interval : 100 - 2025-03-24 14:05:46,499 - log
        - eval_interval : 2000 - 2025-03-24 14:05:46,499 - log
        - save_interval : 1000 - 2025-03-24 14:05:46,499 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 14:05:46,499 - log
        - lora_dim : 4 - 2025-03-24 14:05:46,499 - log
        - lora_alpha : 32 - 2025-03-24 14:05:46,499 - log
        - obj : clm - 2025-03-24 14:05:46,499 - log
        - lora_dropout : 0.1 - 2025-03-24 14:05:46,499 - log
        - label_smooth : 0.1 - 2025-03-24 14:05:46,499 - log
        - roll_interval : -1 - 2025-03-24 14:05:46,499 - log
        - roll_lr : 1e-05 - 2025-03-24 14:05:46,499 - log
        - roll_step : 100 - 2025-03-24 14:05:46,499 - log
        - eval_epoch : 1 - 2025-03-24 14:05:46,499 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 14:05:46,499 - log
==================================================================================================== - 2025-03-24 14:05:46,499 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 14:05:46,499 - log
loading model pretrained weight. - 2025-03-24 14:05:49,045 - log
set max_step: 525 - 2025-03-24 14:05:52,204 - log
start to train the model................ 1 - 2025-03-24 14:05:52,358 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 14:05:52,792 - log
Exiting from training early - 2025-03-24 14:05:52,792 - log
cleanup dist ... - 2025-03-24 14:05:52,994 - log
==================================================================================================== - 2025-03-24 14:06:35,185 - log
        - platform : local - 2025-03-24 14:06:35,185 - log
        - local_rank : 0 - 2025-03-24 14:06:35,185 - log
        - rank : 0 - 2025-03-24 14:06:35,185 - log
        - device : cuda:0 - 2025-03-24 14:06:35,185 - log
        - world_size : 1 - 2025-03-24 14:06:35,185 - log
        - random_seed : 110 - 2025-03-24 14:06:35,185 - log
        - lr : 0.0002 - 2025-03-24 14:06:35,185 - log
        - weight_decay : 0.01 - 2025-03-24 14:06:35,185 - log
        - correct_bias : True - 2025-03-24 14:06:35,185 - log
        - adam_epislon : 1e-06 - 2025-03-24 14:06:35,185 - log
        - no_decay_bias : False - 2025-03-24 14:06:35,185 - log
        - adam_beta1 : 0.9 - 2025-03-24 14:06:35,185 - log
        - adam_beta2 : 0.999 - 2025-03-24 14:06:35,186 - log
        - scheduler : linear - 2025-03-24 14:06:35,186 - log
        - max_step : None - 2025-03-24 14:06:35,186 - log
        - max_epoch : 5 - 2025-03-24 14:06:35,186 - log
        - warmup_step : 500 - 2025-03-24 14:06:35,186 - log
        - i_steps : 0 - 2025-03-24 14:06:35,186 - log
        - i_lrs : 0.00025 - 2025-03-24 14:06:35,186 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 14:06:35,186 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 14:06:35,186 - log
        - train_batch_size : 4 - 2025-03-24 14:06:35,186 - log
        - valid_batch_size : 2 - 2025-03-24 14:06:35,186 - log
        - grad_acc : 2 - 2025-03-24 14:06:35,186 - log
        - clip : 0.0 - 2025-03-24 14:06:35,186 - log
        - seq_len : 256 - 2025-03-24 14:06:35,186 - log
        - model_card : gpt2.md - 2025-03-24 14:06:35,186 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 14:06:35,186 - log
        - fp16 : False - 2025-03-24 14:06:35,186 - log
        - log_interval : 100 - 2025-03-24 14:06:35,186 - log
        - eval_interval : 2000 - 2025-03-24 14:06:35,186 - log
        - save_interval : 1000 - 2025-03-24 14:06:35,186 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 14:06:35,186 - log
        - lora_dim : 4 - 2025-03-24 14:06:35,186 - log
        - lora_alpha : 32 - 2025-03-24 14:06:35,186 - log
        - obj : clm - 2025-03-24 14:06:35,186 - log
        - lora_dropout : 0.1 - 2025-03-24 14:06:35,186 - log
        - label_smooth : 0.1 - 2025-03-24 14:06:35,186 - log
        - roll_interval : -1 - 2025-03-24 14:06:35,186 - log
        - roll_lr : 1e-05 - 2025-03-24 14:06:35,186 - log
        - roll_step : 100 - 2025-03-24 14:06:35,186 - log
        - eval_epoch : 1 - 2025-03-24 14:06:35,186 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 14:06:35,186 - log
==================================================================================================== - 2025-03-24 14:06:35,186 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 14:06:35,186 - log
loading model pretrained weight. - 2025-03-24 14:06:37,718 - log
set max_step: 525 - 2025-03-24 14:06:40,820 - log
start to train the model................ 1 - 2025-03-24 14:06:40,938 - log
==================================================================================================== - 2025-03-24 14:08:08,822 - log
        - platform : local - 2025-03-24 14:08:08,822 - log
        - local_rank : 0 - 2025-03-24 14:08:08,822 - log
        - rank : 0 - 2025-03-24 14:08:08,822 - log
        - device : cuda:0 - 2025-03-24 14:08:08,822 - log
        - world_size : 1 - 2025-03-24 14:08:08,822 - log
        - random_seed : 110 - 2025-03-24 14:08:08,822 - log
        - lr : 0.0002 - 2025-03-24 14:08:08,822 - log
        - weight_decay : 0.01 - 2025-03-24 14:08:08,822 - log
        - correct_bias : True - 2025-03-24 14:08:08,822 - log
        - adam_epislon : 1e-06 - 2025-03-24 14:08:08,822 - log
        - no_decay_bias : False - 2025-03-24 14:08:08,822 - log
        - adam_beta1 : 0.9 - 2025-03-24 14:08:08,822 - log
        - adam_beta2 : 0.999 - 2025-03-24 14:08:08,822 - log
        - scheduler : linear - 2025-03-24 14:08:08,822 - log
        - max_step : None - 2025-03-24 14:08:08,822 - log
        - max_epoch : 5 - 2025-03-24 14:08:08,822 - log
        - warmup_step : 500 - 2025-03-24 14:08:08,822 - log
        - i_steps : 0 - 2025-03-24 14:08:08,822 - log
        - i_lrs : 0.00025 - 2025-03-24 14:08:08,822 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 14:08:08,822 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 14:08:08,822 - log
        - train_batch_size : 4 - 2025-03-24 14:08:08,822 - log
        - valid_batch_size : 2 - 2025-03-24 14:08:08,822 - log
        - grad_acc : 2 - 2025-03-24 14:08:08,822 - log
        - clip : 0.0 - 2025-03-24 14:08:08,822 - log
        - seq_len : 256 - 2025-03-24 14:08:08,822 - log
        - model_card : gpt2.md - 2025-03-24 14:08:08,822 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 14:08:08,822 - log
        - fp16 : False - 2025-03-24 14:08:08,822 - log
        - log_interval : 100 - 2025-03-24 14:08:08,822 - log
        - eval_interval : 2000 - 2025-03-24 14:08:08,822 - log
        - save_interval : 1000 - 2025-03-24 14:08:08,822 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 14:08:08,822 - log
        - lora_dim : 4 - 2025-03-24 14:08:08,822 - log
        - lora_alpha : 32 - 2025-03-24 14:08:08,822 - log
        - obj : clm - 2025-03-24 14:08:08,822 - log
        - lora_dropout : 0.1 - 2025-03-24 14:08:08,822 - log
        - label_smooth : 0.1 - 2025-03-24 14:08:08,822 - log
        - roll_interval : -1 - 2025-03-24 14:08:08,822 - log
        - roll_lr : 1e-05 - 2025-03-24 14:08:08,822 - log
        - roll_step : 100 - 2025-03-24 14:08:08,822 - log
        - eval_epoch : 1 - 2025-03-24 14:08:08,822 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 14:08:08,822 - log
==================================================================================================== - 2025-03-24 14:08:08,822 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 14:08:08,822 - log
loading model pretrained weight. - 2025-03-24 14:08:11,370 - log
==================================================================================================== - 2025-03-24 14:08:50,227 - log
        - platform : local - 2025-03-24 14:08:50,227 - log
        - local_rank : 0 - 2025-03-24 14:08:50,227 - log
        - rank : 0 - 2025-03-24 14:08:50,227 - log
        - device : cuda:0 - 2025-03-24 14:08:50,227 - log
        - world_size : 1 - 2025-03-24 14:08:50,227 - log
        - random_seed : 110 - 2025-03-24 14:08:50,227 - log
        - lr : 0.0002 - 2025-03-24 14:08:50,227 - log
        - weight_decay : 0.01 - 2025-03-24 14:08:50,227 - log
        - correct_bias : True - 2025-03-24 14:08:50,227 - log
        - adam_epislon : 1e-06 - 2025-03-24 14:08:50,227 - log
        - no_decay_bias : False - 2025-03-24 14:08:50,227 - log
        - adam_beta1 : 0.9 - 2025-03-24 14:08:50,227 - log
        - adam_beta2 : 0.999 - 2025-03-24 14:08:50,227 - log
        - scheduler : linear - 2025-03-24 14:08:50,227 - log
        - max_step : None - 2025-03-24 14:08:50,227 - log
        - max_epoch : 5 - 2025-03-24 14:08:50,227 - log
        - warmup_step : 500 - 2025-03-24 14:08:50,227 - log
        - i_steps : 0 - 2025-03-24 14:08:50,227 - log
        - i_lrs : 0.00025 - 2025-03-24 14:08:50,227 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 14:08:50,227 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 14:08:50,227 - log
        - train_batch_size : 4 - 2025-03-24 14:08:50,227 - log
        - valid_batch_size : 2 - 2025-03-24 14:08:50,227 - log
        - grad_acc : 2 - 2025-03-24 14:08:50,227 - log
        - clip : 0.0 - 2025-03-24 14:08:50,227 - log
        - seq_len : 256 - 2025-03-24 14:08:50,227 - log
        - model_card : gpt2.md - 2025-03-24 14:08:50,227 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 14:08:50,227 - log
        - fp16 : False - 2025-03-24 14:08:50,227 - log
        - log_interval : 100 - 2025-03-24 14:08:50,227 - log
        - eval_interval : 2000 - 2025-03-24 14:08:50,227 - log
        - save_interval : 1000 - 2025-03-24 14:08:50,227 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 14:08:50,227 - log
        - lora_dim : 4 - 2025-03-24 14:08:50,227 - log
        - lora_alpha : 32 - 2025-03-24 14:08:50,227 - log
        - obj : clm - 2025-03-24 14:08:50,228 - log
        - lora_dropout : 0.1 - 2025-03-24 14:08:50,228 - log
        - label_smooth : 0.1 - 2025-03-24 14:08:50,228 - log
        - roll_interval : -1 - 2025-03-24 14:08:50,228 - log
        - roll_lr : 1e-05 - 2025-03-24 14:08:50,228 - log
        - roll_step : 100 - 2025-03-24 14:08:50,228 - log
        - eval_epoch : 1 - 2025-03-24 14:08:50,228 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 14:08:50,228 - log
==================================================================================================== - 2025-03-24 14:08:50,228 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 14:08:50,228 - log
loading model pretrained weight. - 2025-03-24 14:08:52,809 - log
set max_step: 525 - 2025-03-24 14:08:55,143 - log
start to train the model................ 1 - 2025-03-24 14:08:55,271 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 14:08:57,723 - log
Exiting from training early - 2025-03-24 14:08:57,723 - log
cleanup dist ... - 2025-03-24 14:08:57,889 - log
==================================================================================================== - 2025-03-24 14:45:43,702 - log
        - platform : local - 2025-03-24 14:45:43,707 - log
        - local_rank : 0 - 2025-03-24 14:45:43,707 - log
        - rank : 0 - 2025-03-24 14:45:43,707 - log
        - device : cuda:0 - 2025-03-24 14:45:43,707 - log
        - world_size : 1 - 2025-03-24 14:45:43,707 - log
        - random_seed : 110 - 2025-03-24 14:45:43,707 - log
        - lr : 0.0002 - 2025-03-24 14:45:43,707 - log
        - weight_decay : 0.01 - 2025-03-24 14:45:43,707 - log
        - correct_bias : True - 2025-03-24 14:45:43,707 - log
        - adam_epislon : 1e-06 - 2025-03-24 14:45:43,707 - log
        - no_decay_bias : False - 2025-03-24 14:45:43,707 - log
        - adam_beta1 : 0.9 - 2025-03-24 14:45:43,707 - log
        - adam_beta2 : 0.999 - 2025-03-24 14:45:43,707 - log
        - scheduler : linear - 2025-03-24 14:45:43,707 - log
        - max_step : None - 2025-03-24 14:45:43,707 - log
        - max_epoch : 5 - 2025-03-24 14:45:43,707 - log
        - warmup_step : 500 - 2025-03-24 14:45:43,707 - log
        - i_steps : 0 - 2025-03-24 14:45:43,707 - log
        - i_lrs : 0.00025 - 2025-03-24 14:45:43,707 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 14:45:43,707 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 14:45:43,707 - log
        - train_batch_size : 4 - 2025-03-24 14:45:43,707 - log
        - valid_batch_size : 2 - 2025-03-24 14:45:43,707 - log
        - grad_acc : 2 - 2025-03-24 14:45:43,707 - log
        - clip : 0.0 - 2025-03-24 14:45:43,707 - log
        - seq_len : 256 - 2025-03-24 14:45:43,707 - log
        - model_card : gpt2.md - 2025-03-24 14:45:43,707 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 14:45:43,707 - log
        - fp16 : False - 2025-03-24 14:45:43,707 - log
        - log_interval : 100 - 2025-03-24 14:45:43,707 - log
        - eval_interval : 2000 - 2025-03-24 14:45:43,707 - log
        - save_interval : 1000 - 2025-03-24 14:45:43,707 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 14:45:43,707 - log
        - lora_dim : 4 - 2025-03-24 14:45:43,707 - log
        - lora_alpha : 32 - 2025-03-24 14:45:43,707 - log
        - obj : clm - 2025-03-24 14:45:43,707 - log
        - lora_dropout : 0.1 - 2025-03-24 14:45:43,707 - log
        - label_smooth : 0.1 - 2025-03-24 14:45:43,707 - log
        - roll_interval : -1 - 2025-03-24 14:45:43,707 - log
        - roll_lr : 1e-05 - 2025-03-24 14:45:43,707 - log
        - roll_step : 100 - 2025-03-24 14:45:43,707 - log
        - eval_epoch : 1 - 2025-03-24 14:45:43,707 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 14:45:43,707 - log
==================================================================================================== - 2025-03-24 14:45:43,707 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 14:45:43,707 - log
loading model pretrained weight. - 2025-03-24 14:45:46,196 - log
==================================================================================================== - 2025-03-24 14:46:40,257 - log
        - platform : local - 2025-03-24 14:46:40,257 - log
        - local_rank : 0 - 2025-03-24 14:46:40,257 - log
        - rank : 0 - 2025-03-24 14:46:40,257 - log
        - device : cuda:0 - 2025-03-24 14:46:40,257 - log
        - world_size : 1 - 2025-03-24 14:46:40,257 - log
        - random_seed : 110 - 2025-03-24 14:46:40,257 - log
        - lr : 0.0002 - 2025-03-24 14:46:40,257 - log
        - weight_decay : 0.01 - 2025-03-24 14:46:40,257 - log
        - correct_bias : True - 2025-03-24 14:46:40,257 - log
        - adam_epislon : 1e-06 - 2025-03-24 14:46:40,257 - log
        - no_decay_bias : False - 2025-03-24 14:46:40,257 - log
        - adam_beta1 : 0.9 - 2025-03-24 14:46:40,257 - log
        - adam_beta2 : 0.999 - 2025-03-24 14:46:40,257 - log
        - scheduler : linear - 2025-03-24 14:46:40,257 - log
        - max_step : None - 2025-03-24 14:46:40,257 - log
        - max_epoch : 5 - 2025-03-24 14:46:40,257 - log
        - warmup_step : 500 - 2025-03-24 14:46:40,257 - log
        - i_steps : 0 - 2025-03-24 14:46:40,257 - log
        - i_lrs : 0.00025 - 2025-03-24 14:46:40,257 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 14:46:40,257 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 14:46:40,257 - log
        - train_batch_size : 4 - 2025-03-24 14:46:40,257 - log
        - valid_batch_size : 2 - 2025-03-24 14:46:40,257 - log
        - grad_acc : 2 - 2025-03-24 14:46:40,257 - log
        - clip : 0.0 - 2025-03-24 14:46:40,257 - log
        - seq_len : 256 - 2025-03-24 14:46:40,258 - log
        - model_card : gpt2.md - 2025-03-24 14:46:40,258 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 14:46:40,258 - log
        - fp16 : False - 2025-03-24 14:46:40,258 - log
        - log_interval : 100 - 2025-03-24 14:46:40,258 - log
        - eval_interval : 2000 - 2025-03-24 14:46:40,258 - log
        - save_interval : 1000 - 2025-03-24 14:46:40,258 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 14:46:40,258 - log
        - lora_dim : 4 - 2025-03-24 14:46:40,258 - log
        - lora_alpha : 32 - 2025-03-24 14:46:40,258 - log
        - obj : clm - 2025-03-24 14:46:40,258 - log
        - lora_dropout : 0.1 - 2025-03-24 14:46:40,258 - log
        - label_smooth : 0.1 - 2025-03-24 14:46:40,258 - log
        - roll_interval : -1 - 2025-03-24 14:46:40,258 - log
        - roll_lr : 1e-05 - 2025-03-24 14:46:40,258 - log
        - roll_step : 100 - 2025-03-24 14:46:40,258 - log
        - eval_epoch : 1 - 2025-03-24 14:46:40,258 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 14:46:40,258 - log
==================================================================================================== - 2025-03-24 14:46:40,258 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 14:46:40,258 - log
loading model pretrained weight. - 2025-03-24 14:46:42,731 - log
==================================================================================================== - 2025-03-24 14:48:13,562 - log
        - platform : local - 2025-03-24 14:48:13,562 - log
        - local_rank : 0 - 2025-03-24 14:48:13,562 - log
        - rank : 0 - 2025-03-24 14:48:13,562 - log
        - device : cuda:0 - 2025-03-24 14:48:13,562 - log
        - world_size : 1 - 2025-03-24 14:48:13,562 - log
        - random_seed : 110 - 2025-03-24 14:48:13,562 - log
        - lr : 0.0002 - 2025-03-24 14:48:13,562 - log
        - weight_decay : 0.01 - 2025-03-24 14:48:13,562 - log
        - correct_bias : True - 2025-03-24 14:48:13,562 - log
        - adam_epislon : 1e-06 - 2025-03-24 14:48:13,562 - log
        - no_decay_bias : False - 2025-03-24 14:48:13,562 - log
        - adam_beta1 : 0.9 - 2025-03-24 14:48:13,562 - log
        - adam_beta2 : 0.999 - 2025-03-24 14:48:13,562 - log
        - scheduler : linear - 2025-03-24 14:48:13,562 - log
        - max_step : None - 2025-03-24 14:48:13,562 - log
        - max_epoch : 5 - 2025-03-24 14:48:13,562 - log
        - warmup_step : 500 - 2025-03-24 14:48:13,562 - log
        - i_steps : 0 - 2025-03-24 14:48:13,562 - log
        - i_lrs : 0.00025 - 2025-03-24 14:48:13,562 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 14:48:13,562 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 14:48:13,562 - log
        - train_batch_size : 4 - 2025-03-24 14:48:13,562 - log
        - valid_batch_size : 2 - 2025-03-24 14:48:13,562 - log
        - grad_acc : 2 - 2025-03-24 14:48:13,562 - log
        - clip : 0.0 - 2025-03-24 14:48:13,562 - log
        - seq_len : 256 - 2025-03-24 14:48:13,562 - log
        - model_card : gpt2.md - 2025-03-24 14:48:13,562 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 14:48:13,562 - log
        - fp16 : False - 2025-03-24 14:48:13,562 - log
        - log_interval : 100 - 2025-03-24 14:48:13,562 - log
        - eval_interval : 2000 - 2025-03-24 14:48:13,562 - log
        - save_interval : 1000 - 2025-03-24 14:48:13,562 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 14:48:13,562 - log
        - lora_dim : 4 - 2025-03-24 14:48:13,562 - log
        - lora_alpha : 32 - 2025-03-24 14:48:13,562 - log
        - obj : clm - 2025-03-24 14:48:13,562 - log
        - lora_dropout : 0.1 - 2025-03-24 14:48:13,563 - log
        - label_smooth : 0.1 - 2025-03-24 14:48:13,563 - log
        - roll_interval : -1 - 2025-03-24 14:48:13,563 - log
        - roll_lr : 1e-05 - 2025-03-24 14:48:13,563 - log
        - roll_step : 100 - 2025-03-24 14:48:13,563 - log
        - eval_epoch : 1 - 2025-03-24 14:48:13,563 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 14:48:13,563 - log
==================================================================================================== - 2025-03-24 14:48:13,563 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 14:48:13,563 - log
loading model pretrained weight. - 2025-03-24 14:48:16,104 - log
set max_step: 525 - 2025-03-24 14:48:18,263 - log
start to train the model................ 1 - 2025-03-24 14:48:18,387 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 14:48:19,877 - log
Exiting from training early - 2025-03-24 14:48:19,878 - log
==================================================================================================== - 2025-03-24 15:57:41,312 - log
        - platform : local - 2025-03-24 15:57:41,312 - log
        - local_rank : 0 - 2025-03-24 15:57:41,312 - log
        - rank : 0 - 2025-03-24 15:57:41,312 - log
        - device : cuda:0 - 2025-03-24 15:57:41,312 - log
        - world_size : 1 - 2025-03-24 15:57:41,312 - log
        - random_seed : 110 - 2025-03-24 15:57:41,312 - log
        - lr : 0.0002 - 2025-03-24 15:57:41,312 - log
        - weight_decay : 0.01 - 2025-03-24 15:57:41,312 - log
        - correct_bias : True - 2025-03-24 15:57:41,312 - log
        - adam_epislon : 1e-06 - 2025-03-24 15:57:41,312 - log
        - no_decay_bias : False - 2025-03-24 15:57:41,312 - log
        - adam_beta1 : 0.9 - 2025-03-24 15:57:41,312 - log
        - adam_beta2 : 0.999 - 2025-03-24 15:57:41,312 - log
        - scheduler : linear - 2025-03-24 15:57:41,312 - log
        - max_step : None - 2025-03-24 15:57:41,312 - log
        - max_epoch : 5 - 2025-03-24 15:57:41,312 - log
        - warmup_step : 500 - 2025-03-24 15:57:41,312 - log
        - i_steps : 0 - 2025-03-24 15:57:41,312 - log
        - i_lrs : 0.00025 - 2025-03-24 15:57:41,312 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 15:57:41,312 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 15:57:41,312 - log
        - train_batch_size : 4 - 2025-03-24 15:57:41,312 - log
        - valid_batch_size : 2 - 2025-03-24 15:57:41,312 - log
        - grad_acc : 2 - 2025-03-24 15:57:41,312 - log
        - clip : 0.0 - 2025-03-24 15:57:41,312 - log
        - seq_len : 256 - 2025-03-24 15:57:41,312 - log
        - model_card : gpt2.md - 2025-03-24 15:57:41,312 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 15:57:41,312 - log
        - fp16 : False - 2025-03-24 15:57:41,312 - log
        - log_interval : 100 - 2025-03-24 15:57:41,312 - log
        - eval_interval : 2000 - 2025-03-24 15:57:41,312 - log
        - save_interval : 1000 - 2025-03-24 15:57:41,312 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 15:57:41,312 - log
        - lora_dim : 4 - 2025-03-24 15:57:41,312 - log
        - lora_alpha : 32 - 2025-03-24 15:57:41,312 - log
        - obj : clm - 2025-03-24 15:57:41,312 - log
        - lora_dropout : 0.1 - 2025-03-24 15:57:41,312 - log
        - label_smooth : 0.1 - 2025-03-24 15:57:41,313 - log
        - roll_interval : -1 - 2025-03-24 15:57:41,313 - log
        - roll_lr : 1e-05 - 2025-03-24 15:57:41,313 - log
        - roll_step : 100 - 2025-03-24 15:57:41,313 - log
        - eval_epoch : 1 - 2025-03-24 15:57:41,313 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 15:57:41,313 - log
==================================================================================================== - 2025-03-24 15:57:41,313 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 15:57:41,313 - log
==================================================================================================== - 2025-03-24 15:57:47,999 - log
        - platform : local - 2025-03-24 15:57:47,999 - log
        - local_rank : 0 - 2025-03-24 15:57:47,999 - log
        - rank : 0 - 2025-03-24 15:57:47,999 - log
        - device : cuda:0 - 2025-03-24 15:57:48,000 - log
        - world_size : 1 - 2025-03-24 15:57:48,000 - log
        - random_seed : 110 - 2025-03-24 15:57:48,000 - log
        - lr : 0.0002 - 2025-03-24 15:57:48,000 - log
        - weight_decay : 0.01 - 2025-03-24 15:57:48,000 - log
        - correct_bias : True - 2025-03-24 15:57:48,000 - log
        - adam_epislon : 1e-06 - 2025-03-24 15:57:48,000 - log
        - no_decay_bias : False - 2025-03-24 15:57:48,000 - log
        - adam_beta1 : 0.9 - 2025-03-24 15:57:48,000 - log
        - adam_beta2 : 0.999 - 2025-03-24 15:57:48,000 - log
        - scheduler : linear - 2025-03-24 15:57:48,000 - log
        - max_step : None - 2025-03-24 15:57:48,000 - log
        - max_epoch : 5 - 2025-03-24 15:57:48,000 - log
        - warmup_step : 500 - 2025-03-24 15:57:48,000 - log
        - i_steps : 0 - 2025-03-24 15:57:48,000 - log
        - i_lrs : 0.00025 - 2025-03-24 15:57:48,000 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 15:57:48,000 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 15:57:48,000 - log
        - train_batch_size : 4 - 2025-03-24 15:57:48,000 - log
        - valid_batch_size : 2 - 2025-03-24 15:57:48,000 - log
        - grad_acc : 2 - 2025-03-24 15:57:48,000 - log
        - clip : 0.0 - 2025-03-24 15:57:48,000 - log
        - seq_len : 256 - 2025-03-24 15:57:48,000 - log
        - model_card : gpt2.md - 2025-03-24 15:57:48,000 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 15:57:48,000 - log
        - fp16 : False - 2025-03-24 15:57:48,000 - log
        - log_interval : 100 - 2025-03-24 15:57:48,000 - log
        - eval_interval : 2000 - 2025-03-24 15:57:48,000 - log
        - save_interval : 1000 - 2025-03-24 15:57:48,000 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 15:57:48,000 - log
        - lora_dim : 4 - 2025-03-24 15:57:48,000 - log
        - lora_alpha : 32 - 2025-03-24 15:57:48,000 - log
        - obj : clm - 2025-03-24 15:57:48,000 - log
        - lora_dropout : 0.1 - 2025-03-24 15:57:48,000 - log
        - label_smooth : 0.1 - 2025-03-24 15:57:48,000 - log
        - roll_interval : -1 - 2025-03-24 15:57:48,000 - log
        - roll_lr : 1e-05 - 2025-03-24 15:57:48,000 - log
        - roll_step : 100 - 2025-03-24 15:57:48,000 - log
        - eval_epoch : 1 - 2025-03-24 15:57:48,000 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 15:57:48,000 - log
==================================================================================================== - 2025-03-24 15:57:48,000 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 15:57:48,000 - log
loading model pretrained weight. - 2025-03-24 15:57:50,521 - log
set max_step: 525 - 2025-03-24 15:57:53,895 - log
start to train the model................ 1 - 2025-03-24 15:57:54,033 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 15:58:00,079 - log
Exiting from training early - 2025-03-24 15:58:00,079 - log
cleanup dist ... - 2025-03-24 15:58:00,292 - log
==================================================================================================== - 2025-03-24 15:59:18,107 - log
        - platform : local - 2025-03-24 15:59:18,108 - log
        - local_rank : 0 - 2025-03-24 15:59:18,108 - log
        - rank : 0 - 2025-03-24 15:59:18,108 - log
        - device : cuda:0 - 2025-03-24 15:59:18,108 - log
        - world_size : 1 - 2025-03-24 15:59:18,108 - log
        - random_seed : 110 - 2025-03-24 15:59:18,108 - log
        - lr : 0.0002 - 2025-03-24 15:59:18,108 - log
        - weight_decay : 0.01 - 2025-03-24 15:59:18,108 - log
        - correct_bias : True - 2025-03-24 15:59:18,108 - log
        - adam_epislon : 1e-06 - 2025-03-24 15:59:18,108 - log
        - no_decay_bias : False - 2025-03-24 15:59:18,108 - log
        - adam_beta1 : 0.9 - 2025-03-24 15:59:18,108 - log
        - adam_beta2 : 0.999 - 2025-03-24 15:59:18,108 - log
        - scheduler : linear - 2025-03-24 15:59:18,108 - log
        - max_step : None - 2025-03-24 15:59:18,108 - log
        - max_epoch : 5 - 2025-03-24 15:59:18,108 - log
        - warmup_step : 500 - 2025-03-24 15:59:18,108 - log
        - i_steps : 0 - 2025-03-24 15:59:18,108 - log
        - i_lrs : 0.00025 - 2025-03-24 15:59:18,108 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 15:59:18,108 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 15:59:18,108 - log
        - train_batch_size : 4 - 2025-03-24 15:59:18,108 - log
        - valid_batch_size : 2 - 2025-03-24 15:59:18,108 - log
        - grad_acc : 2 - 2025-03-24 15:59:18,108 - log
        - clip : 0.0 - 2025-03-24 15:59:18,108 - log
        - seq_len : 256 - 2025-03-24 15:59:18,108 - log
        - model_card : gpt2.md - 2025-03-24 15:59:18,108 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 15:59:18,108 - log
        - fp16 : False - 2025-03-24 15:59:18,108 - log
        - log_interval : 100 - 2025-03-24 15:59:18,108 - log
        - eval_interval : 2000 - 2025-03-24 15:59:18,108 - log
        - save_interval : 1000 - 2025-03-24 15:59:18,108 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 15:59:18,108 - log
        - lora_dim : 4 - 2025-03-24 15:59:18,108 - log
        - lora_alpha : 32 - 2025-03-24 15:59:18,108 - log
        - obj : clm - 2025-03-24 15:59:18,108 - log
        - lora_dropout : 0.1 - 2025-03-24 15:59:18,108 - log
        - label_smooth : 0.1 - 2025-03-24 15:59:18,108 - log
        - roll_interval : -1 - 2025-03-24 15:59:18,108 - log
        - roll_lr : 1e-05 - 2025-03-24 15:59:18,108 - log
        - roll_step : 100 - 2025-03-24 15:59:18,108 - log
        - eval_epoch : 1 - 2025-03-24 15:59:18,108 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 15:59:18,108 - log
==================================================================================================== - 2025-03-24 15:59:18,108 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 15:59:18,108 - log
loading model pretrained weight. - 2025-03-24 15:59:20,659 - log
set max_step: 525 - 2025-03-24 15:59:24,200 - log
start to train the model................ 1 - 2025-03-24 15:59:24,351 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 432.27 | loss  5.20 | avg loss  5.73 | ppl 308.19 - 2025-03-24 16:00:07,578 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.105.pt - 2025-03-24 16:00:09,704 - log
start to train the model................ 2 - 2025-03-24 16:00:11,134 - log
| epoch   2 step      200 |     95 batches | lr 8e-05 | ms/batch 409.65 | loss  3.75 | avg loss  4.69 | ppl 109.10 - 2025-03-24 16:00:52,099 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-24 16:00:56,540 - log
start to train the model................ 3 - 2025-03-24 16:00:58,286 - log
| epoch   3 step      300 |     90 batches | lr 0.00012 | ms/batch 388.44 | loss  2.97 | avg loss  3.37 | ppl 29.00 - 2025-03-24 16:01:37,130 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.315.pt - 2025-03-24 16:01:43,572 - log
start to train the model................ 4 - 2025-03-24 16:01:45,041 - log
| epoch   4 step      400 |     85 batches | lr 0.00016 | ms/batch 367.03 | loss  2.93 | avg loss  3.01 | ppl 20.32 - 2025-03-24 16:02:21,744 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-24 16:02:30,395 - log
start to train the model................ 5 - 2025-03-24 16:02:31,865 - log
| epoch   5 step      500 |     80 batches | lr 0.0002 | ms/batch 343.41 | loss  2.92 | avg loss  2.89 | ppl 18.03 - 2025-03-24 16:03:06,207 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.525.pt - 2025-03-24 16:03:16,988 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 16:03:18,479 - log
End of training - 2025-03-24 16:03:18,479 - log
cleanup dist ... - 2025-03-24 16:03:18,715 - log
==================================================================================================== - 2025-03-24 16:18:18,517 - log
        - platform : local - 2025-03-24 16:18:18,521 - log
        - local_rank : 0 - 2025-03-24 16:18:18,521 - log
        - rank : 0 - 2025-03-24 16:18:18,521 - log
        - device : cuda:0 - 2025-03-24 16:18:18,521 - log
        - world_size : 1 - 2025-03-24 16:18:18,521 - log
        - random_seed : 110 - 2025-03-24 16:18:18,521 - log
        - lr : 0.0002 - 2025-03-24 16:18:18,521 - log
        - weight_decay : 0.01 - 2025-03-24 16:18:18,521 - log
        - correct_bias : True - 2025-03-24 16:18:18,521 - log
        - adam_epislon : 1e-06 - 2025-03-24 16:18:18,521 - log
        - no_decay_bias : False - 2025-03-24 16:18:18,521 - log
        - adam_beta1 : 0.9 - 2025-03-24 16:18:18,521 - log
        - adam_beta2 : 0.999 - 2025-03-24 16:18:18,521 - log
        - scheduler : linear - 2025-03-24 16:18:18,521 - log
        - max_step : None - 2025-03-24 16:18:18,521 - log
        - max_epoch : 5 - 2025-03-24 16:18:18,521 - log
        - warmup_step : 500 - 2025-03-24 16:18:18,521 - log
        - i_steps : 0 - 2025-03-24 16:18:18,521 - log
        - i_lrs : 0.00025 - 2025-03-24 16:18:18,521 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 16:18:18,521 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 16:18:18,521 - log
        - train_batch_size : 4 - 2025-03-24 16:18:18,521 - log
        - valid_batch_size : 2 - 2025-03-24 16:18:18,521 - log
        - grad_acc : 2 - 2025-03-24 16:18:18,521 - log
        - clip : 0.0 - 2025-03-24 16:18:18,521 - log
        - seq_len : 256 - 2025-03-24 16:18:18,521 - log
        - model_card : gpt2.md - 2025-03-24 16:18:18,521 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 16:18:18,521 - log
        - fp16 : False - 2025-03-24 16:18:18,521 - log
        - log_interval : 100 - 2025-03-24 16:18:18,521 - log
        - eval_interval : 2000 - 2025-03-24 16:18:18,521 - log
        - save_interval : 1000 - 2025-03-24 16:18:18,521 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 16:18:18,521 - log
        - lora_dim : 4 - 2025-03-24 16:18:18,521 - log
        - lora_alpha : 32 - 2025-03-24 16:18:18,521 - log
        - obj : clm - 2025-03-24 16:18:18,521 - log
        - lora_dropout : 0.1 - 2025-03-24 16:18:18,521 - log
        - label_smooth : 0.1 - 2025-03-24 16:18:18,521 - log
        - roll_interval : -1 - 2025-03-24 16:18:18,521 - log
        - roll_lr : 1e-05 - 2025-03-24 16:18:18,521 - log
        - roll_step : 100 - 2025-03-24 16:18:18,521 - log
        - eval_epoch : 1 - 2025-03-24 16:18:18,521 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 16:18:18,521 - log
==================================================================================================== - 2025-03-24 16:18:18,521 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 16:18:18,521 - log
loading model pretrained weight. - 2025-03-24 16:18:21,091 - log
set max_step: 525 - 2025-03-24 16:18:25,122 - log
start to train the model................ 1 - 2025-03-24 16:18:25,244 - log
==================================================================================================== - 2025-03-24 16:18:59,609 - log
        - platform : local - 2025-03-24 16:18:59,609 - log
        - local_rank : 0 - 2025-03-24 16:18:59,609 - log
        - rank : 0 - 2025-03-24 16:18:59,609 - log
        - device : cuda:0 - 2025-03-24 16:18:59,609 - log
        - world_size : 1 - 2025-03-24 16:18:59,609 - log
        - random_seed : 110 - 2025-03-24 16:18:59,610 - log
        - lr : 0.0002 - 2025-03-24 16:18:59,610 - log
        - weight_decay : 0.01 - 2025-03-24 16:18:59,610 - log
        - correct_bias : True - 2025-03-24 16:18:59,610 - log
        - adam_epislon : 1e-06 - 2025-03-24 16:18:59,610 - log
        - no_decay_bias : False - 2025-03-24 16:18:59,610 - log
        - adam_beta1 : 0.9 - 2025-03-24 16:18:59,610 - log
        - adam_beta2 : 0.999 - 2025-03-24 16:18:59,610 - log
        - scheduler : linear - 2025-03-24 16:18:59,610 - log
        - max_step : None - 2025-03-24 16:18:59,610 - log
        - max_epoch : 5 - 2025-03-24 16:18:59,610 - log
        - warmup_step : 500 - 2025-03-24 16:18:59,610 - log
        - i_steps : 0 - 2025-03-24 16:18:59,610 - log
        - i_lrs : 0.00025 - 2025-03-24 16:18:59,610 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 16:18:59,610 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 16:18:59,610 - log
        - train_batch_size : 4 - 2025-03-24 16:18:59,610 - log
        - valid_batch_size : 2 - 2025-03-24 16:18:59,610 - log
        - grad_acc : 2 - 2025-03-24 16:18:59,610 - log
        - clip : 0.0 - 2025-03-24 16:18:59,610 - log
        - seq_len : 256 - 2025-03-24 16:18:59,610 - log
        - model_card : gpt2.md - 2025-03-24 16:18:59,610 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 16:18:59,610 - log
        - fp16 : False - 2025-03-24 16:18:59,610 - log
        - log_interval : 100 - 2025-03-24 16:18:59,610 - log
        - eval_interval : 2000 - 2025-03-24 16:18:59,610 - log
        - save_interval : 1000 - 2025-03-24 16:18:59,610 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 16:18:59,610 - log
        - lora_dim : 4 - 2025-03-24 16:18:59,610 - log
        - lora_alpha : 32 - 2025-03-24 16:18:59,610 - log
        - obj : clm - 2025-03-24 16:18:59,610 - log
        - lora_dropout : 0.1 - 2025-03-24 16:18:59,610 - log
        - label_smooth : 0.1 - 2025-03-24 16:18:59,610 - log
        - roll_interval : -1 - 2025-03-24 16:18:59,610 - log
        - roll_lr : 1e-05 - 2025-03-24 16:18:59,610 - log
        - roll_step : 100 - 2025-03-24 16:18:59,610 - log
        - eval_epoch : 1 - 2025-03-24 16:18:59,610 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 16:18:59,610 - log
==================================================================================================== - 2025-03-24 16:18:59,610 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 16:18:59,610 - log
loading model pretrained weight. - 2025-03-24 16:19:02,232 - log
set max_step: 525 - 2025-03-24 16:19:04,623 - log
start to train the model................ 1 - 2025-03-24 16:19:04,745 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 16:19:09,533 - log
Exiting from training early - 2025-03-24 16:19:09,533 - log
cleanup dist ... - 2025-03-24 16:19:09,719 - log
==================================================================================================== - 2025-03-24 16:24:56,234 - log
        - platform : local - 2025-03-24 16:24:56,235 - log
        - local_rank : 0 - 2025-03-24 16:24:56,235 - log
        - rank : 0 - 2025-03-24 16:24:56,235 - log
        - device : cuda:0 - 2025-03-24 16:24:56,235 - log
        - world_size : 1 - 2025-03-24 16:24:56,235 - log
        - random_seed : 110 - 2025-03-24 16:24:56,235 - log
        - lr : 0.0002 - 2025-03-24 16:24:56,235 - log
        - weight_decay : 0.01 - 2025-03-24 16:24:56,235 - log
        - correct_bias : True - 2025-03-24 16:24:56,235 - log
        - adam_epislon : 1e-06 - 2025-03-24 16:24:56,235 - log
        - no_decay_bias : False - 2025-03-24 16:24:56,235 - log
        - adam_beta1 : 0.9 - 2025-03-24 16:24:56,235 - log
        - adam_beta2 : 0.999 - 2025-03-24 16:24:56,235 - log
        - scheduler : linear - 2025-03-24 16:24:56,235 - log
        - max_step : None - 2025-03-24 16:24:56,235 - log
        - max_epoch : 5 - 2025-03-24 16:24:56,235 - log
        - warmup_step : 500 - 2025-03-24 16:24:56,235 - log
        - i_steps : 0 - 2025-03-24 16:24:56,235 - log
        - i_lrs : 0.00025 - 2025-03-24 16:24:56,235 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 16:24:56,235 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 16:24:56,235 - log
        - train_batch_size : 4 - 2025-03-24 16:24:56,235 - log
        - valid_batch_size : 2 - 2025-03-24 16:24:56,235 - log
        - grad_acc : 2 - 2025-03-24 16:24:56,235 - log
        - clip : 0.0 - 2025-03-24 16:24:56,235 - log
        - seq_len : 256 - 2025-03-24 16:24:56,235 - log
        - model_card : gpt2.md - 2025-03-24 16:24:56,235 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 16:24:56,235 - log
        - fp16 : False - 2025-03-24 16:24:56,235 - log
        - log_interval : 100 - 2025-03-24 16:24:56,235 - log
        - eval_interval : 2000 - 2025-03-24 16:24:56,235 - log
        - save_interval : 1000 - 2025-03-24 16:24:56,235 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 16:24:56,235 - log
        - lora_dim : 4 - 2025-03-24 16:24:56,235 - log
        - lora_alpha : 32 - 2025-03-24 16:24:56,235 - log
        - obj : clm - 2025-03-24 16:24:56,235 - log
        - lora_dropout : 0.1 - 2025-03-24 16:24:56,235 - log
        - label_smooth : 0.1 - 2025-03-24 16:24:56,235 - log
        - roll_interval : -1 - 2025-03-24 16:24:56,235 - log
        - roll_lr : 1e-05 - 2025-03-24 16:24:56,235 - log
        - roll_step : 100 - 2025-03-24 16:24:56,235 - log
        - eval_epoch : 1 - 2025-03-24 16:24:56,235 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 16:24:56,235 - log
==================================================================================================== - 2025-03-24 16:24:56,235 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 16:24:56,235 - log
loading model pretrained weight. - 2025-03-24 16:24:58,774 - log
set max_step: 525 - 2025-03-24 16:25:01,591 - log
start to train the model................ 1 - 2025-03-24 16:25:01,716 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 16:25:06,550 - log
Exiting from training early - 2025-03-24 16:25:06,550 - log
==================================================================================================== - 2025-03-24 16:26:18,945 - log
        - platform : local - 2025-03-24 16:26:18,945 - log
        - local_rank : 0 - 2025-03-24 16:26:18,945 - log
        - rank : 0 - 2025-03-24 16:26:18,945 - log
        - device : cuda:0 - 2025-03-24 16:26:18,945 - log
        - world_size : 1 - 2025-03-24 16:26:18,945 - log
        - random_seed : 110 - 2025-03-24 16:26:18,945 - log
        - lr : 0.0002 - 2025-03-24 16:26:18,945 - log
        - weight_decay : 0.01 - 2025-03-24 16:26:18,945 - log
        - correct_bias : True - 2025-03-24 16:26:18,945 - log
        - adam_epislon : 1e-06 - 2025-03-24 16:26:18,945 - log
        - no_decay_bias : False - 2025-03-24 16:26:18,945 - log
        - adam_beta1 : 0.9 - 2025-03-24 16:26:18,945 - log
        - adam_beta2 : 0.999 - 2025-03-24 16:26:18,946 - log
        - scheduler : linear - 2025-03-24 16:26:18,946 - log
        - max_step : None - 2025-03-24 16:26:18,946 - log
        - max_epoch : 5 - 2025-03-24 16:26:18,946 - log
        - warmup_step : 500 - 2025-03-24 16:26:18,946 - log
        - i_steps : 0 - 2025-03-24 16:26:18,946 - log
        - i_lrs : 0.00025 - 2025-03-24 16:26:18,946 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 16:26:18,946 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 16:26:18,946 - log
        - train_batch_size : 4 - 2025-03-24 16:26:18,946 - log
        - valid_batch_size : 2 - 2025-03-24 16:26:18,946 - log
        - grad_acc : 2 - 2025-03-24 16:26:18,946 - log
        - clip : 0.0 - 2025-03-24 16:26:18,946 - log
        - seq_len : 256 - 2025-03-24 16:26:18,946 - log
        - model_card : gpt2.md - 2025-03-24 16:26:18,946 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 16:26:18,946 - log
        - fp16 : False - 2025-03-24 16:26:18,946 - log
        - log_interval : 100 - 2025-03-24 16:26:18,946 - log
        - eval_interval : 2000 - 2025-03-24 16:26:18,946 - log
        - save_interval : 1000 - 2025-03-24 16:26:18,946 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 16:26:18,946 - log
        - lora_dim : 4 - 2025-03-24 16:26:18,946 - log
        - lora_alpha : 32 - 2025-03-24 16:26:18,946 - log
        - obj : clm - 2025-03-24 16:26:18,946 - log
        - lora_dropout : 0.1 - 2025-03-24 16:26:18,946 - log
        - label_smooth : 0.1 - 2025-03-24 16:26:18,946 - log
        - roll_interval : -1 - 2025-03-24 16:26:18,946 - log
        - roll_lr : 1e-05 - 2025-03-24 16:26:18,946 - log
        - roll_step : 100 - 2025-03-24 16:26:18,946 - log
        - eval_epoch : 1 - 2025-03-24 16:26:18,946 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 16:26:18,946 - log
==================================================================================================== - 2025-03-24 16:26:18,946 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 16:26:18,946 - log
loading model pretrained weight. - 2025-03-24 16:26:21,639 - log
set max_step: 525 - 2025-03-24 16:26:25,266 - log
start to train the model................ 1 - 2025-03-24 16:26:25,397 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 16:26:32,044 - log
Exiting from training early - 2025-03-24 16:26:32,045 - log
cleanup dist ... - 2025-03-24 16:26:32,231 - log
==================================================================================================== - 2025-03-24 16:28:10,162 - log
        - platform : local - 2025-03-24 16:28:10,162 - log
        - local_rank : 0 - 2025-03-24 16:28:10,163 - log
        - rank : 0 - 2025-03-24 16:28:10,163 - log
        - device : cuda:0 - 2025-03-24 16:28:10,163 - log
        - world_size : 1 - 2025-03-24 16:28:10,163 - log
        - random_seed : 110 - 2025-03-24 16:28:10,163 - log
        - lr : 0.0002 - 2025-03-24 16:28:10,163 - log
        - weight_decay : 0.01 - 2025-03-24 16:28:10,163 - log
        - correct_bias : True - 2025-03-24 16:28:10,163 - log
        - adam_epislon : 1e-06 - 2025-03-24 16:28:10,163 - log
        - no_decay_bias : False - 2025-03-24 16:28:10,163 - log
        - adam_beta1 : 0.9 - 2025-03-24 16:28:10,163 - log
        - adam_beta2 : 0.999 - 2025-03-24 16:28:10,163 - log
        - scheduler : linear - 2025-03-24 16:28:10,163 - log
        - max_step : None - 2025-03-24 16:28:10,163 - log
        - max_epoch : 5 - 2025-03-24 16:28:10,163 - log
        - warmup_step : 500 - 2025-03-24 16:28:10,163 - log
        - i_steps : 0 - 2025-03-24 16:28:10,163 - log
        - i_lrs : 0.00025 - 2025-03-24 16:28:10,163 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 16:28:10,163 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 16:28:10,163 - log
        - train_batch_size : 4 - 2025-03-24 16:28:10,163 - log
        - valid_batch_size : 2 - 2025-03-24 16:28:10,163 - log
        - grad_acc : 2 - 2025-03-24 16:28:10,163 - log
        - clip : 0.0 - 2025-03-24 16:28:10,163 - log
        - seq_len : 256 - 2025-03-24 16:28:10,163 - log
        - model_card : gpt2.md - 2025-03-24 16:28:10,163 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 16:28:10,163 - log
        - fp16 : False - 2025-03-24 16:28:10,163 - log
        - log_interval : 100 - 2025-03-24 16:28:10,163 - log
        - eval_interval : 2000 - 2025-03-24 16:28:10,163 - log
        - save_interval : 1000 - 2025-03-24 16:28:10,163 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 16:28:10,163 - log
        - lora_dim : 4 - 2025-03-24 16:28:10,163 - log
        - lora_alpha : 32 - 2025-03-24 16:28:10,163 - log
        - obj : clm - 2025-03-24 16:28:10,163 - log
        - lora_dropout : 0.1 - 2025-03-24 16:28:10,163 - log
        - label_smooth : 0.1 - 2025-03-24 16:28:10,163 - log
        - roll_interval : -1 - 2025-03-24 16:28:10,163 - log
        - roll_lr : 1e-05 - 2025-03-24 16:28:10,163 - log
        - roll_step : 100 - 2025-03-24 16:28:10,163 - log
        - eval_epoch : 1 - 2025-03-24 16:28:10,163 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 16:28:10,163 - log
==================================================================================================== - 2025-03-24 16:28:10,163 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 16:28:10,163 - log
loading model pretrained weight. - 2025-03-24 16:28:12,741 - log
set max_step: 525 - 2025-03-24 16:28:15,117 - log
start to train the model................ 1 - 2025-03-24 16:28:15,254 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 16:28:21,571 - log
Exiting from training early - 2025-03-24 16:28:21,571 - log
==================================================================================================== - 2025-03-24 16:32:47,664 - log
        - platform : local - 2025-03-24 16:32:47,664 - log
        - local_rank : 0 - 2025-03-24 16:32:47,664 - log
        - rank : 0 - 2025-03-24 16:32:47,664 - log
        - device : cuda:0 - 2025-03-24 16:32:47,664 - log
        - world_size : 1 - 2025-03-24 16:32:47,664 - log
        - random_seed : 110 - 2025-03-24 16:32:47,664 - log
        - lr : 0.0002 - 2025-03-24 16:32:47,664 - log
        - weight_decay : 0.01 - 2025-03-24 16:32:47,664 - log
        - correct_bias : True - 2025-03-24 16:32:47,664 - log
        - adam_epislon : 1e-06 - 2025-03-24 16:32:47,664 - log
        - no_decay_bias : False - 2025-03-24 16:32:47,664 - log
        - adam_beta1 : 0.9 - 2025-03-24 16:32:47,664 - log
        - adam_beta2 : 0.999 - 2025-03-24 16:32:47,664 - log
        - scheduler : linear - 2025-03-24 16:32:47,664 - log
        - max_step : None - 2025-03-24 16:32:47,664 - log
        - max_epoch : 5 - 2025-03-24 16:32:47,664 - log
        - warmup_step : 500 - 2025-03-24 16:32:47,664 - log
        - i_steps : 0 - 2025-03-24 16:32:47,664 - log
        - i_lrs : 0.00025 - 2025-03-24 16:32:47,664 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 16:32:47,664 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 16:32:47,664 - log
        - train_batch_size : 4 - 2025-03-24 16:32:47,664 - log
        - valid_batch_size : 2 - 2025-03-24 16:32:47,664 - log
        - grad_acc : 2 - 2025-03-24 16:32:47,664 - log
        - clip : 0.0 - 2025-03-24 16:32:47,664 - log
        - seq_len : 256 - 2025-03-24 16:32:47,664 - log
        - model_card : gpt2.md - 2025-03-24 16:32:47,664 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 16:32:47,664 - log
        - fp16 : False - 2025-03-24 16:32:47,664 - log
        - log_interval : 100 - 2025-03-24 16:32:47,664 - log
        - eval_interval : 2000 - 2025-03-24 16:32:47,664 - log
        - save_interval : 1000 - 2025-03-24 16:32:47,664 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 16:32:47,664 - log
        - lora_dim : 4 - 2025-03-24 16:32:47,664 - log
        - lora_alpha : 32 - 2025-03-24 16:32:47,665 - log
        - obj : clm - 2025-03-24 16:32:47,665 - log
        - lora_dropout : 0.1 - 2025-03-24 16:32:47,665 - log
        - label_smooth : 0.1 - 2025-03-24 16:32:47,665 - log
        - roll_interval : -1 - 2025-03-24 16:32:47,665 - log
        - roll_lr : 1e-05 - 2025-03-24 16:32:47,665 - log
        - roll_step : 100 - 2025-03-24 16:32:47,665 - log
        - eval_epoch : 1 - 2025-03-24 16:32:47,665 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 16:32:47,665 - log
==================================================================================================== - 2025-03-24 16:32:47,665 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 16:32:47,665 - log
loading model pretrained weight. - 2025-03-24 16:32:50,272 - log
set max_step: 525 - 2025-03-24 16:32:53,991 - log
start to train the model................ 1 - 2025-03-24 16:32:54,107 - log
==================================================================================================== - 2025-03-24 16:33:35,833 - log
        - platform : local - 2025-03-24 16:33:35,833 - log
        - local_rank : 0 - 2025-03-24 16:33:35,833 - log
        - rank : 0 - 2025-03-24 16:33:35,833 - log
        - device : cuda:0 - 2025-03-24 16:33:35,833 - log
        - world_size : 1 - 2025-03-24 16:33:35,833 - log
        - random_seed : 110 - 2025-03-24 16:33:35,833 - log
        - lr : 0.0002 - 2025-03-24 16:33:35,833 - log
        - weight_decay : 0.01 - 2025-03-24 16:33:35,833 - log
        - correct_bias : True - 2025-03-24 16:33:35,833 - log
        - adam_epislon : 1e-06 - 2025-03-24 16:33:35,833 - log
        - no_decay_bias : False - 2025-03-24 16:33:35,833 - log
        - adam_beta1 : 0.9 - 2025-03-24 16:33:35,833 - log
        - adam_beta2 : 0.999 - 2025-03-24 16:33:35,833 - log
        - scheduler : linear - 2025-03-24 16:33:35,833 - log
        - max_step : None - 2025-03-24 16:33:35,833 - log
        - max_epoch : 5 - 2025-03-24 16:33:35,833 - log
        - warmup_step : 500 - 2025-03-24 16:33:35,833 - log
        - i_steps : 0 - 2025-03-24 16:33:35,833 - log
        - i_lrs : 0.00025 - 2025-03-24 16:33:35,833 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 16:33:35,833 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 16:33:35,833 - log
        - train_batch_size : 4 - 2025-03-24 16:33:35,833 - log
        - valid_batch_size : 2 - 2025-03-24 16:33:35,833 - log
        - grad_acc : 2 - 2025-03-24 16:33:35,833 - log
        - clip : 0.0 - 2025-03-24 16:33:35,833 - log
        - seq_len : 256 - 2025-03-24 16:33:35,833 - log
        - model_card : gpt2.md - 2025-03-24 16:33:35,833 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 16:33:35,833 - log
        - fp16 : False - 2025-03-24 16:33:35,833 - log
        - log_interval : 100 - 2025-03-24 16:33:35,833 - log
        - eval_interval : 2000 - 2025-03-24 16:33:35,833 - log
        - save_interval : 1000 - 2025-03-24 16:33:35,833 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 16:33:35,833 - log
        - lora_dim : 4 - 2025-03-24 16:33:35,833 - log
        - lora_alpha : 32 - 2025-03-24 16:33:35,833 - log
        - obj : clm - 2025-03-24 16:33:35,833 - log
        - lora_dropout : 0.1 - 2025-03-24 16:33:35,833 - log
        - label_smooth : 0.1 - 2025-03-24 16:33:35,833 - log
        - roll_interval : -1 - 2025-03-24 16:33:35,833 - log
        - roll_lr : 1e-05 - 2025-03-24 16:33:35,833 - log
        - roll_step : 100 - 2025-03-24 16:33:35,833 - log
        - eval_epoch : 1 - 2025-03-24 16:33:35,833 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 16:33:35,833 - log
==================================================================================================== - 2025-03-24 16:33:35,833 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 16:33:35,833 - log
loading model pretrained weight. - 2025-03-24 16:33:38,412 - log
set max_step: 525 - 2025-03-24 16:33:40,987 - log
start to train the model................ 1 - 2025-03-24 16:33:41,106 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 16:34:00,241 - log
Exiting from training early - 2025-03-24 16:34:00,241 - log
cleanup dist ... - 2025-03-24 16:34:00,437 - log
==================================================================================================== - 2025-03-24 16:44:15,354 - log
        - platform : local - 2025-03-24 16:44:15,359 - log
        - local_rank : 0 - 2025-03-24 16:44:15,359 - log
        - rank : 0 - 2025-03-24 16:44:15,359 - log
        - device : cuda:0 - 2025-03-24 16:44:15,359 - log
        - world_size : 1 - 2025-03-24 16:44:15,359 - log
        - random_seed : 110 - 2025-03-24 16:44:15,359 - log
        - lr : 0.0002 - 2025-03-24 16:44:15,359 - log
        - weight_decay : 0.01 - 2025-03-24 16:44:15,359 - log
        - correct_bias : True - 2025-03-24 16:44:15,359 - log
        - adam_epislon : 1e-06 - 2025-03-24 16:44:15,359 - log
        - no_decay_bias : False - 2025-03-24 16:44:15,359 - log
        - adam_beta1 : 0.9 - 2025-03-24 16:44:15,359 - log
        - adam_beta2 : 0.999 - 2025-03-24 16:44:15,359 - log
        - scheduler : linear - 2025-03-24 16:44:15,359 - log
        - max_step : None - 2025-03-24 16:44:15,359 - log
        - max_epoch : 5 - 2025-03-24 16:44:15,359 - log
        - warmup_step : 500 - 2025-03-24 16:44:15,359 - log
        - i_steps : 0 - 2025-03-24 16:44:15,359 - log
        - i_lrs : 0.00025 - 2025-03-24 16:44:15,359 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 16:44:15,359 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 16:44:15,359 - log
        - train_batch_size : 4 - 2025-03-24 16:44:15,359 - log
        - valid_batch_size : 2 - 2025-03-24 16:44:15,359 - log
        - grad_acc : 2 - 2025-03-24 16:44:15,359 - log
        - clip : 0.0 - 2025-03-24 16:44:15,359 - log
        - seq_len : 256 - 2025-03-24 16:44:15,359 - log
        - model_card : gpt2.md - 2025-03-24 16:44:15,359 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 16:44:15,359 - log
        - fp16 : False - 2025-03-24 16:44:15,359 - log
        - log_interval : 100 - 2025-03-24 16:44:15,359 - log
        - eval_interval : 2000 - 2025-03-24 16:44:15,359 - log
        - save_interval : 1000 - 2025-03-24 16:44:15,359 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 16:44:15,359 - log
        - lora_dim : 4 - 2025-03-24 16:44:15,359 - log
        - lora_alpha : 32 - 2025-03-24 16:44:15,359 - log
        - obj : clm - 2025-03-24 16:44:15,359 - log
        - lora_dropout : 0.1 - 2025-03-24 16:44:15,359 - log
        - label_smooth : 0.1 - 2025-03-24 16:44:15,359 - log
        - roll_interval : -1 - 2025-03-24 16:44:15,359 - log
        - roll_lr : 1e-05 - 2025-03-24 16:44:15,359 - log
        - roll_step : 100 - 2025-03-24 16:44:15,359 - log
        - eval_epoch : 1 - 2025-03-24 16:44:15,359 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 16:44:15,359 - log
==================================================================================================== - 2025-03-24 16:44:15,359 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 16:44:15,359 - log
loading model pretrained weight. - 2025-03-24 16:44:17,780 - log
set max_step: 525 - 2025-03-24 16:44:21,277 - log
start to train the model................ 1 - 2025-03-24 16:44:21,411 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 16:44:30,023 - log
Exiting from training early - 2025-03-24 16:44:30,023 - log
cleanup dist ... - 2025-03-24 16:44:30,219 - log
==================================================================================================== - 2025-03-24 16:45:37,394 - log
        - platform : local - 2025-03-24 16:45:37,394 - log
        - local_rank : 0 - 2025-03-24 16:45:37,394 - log
        - rank : 0 - 2025-03-24 16:45:37,394 - log
        - device : cuda:0 - 2025-03-24 16:45:37,394 - log
        - world_size : 1 - 2025-03-24 16:45:37,394 - log
        - random_seed : 110 - 2025-03-24 16:45:37,394 - log
        - lr : 0.0002 - 2025-03-24 16:45:37,394 - log
        - weight_decay : 0.01 - 2025-03-24 16:45:37,394 - log
        - correct_bias : True - 2025-03-24 16:45:37,394 - log
        - adam_epislon : 1e-06 - 2025-03-24 16:45:37,394 - log
        - no_decay_bias : False - 2025-03-24 16:45:37,394 - log
        - adam_beta1 : 0.9 - 2025-03-24 16:45:37,394 - log
        - adam_beta2 : 0.999 - 2025-03-24 16:45:37,394 - log
        - scheduler : linear - 2025-03-24 16:45:37,394 - log
        - max_step : None - 2025-03-24 16:45:37,394 - log
        - max_epoch : 5 - 2025-03-24 16:45:37,394 - log
        - warmup_step : 500 - 2025-03-24 16:45:37,394 - log
        - i_steps : 0 - 2025-03-24 16:45:37,394 - log
        - i_lrs : 0.00025 - 2025-03-24 16:45:37,394 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 16:45:37,394 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 16:45:37,394 - log
        - train_batch_size : 4 - 2025-03-24 16:45:37,394 - log
        - valid_batch_size : 2 - 2025-03-24 16:45:37,394 - log
        - grad_acc : 2 - 2025-03-24 16:45:37,394 - log
        - clip : 0.0 - 2025-03-24 16:45:37,394 - log
        - seq_len : 256 - 2025-03-24 16:45:37,394 - log
        - model_card : gpt2.md - 2025-03-24 16:45:37,394 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 16:45:37,394 - log
        - fp16 : False - 2025-03-24 16:45:37,394 - log
        - log_interval : 100 - 2025-03-24 16:45:37,394 - log
        - eval_interval : 2000 - 2025-03-24 16:45:37,394 - log
        - save_interval : 1000 - 2025-03-24 16:45:37,394 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 16:45:37,394 - log
        - lora_dim : 4 - 2025-03-24 16:45:37,394 - log
        - lora_alpha : 32 - 2025-03-24 16:45:37,394 - log
        - obj : clm - 2025-03-24 16:45:37,394 - log
        - lora_dropout : 0.1 - 2025-03-24 16:45:37,394 - log
        - label_smooth : 0.1 - 2025-03-24 16:45:37,394 - log
        - roll_interval : -1 - 2025-03-24 16:45:37,394 - log
        - roll_lr : 1e-05 - 2025-03-24 16:45:37,394 - log
        - roll_step : 100 - 2025-03-24 16:45:37,394 - log
        - eval_epoch : 1 - 2025-03-24 16:45:37,394 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 16:45:37,394 - log
==================================================================================================== - 2025-03-24 16:45:37,394 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 16:45:37,394 - log
loading model pretrained weight. - 2025-03-24 16:45:39,958 - log
set max_step: 525 - 2025-03-24 16:45:42,731 - log
start to train the model................ 1 - 2025-03-24 16:45:42,844 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 16:45:53,752 - log
Exiting from training early - 2025-03-24 16:45:53,752 - log
==================================================================================================== - 2025-03-24 16:56:56,052 - log
        - platform : local - 2025-03-24 16:56:56,052 - log
        - local_rank : 0 - 2025-03-24 16:56:56,052 - log
        - rank : 0 - 2025-03-24 16:56:56,052 - log
        - device : cuda:0 - 2025-03-24 16:56:56,052 - log
        - world_size : 1 - 2025-03-24 16:56:56,052 - log
        - random_seed : 110 - 2025-03-24 16:56:56,052 - log
        - lr : 0.0002 - 2025-03-24 16:56:56,052 - log
        - weight_decay : 0.01 - 2025-03-24 16:56:56,052 - log
        - correct_bias : True - 2025-03-24 16:56:56,052 - log
        - adam_epislon : 1e-06 - 2025-03-24 16:56:56,052 - log
        - no_decay_bias : False - 2025-03-24 16:56:56,052 - log
        - adam_beta1 : 0.9 - 2025-03-24 16:56:56,052 - log
        - adam_beta2 : 0.999 - 2025-03-24 16:56:56,052 - log
        - scheduler : linear - 2025-03-24 16:56:56,052 - log
        - max_step : None - 2025-03-24 16:56:56,052 - log
        - max_epoch : 5 - 2025-03-24 16:56:56,052 - log
        - warmup_step : 500 - 2025-03-24 16:56:56,052 - log
        - i_steps : 0 - 2025-03-24 16:56:56,052 - log
        - i_lrs : 0.00025 - 2025-03-24 16:56:56,052 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 16:56:56,052 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 16:56:56,052 - log
        - train_batch_size : 4 - 2025-03-24 16:56:56,052 - log
        - valid_batch_size : 2 - 2025-03-24 16:56:56,052 - log
        - grad_acc : 2 - 2025-03-24 16:56:56,052 - log
        - clip : 0.0 - 2025-03-24 16:56:56,052 - log
        - seq_len : 256 - 2025-03-24 16:56:56,052 - log
        - model_card : gpt2.md - 2025-03-24 16:56:56,052 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 16:56:56,052 - log
        - fp16 : False - 2025-03-24 16:56:56,052 - log
        - log_interval : 100 - 2025-03-24 16:56:56,052 - log
        - eval_interval : 2000 - 2025-03-24 16:56:56,052 - log
        - save_interval : 1000 - 2025-03-24 16:56:56,052 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 16:56:56,052 - log
        - lora_dim : 4 - 2025-03-24 16:56:56,052 - log
        - lora_alpha : 32 - 2025-03-24 16:56:56,052 - log
        - obj : clm - 2025-03-24 16:56:56,052 - log
        - lora_dropout : 0.1 - 2025-03-24 16:56:56,052 - log
        - label_smooth : 0.1 - 2025-03-24 16:56:56,053 - log
        - roll_interval : -1 - 2025-03-24 16:56:56,053 - log
        - roll_lr : 1e-05 - 2025-03-24 16:56:56,053 - log
        - roll_step : 100 - 2025-03-24 16:56:56,053 - log
        - eval_epoch : 1 - 2025-03-24 16:56:56,053 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 16:56:56,053 - log
==================================================================================================== - 2025-03-24 16:56:56,053 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 16:56:56,053 - log
loading model pretrained weight. - 2025-03-24 16:56:58,648 - log
set max_step: 525 - 2025-03-24 16:57:01,517 - log
start to train the model................ 1 - 2025-03-24 16:57:01,664 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 16:57:06,156 - log
Exiting from training early - 2025-03-24 16:57:06,156 - log
==================================================================================================== - 2025-03-24 16:58:12,104 - log
        - platform : local - 2025-03-24 16:58:12,104 - log
        - local_rank : 0 - 2025-03-24 16:58:12,104 - log
        - rank : 0 - 2025-03-24 16:58:12,104 - log
        - device : cuda:0 - 2025-03-24 16:58:12,104 - log
        - world_size : 1 - 2025-03-24 16:58:12,104 - log
        - random_seed : 110 - 2025-03-24 16:58:12,104 - log
        - lr : 0.0002 - 2025-03-24 16:58:12,104 - log
        - weight_decay : 0.01 - 2025-03-24 16:58:12,104 - log
        - correct_bias : True - 2025-03-24 16:58:12,104 - log
        - adam_epislon : 1e-06 - 2025-03-24 16:58:12,104 - log
        - no_decay_bias : False - 2025-03-24 16:58:12,104 - log
        - adam_beta1 : 0.9 - 2025-03-24 16:58:12,104 - log
        - adam_beta2 : 0.999 - 2025-03-24 16:58:12,104 - log
        - scheduler : linear - 2025-03-24 16:58:12,104 - log
        - max_step : None - 2025-03-24 16:58:12,104 - log
        - max_epoch : 5 - 2025-03-24 16:58:12,104 - log
        - warmup_step : 500 - 2025-03-24 16:58:12,104 - log
        - i_steps : 0 - 2025-03-24 16:58:12,104 - log
        - i_lrs : 0.00025 - 2025-03-24 16:58:12,104 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 16:58:12,104 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 16:58:12,104 - log
        - train_batch_size : 4 - 2025-03-24 16:58:12,105 - log
        - valid_batch_size : 2 - 2025-03-24 16:58:12,105 - log
        - grad_acc : 2 - 2025-03-24 16:58:12,105 - log
        - clip : 0.0 - 2025-03-24 16:58:12,105 - log
        - seq_len : 256 - 2025-03-24 16:58:12,105 - log
        - model_card : gpt2.md - 2025-03-24 16:58:12,105 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 16:58:12,105 - log
        - fp16 : False - 2025-03-24 16:58:12,105 - log
        - log_interval : 100 - 2025-03-24 16:58:12,105 - log
        - eval_interval : 2000 - 2025-03-24 16:58:12,105 - log
        - save_interval : 1000 - 2025-03-24 16:58:12,105 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 16:58:12,105 - log
        - lora_dim : 4 - 2025-03-24 16:58:12,105 - log
        - lora_alpha : 32 - 2025-03-24 16:58:12,105 - log
        - obj : clm - 2025-03-24 16:58:12,105 - log
        - lora_dropout : 0.1 - 2025-03-24 16:58:12,105 - log
        - label_smooth : 0.1 - 2025-03-24 16:58:12,105 - log
        - roll_interval : -1 - 2025-03-24 16:58:12,105 - log
        - roll_lr : 1e-05 - 2025-03-24 16:58:12,105 - log
        - roll_step : 100 - 2025-03-24 16:58:12,105 - log
        - eval_epoch : 1 - 2025-03-24 16:58:12,105 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 16:58:12,105 - log
==================================================================================================== - 2025-03-24 16:58:12,105 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 16:58:12,105 - log
loading model pretrained weight. - 2025-03-24 16:58:14,648 - log
set max_step: 525 - 2025-03-24 16:58:17,549 - log
start to train the model................ 1 - 2025-03-24 16:58:17,666 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 16:58:18,813 - log
Exiting from training early - 2025-03-24 16:58:18,813 - log
cleanup dist ... - 2025-03-24 16:58:18,988 - log
==================================================================================================== - 2025-03-24 17:01:46,405 - log
        - platform : local - 2025-03-24 17:01:46,405 - log
        - local_rank : 0 - 2025-03-24 17:01:46,405 - log
        - rank : 0 - 2025-03-24 17:01:46,405 - log
        - device : cuda:0 - 2025-03-24 17:01:46,405 - log
        - world_size : 1 - 2025-03-24 17:01:46,405 - log
        - random_seed : 110 - 2025-03-24 17:01:46,405 - log
        - lr : 0.0002 - 2025-03-24 17:01:46,405 - log
        - weight_decay : 0.01 - 2025-03-24 17:01:46,405 - log
        - correct_bias : True - 2025-03-24 17:01:46,405 - log
        - adam_epislon : 1e-06 - 2025-03-24 17:01:46,405 - log
        - no_decay_bias : False - 2025-03-24 17:01:46,405 - log
        - adam_beta1 : 0.9 - 2025-03-24 17:01:46,405 - log
        - adam_beta2 : 0.999 - 2025-03-24 17:01:46,405 - log
        - scheduler : linear - 2025-03-24 17:01:46,405 - log
        - max_step : None - 2025-03-24 17:01:46,405 - log
        - max_epoch : 5 - 2025-03-24 17:01:46,405 - log
        - warmup_step : 500 - 2025-03-24 17:01:46,405 - log
        - i_steps : 0 - 2025-03-24 17:01:46,405 - log
        - i_lrs : 0.00025 - 2025-03-24 17:01:46,405 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 17:01:46,405 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 17:01:46,405 - log
        - train_batch_size : 4 - 2025-03-24 17:01:46,405 - log
        - valid_batch_size : 2 - 2025-03-24 17:01:46,405 - log
        - grad_acc : 2 - 2025-03-24 17:01:46,405 - log
        - clip : 0.0 - 2025-03-24 17:01:46,405 - log
        - seq_len : 256 - 2025-03-24 17:01:46,405 - log
        - model_card : gpt2.md - 2025-03-24 17:01:46,405 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 17:01:46,405 - log
        - fp16 : False - 2025-03-24 17:01:46,405 - log
        - log_interval : 100 - 2025-03-24 17:01:46,405 - log
        - eval_interval : 2000 - 2025-03-24 17:01:46,405 - log
        - save_interval : 1000 - 2025-03-24 17:01:46,405 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 17:01:46,405 - log
        - lora_dim : 4 - 2025-03-24 17:01:46,405 - log
        - lora_alpha : 32 - 2025-03-24 17:01:46,405 - log
        - obj : clm - 2025-03-24 17:01:46,405 - log
        - lora_dropout : 0.1 - 2025-03-24 17:01:46,405 - log
        - label_smooth : 0.1 - 2025-03-24 17:01:46,405 - log
        - roll_interval : -1 - 2025-03-24 17:01:46,405 - log
        - roll_lr : 1e-05 - 2025-03-24 17:01:46,405 - log
        - roll_step : 100 - 2025-03-24 17:01:46,406 - log
        - eval_epoch : 1 - 2025-03-24 17:01:46,406 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 17:01:46,406 - log
==================================================================================================== - 2025-03-24 17:01:46,406 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 17:01:46,406 - log
loading model pretrained weight. - 2025-03-24 17:01:48,973 - log
set max_step: 525 - 2025-03-24 17:01:51,380 - log
start to train the model................ 1 - 2025-03-24 17:01:51,575 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 17:01:58,630 - log
Exiting from training early - 2025-03-24 17:01:58,630 - log
cleanup dist ... - 2025-03-24 17:01:58,809 - log
==================================================================================================== - 2025-03-24 17:12:30,334 - log
        - platform : local - 2025-03-24 17:12:30,334 - log
        - local_rank : 0 - 2025-03-24 17:12:30,334 - log
        - rank : 0 - 2025-03-24 17:12:30,334 - log
        - device : cuda:0 - 2025-03-24 17:12:30,334 - log
        - world_size : 1 - 2025-03-24 17:12:30,334 - log
        - random_seed : 110 - 2025-03-24 17:12:30,334 - log
        - lr : 0.0002 - 2025-03-24 17:12:30,334 - log
        - weight_decay : 0.01 - 2025-03-24 17:12:30,334 - log
        - correct_bias : True - 2025-03-24 17:12:30,334 - log
        - adam_epislon : 1e-06 - 2025-03-24 17:12:30,334 - log
        - no_decay_bias : False - 2025-03-24 17:12:30,334 - log
        - adam_beta1 : 0.9 - 2025-03-24 17:12:30,334 - log
        - adam_beta2 : 0.999 - 2025-03-24 17:12:30,334 - log
        - scheduler : linear - 2025-03-24 17:12:30,334 - log
        - max_step : None - 2025-03-24 17:12:30,334 - log
        - max_epoch : 5 - 2025-03-24 17:12:30,334 - log
        - warmup_step : 500 - 2025-03-24 17:12:30,334 - log
        - i_steps : 0 - 2025-03-24 17:12:30,334 - log
        - i_lrs : 0.00025 - 2025-03-24 17:12:30,334 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 17:12:30,334 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 17:12:30,334 - log
        - train_batch_size : 4 - 2025-03-24 17:12:30,334 - log
        - valid_batch_size : 2 - 2025-03-24 17:12:30,334 - log
        - grad_acc : 2 - 2025-03-24 17:12:30,334 - log
        - clip : 0.0 - 2025-03-24 17:12:30,334 - log
        - seq_len : 256 - 2025-03-24 17:12:30,334 - log
        - model_card : gpt2.md - 2025-03-24 17:12:30,334 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 17:12:30,334 - log
        - fp16 : False - 2025-03-24 17:12:30,334 - log
        - log_interval : 100 - 2025-03-24 17:12:30,334 - log
        - eval_interval : 2000 - 2025-03-24 17:12:30,334 - log
        - save_interval : 1000 - 2025-03-24 17:12:30,334 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 17:12:30,334 - log
        - lora_dim : 4 - 2025-03-24 17:12:30,334 - log
        - lora_alpha : 32 - 2025-03-24 17:12:30,334 - log
        - obj : clm - 2025-03-24 17:12:30,334 - log
        - lora_dropout : 0.1 - 2025-03-24 17:12:30,334 - log
        - label_smooth : 0.1 - 2025-03-24 17:12:30,334 - log
        - roll_interval : -1 - 2025-03-24 17:12:30,334 - log
        - roll_lr : 1e-05 - 2025-03-24 17:12:30,334 - log
        - roll_step : 100 - 2025-03-24 17:12:30,334 - log
        - eval_epoch : 1 - 2025-03-24 17:12:30,334 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 17:12:30,334 - log
==================================================================================================== - 2025-03-24 17:12:30,334 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 17:12:30,334 - log
loading model pretrained weight. - 2025-03-24 17:12:32,853 - log
set max_step: 525 - 2025-03-24 17:12:35,031 - log
start to train the model................ 1 - 2025-03-24 17:12:35,182 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 17:12:37,595 - log
Exiting from training early - 2025-03-24 17:12:37,595 - log
cleanup dist ... - 2025-03-24 17:12:37,763 - log
==================================================================================================== - 2025-03-24 17:21:27,242 - log
        - platform : local - 2025-03-24 17:21:27,242 - log
        - local_rank : 0 - 2025-03-24 17:21:27,242 - log
        - rank : 0 - 2025-03-24 17:21:27,242 - log
        - device : cuda:0 - 2025-03-24 17:21:27,242 - log
        - world_size : 1 - 2025-03-24 17:21:27,242 - log
        - random_seed : 110 - 2025-03-24 17:21:27,242 - log
        - lr : 0.0002 - 2025-03-24 17:21:27,242 - log
        - weight_decay : 0.01 - 2025-03-24 17:21:27,242 - log
        - correct_bias : True - 2025-03-24 17:21:27,242 - log
        - adam_epislon : 1e-06 - 2025-03-24 17:21:27,242 - log
        - no_decay_bias : False - 2025-03-24 17:21:27,242 - log
        - adam_beta1 : 0.9 - 2025-03-24 17:21:27,242 - log
        - adam_beta2 : 0.999 - 2025-03-24 17:21:27,242 - log
        - scheduler : linear - 2025-03-24 17:21:27,242 - log
        - max_step : None - 2025-03-24 17:21:27,242 - log
        - max_epoch : 5 - 2025-03-24 17:21:27,242 - log
        - warmup_step : 500 - 2025-03-24 17:21:27,242 - log
        - i_steps : 0 - 2025-03-24 17:21:27,242 - log
        - i_lrs : 0.00025 - 2025-03-24 17:21:27,242 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 17:21:27,242 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 17:21:27,242 - log
        - train_batch_size : 4 - 2025-03-24 17:21:27,242 - log
        - valid_batch_size : 2 - 2025-03-24 17:21:27,242 - log
        - grad_acc : 2 - 2025-03-24 17:21:27,242 - log
        - clip : 0.0 - 2025-03-24 17:21:27,242 - log
        - seq_len : 256 - 2025-03-24 17:21:27,243 - log
        - model_card : gpt2.md - 2025-03-24 17:21:27,243 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 17:21:27,243 - log
        - fp16 : False - 2025-03-24 17:21:27,243 - log
        - log_interval : 100 - 2025-03-24 17:21:27,243 - log
        - eval_interval : 2000 - 2025-03-24 17:21:27,243 - log
        - save_interval : 1000 - 2025-03-24 17:21:27,243 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 17:21:27,243 - log
        - lora_dim : 4 - 2025-03-24 17:21:27,243 - log
        - lora_alpha : 32 - 2025-03-24 17:21:27,243 - log
        - obj : clm - 2025-03-24 17:21:27,243 - log
        - lora_dropout : 0.1 - 2025-03-24 17:21:27,243 - log
        - label_smooth : 0.1 - 2025-03-24 17:21:27,243 - log
        - roll_interval : -1 - 2025-03-24 17:21:27,243 - log
        - roll_lr : 1e-05 - 2025-03-24 17:21:27,243 - log
        - roll_step : 100 - 2025-03-24 17:21:27,243 - log
        - eval_epoch : 1 - 2025-03-24 17:21:27,243 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 17:21:27,243 - log
==================================================================================================== - 2025-03-24 17:21:27,243 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 17:21:27,243 - log
loading model pretrained weight. - 2025-03-24 17:21:29,773 - log
set max_step: 525 - 2025-03-24 17:21:34,102 - log
start to train the model................ 1 - 2025-03-24 17:21:34,244 - log
==================================================================================================== - 2025-03-24 17:25:54,227 - log
        - platform : local - 2025-03-24 17:25:54,227 - log
        - local_rank : 0 - 2025-03-24 17:25:54,227 - log
        - rank : 0 - 2025-03-24 17:25:54,227 - log
        - device : cuda:0 - 2025-03-24 17:25:54,227 - log
        - world_size : 1 - 2025-03-24 17:25:54,227 - log
        - random_seed : 110 - 2025-03-24 17:25:54,228 - log
        - lr : 0.0002 - 2025-03-24 17:25:54,228 - log
        - weight_decay : 0.01 - 2025-03-24 17:25:54,228 - log
        - correct_bias : True - 2025-03-24 17:25:54,228 - log
        - adam_epislon : 1e-06 - 2025-03-24 17:25:54,228 - log
        - no_decay_bias : False - 2025-03-24 17:25:54,228 - log
        - adam_beta1 : 0.9 - 2025-03-24 17:25:54,228 - log
        - adam_beta2 : 0.999 - 2025-03-24 17:25:54,228 - log
        - scheduler : linear - 2025-03-24 17:25:54,228 - log
        - max_step : None - 2025-03-24 17:25:54,228 - log
        - max_epoch : 5 - 2025-03-24 17:25:54,228 - log
        - warmup_step : 500 - 2025-03-24 17:25:54,228 - log
        - i_steps : 0 - 2025-03-24 17:25:54,228 - log
        - i_lrs : 0.00025 - 2025-03-24 17:25:54,228 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 17:25:54,228 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 17:25:54,228 - log
        - train_batch_size : 4 - 2025-03-24 17:25:54,228 - log
        - valid_batch_size : 2 - 2025-03-24 17:25:54,228 - log
        - grad_acc : 2 - 2025-03-24 17:25:54,228 - log
        - clip : 0.0 - 2025-03-24 17:25:54,228 - log
        - seq_len : 256 - 2025-03-24 17:25:54,228 - log
        - model_card : gpt2.md - 2025-03-24 17:25:54,228 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 17:25:54,228 - log
        - fp16 : False - 2025-03-24 17:25:54,228 - log
        - log_interval : 100 - 2025-03-24 17:25:54,228 - log
        - eval_interval : 2000 - 2025-03-24 17:25:54,228 - log
        - save_interval : 1000 - 2025-03-24 17:25:54,228 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 17:25:54,228 - log
        - lora_dim : 4 - 2025-03-24 17:25:54,228 - log
        - lora_alpha : 32 - 2025-03-24 17:25:54,228 - log
        - obj : clm - 2025-03-24 17:25:54,228 - log
        - lora_dropout : 0.1 - 2025-03-24 17:25:54,228 - log
        - label_smooth : 0.1 - 2025-03-24 17:25:54,228 - log
        - roll_interval : -1 - 2025-03-24 17:25:54,228 - log
        - roll_lr : 1e-05 - 2025-03-24 17:25:54,228 - log
        - roll_step : 100 - 2025-03-24 17:25:54,228 - log
        - eval_epoch : 1 - 2025-03-24 17:25:54,228 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 17:25:54,228 - log
==================================================================================================== - 2025-03-24 17:25:54,228 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 17:25:54,228 - log
loading model pretrained weight. - 2025-03-24 17:25:56,801 - log
set max_step: 525 - 2025-03-24 17:25:59,632 - log
start to train the model................ 1 - 2025-03-24 17:25:59,769 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 17:26:01,350 - log
Exiting from training early - 2025-03-24 17:26:01,350 - log
cleanup dist ... - 2025-03-24 17:26:01,568 - log
==================================================================================================== - 2025-03-24 17:38:56,629 - log
        - platform : local - 2025-03-24 17:38:56,630 - log
        - local_rank : 0 - 2025-03-24 17:38:56,630 - log
        - rank : 0 - 2025-03-24 17:38:56,630 - log
        - device : cuda:0 - 2025-03-24 17:38:56,630 - log
        - world_size : 1 - 2025-03-24 17:38:56,630 - log
        - random_seed : 110 - 2025-03-24 17:38:56,630 - log
        - lr : 0.0002 - 2025-03-24 17:38:56,630 - log
        - weight_decay : 0.01 - 2025-03-24 17:38:56,630 - log
        - correct_bias : True - 2025-03-24 17:38:56,630 - log
        - adam_epislon : 1e-06 - 2025-03-24 17:38:56,630 - log
        - no_decay_bias : False - 2025-03-24 17:38:56,630 - log
        - adam_beta1 : 0.9 - 2025-03-24 17:38:56,630 - log
        - adam_beta2 : 0.999 - 2025-03-24 17:38:56,630 - log
        - scheduler : linear - 2025-03-24 17:38:56,630 - log
        - max_step : None - 2025-03-24 17:38:56,630 - log
        - max_epoch : 5 - 2025-03-24 17:38:56,630 - log
        - warmup_step : 500 - 2025-03-24 17:38:56,630 - log
        - i_steps : 0 - 2025-03-24 17:38:56,630 - log
        - i_lrs : 0.00025 - 2025-03-24 17:38:56,630 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 17:38:56,630 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 17:38:56,630 - log
        - train_batch_size : 4 - 2025-03-24 17:38:56,630 - log
        - valid_batch_size : 2 - 2025-03-24 17:38:56,630 - log
        - grad_acc : 2 - 2025-03-24 17:38:56,630 - log
        - clip : 0.0 - 2025-03-24 17:38:56,630 - log
        - seq_len : 256 - 2025-03-24 17:38:56,630 - log
        - model_card : gpt2.md - 2025-03-24 17:38:56,630 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 17:38:56,630 - log
        - fp16 : False - 2025-03-24 17:38:56,630 - log
        - log_interval : 100 - 2025-03-24 17:38:56,630 - log
        - eval_interval : 2000 - 2025-03-24 17:38:56,630 - log
        - save_interval : 1000 - 2025-03-24 17:38:56,630 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 17:38:56,630 - log
        - lora_dim : 4 - 2025-03-24 17:38:56,630 - log
        - lora_alpha : 32 - 2025-03-24 17:38:56,630 - log
        - obj : clm - 2025-03-24 17:38:56,630 - log
        - lora_dropout : 0.1 - 2025-03-24 17:38:56,630 - log
        - label_smooth : 0.1 - 2025-03-24 17:38:56,630 - log
        - roll_interval : -1 - 2025-03-24 17:38:56,630 - log
        - roll_lr : 1e-05 - 2025-03-24 17:38:56,630 - log
        - roll_step : 100 - 2025-03-24 17:38:56,630 - log
        - eval_epoch : 1 - 2025-03-24 17:38:56,630 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 17:38:56,630 - log
==================================================================================================== - 2025-03-24 17:38:56,630 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 17:38:56,630 - log
loading model pretrained weight. - 2025-03-24 17:38:59,145 - log
set max_step: 525 - 2025-03-24 17:39:02,712 - log
start to train the model................ 1 - 2025-03-24 17:39:02,860 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 17:39:35,078 - log
Exiting from training early - 2025-03-24 17:39:35,078 - log
==================================================================================================== - 2025-03-24 17:40:16,045 - log
        - platform : local - 2025-03-24 17:40:16,045 - log
        - local_rank : 0 - 2025-03-24 17:40:16,045 - log
        - rank : 0 - 2025-03-24 17:40:16,045 - log
        - device : cuda:0 - 2025-03-24 17:40:16,045 - log
        - world_size : 1 - 2025-03-24 17:40:16,045 - log
        - random_seed : 110 - 2025-03-24 17:40:16,045 - log
        - lr : 0.0002 - 2025-03-24 17:40:16,045 - log
        - weight_decay : 0.01 - 2025-03-24 17:40:16,045 - log
        - correct_bias : True - 2025-03-24 17:40:16,045 - log
        - adam_epislon : 1e-06 - 2025-03-24 17:40:16,045 - log
        - no_decay_bias : False - 2025-03-24 17:40:16,045 - log
        - adam_beta1 : 0.9 - 2025-03-24 17:40:16,045 - log
        - adam_beta2 : 0.999 - 2025-03-24 17:40:16,045 - log
        - scheduler : linear - 2025-03-24 17:40:16,045 - log
        - max_step : None - 2025-03-24 17:40:16,045 - log
        - max_epoch : 5 - 2025-03-24 17:40:16,045 - log
        - warmup_step : 500 - 2025-03-24 17:40:16,045 - log
        - i_steps : 0 - 2025-03-24 17:40:16,045 - log
        - i_lrs : 0.00025 - 2025-03-24 17:40:16,045 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 17:40:16,045 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 17:40:16,045 - log
        - train_batch_size : 4 - 2025-03-24 17:40:16,045 - log
        - valid_batch_size : 2 - 2025-03-24 17:40:16,045 - log
        - grad_acc : 2 - 2025-03-24 17:40:16,045 - log
        - clip : 0.0 - 2025-03-24 17:40:16,045 - log
        - seq_len : 256 - 2025-03-24 17:40:16,045 - log
        - model_card : gpt2.md - 2025-03-24 17:40:16,045 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 17:40:16,045 - log
        - fp16 : False - 2025-03-24 17:40:16,045 - log
        - log_interval : 100 - 2025-03-24 17:40:16,045 - log
        - eval_interval : 2000 - 2025-03-24 17:40:16,045 - log
        - save_interval : 1000 - 2025-03-24 17:40:16,045 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 17:40:16,046 - log
        - lora_dim : 4 - 2025-03-24 17:40:16,046 - log
        - lora_alpha : 32 - 2025-03-24 17:40:16,046 - log
        - obj : clm - 2025-03-24 17:40:16,046 - log
        - lora_dropout : 0.1 - 2025-03-24 17:40:16,046 - log
        - label_smooth : 0.1 - 2025-03-24 17:40:16,046 - log
        - roll_interval : -1 - 2025-03-24 17:40:16,046 - log
        - roll_lr : 1e-05 - 2025-03-24 17:40:16,046 - log
        - roll_step : 100 - 2025-03-24 17:40:16,046 - log
        - eval_epoch : 1 - 2025-03-24 17:40:16,046 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 17:40:16,046 - log
==================================================================================================== - 2025-03-24 17:40:16,046 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 17:40:16,046 - log
loading model pretrained weight. - 2025-03-24 17:40:18,584 - log
set max_step: 525 - 2025-03-24 17:40:21,631 - log
start to train the model................ 1 - 2025-03-24 17:40:21,778 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 17:40:40,742 - log
Exiting from training early - 2025-03-24 17:40:40,742 - log
==================================================================================================== - 2025-03-24 17:46:27,496 - log
        - platform : local - 2025-03-24 17:46:27,497 - log
        - local_rank : 0 - 2025-03-24 17:46:27,497 - log
        - rank : 0 - 2025-03-24 17:46:27,497 - log
        - device : cuda:0 - 2025-03-24 17:46:27,497 - log
        - world_size : 1 - 2025-03-24 17:46:27,497 - log
        - random_seed : 110 - 2025-03-24 17:46:27,497 - log
        - lr : 0.0002 - 2025-03-24 17:46:27,497 - log
        - weight_decay : 0.01 - 2025-03-24 17:46:27,497 - log
        - correct_bias : True - 2025-03-24 17:46:27,497 - log
        - adam_epislon : 1e-06 - 2025-03-24 17:46:27,497 - log
        - no_decay_bias : False - 2025-03-24 17:46:27,497 - log
        - adam_beta1 : 0.9 - 2025-03-24 17:46:27,497 - log
        - adam_beta2 : 0.999 - 2025-03-24 17:46:27,497 - log
        - scheduler : linear - 2025-03-24 17:46:27,497 - log
        - max_step : None - 2025-03-24 17:46:27,497 - log
        - max_epoch : 5 - 2025-03-24 17:46:27,497 - log
        - warmup_step : 500 - 2025-03-24 17:46:27,497 - log
        - i_steps : 0 - 2025-03-24 17:46:27,497 - log
        - i_lrs : 0.00025 - 2025-03-24 17:46:27,497 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 17:46:27,497 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 17:46:27,497 - log
        - train_batch_size : 4 - 2025-03-24 17:46:27,497 - log
        - valid_batch_size : 2 - 2025-03-24 17:46:27,497 - log
        - grad_acc : 2 - 2025-03-24 17:46:27,497 - log
        - clip : 0.0 - 2025-03-24 17:46:27,497 - log
        - seq_len : 256 - 2025-03-24 17:46:27,497 - log
        - model_card : gpt2.md - 2025-03-24 17:46:27,497 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 17:46:27,497 - log
        - fp16 : False - 2025-03-24 17:46:27,497 - log
        - log_interval : 100 - 2025-03-24 17:46:27,497 - log
        - eval_interval : 2000 - 2025-03-24 17:46:27,497 - log
        - save_interval : 1000 - 2025-03-24 17:46:27,497 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 17:46:27,497 - log
        - lora_dim : 4 - 2025-03-24 17:46:27,497 - log
        - lora_alpha : 32 - 2025-03-24 17:46:27,497 - log
        - obj : clm - 2025-03-24 17:46:27,497 - log
        - lora_dropout : 0.1 - 2025-03-24 17:46:27,497 - log
        - label_smooth : 0.1 - 2025-03-24 17:46:27,497 - log
        - roll_interval : -1 - 2025-03-24 17:46:27,497 - log
        - roll_lr : 1e-05 - 2025-03-24 17:46:27,497 - log
        - roll_step : 100 - 2025-03-24 17:46:27,497 - log
        - eval_epoch : 1 - 2025-03-24 17:46:27,497 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 17:46:27,497 - log
==================================================================================================== - 2025-03-24 17:46:27,497 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 17:46:27,497 - log
loading model pretrained weight. - 2025-03-24 17:46:30,054 - log
==================================================================================================== - 2025-03-24 17:46:59,714 - log
        - platform : local - 2025-03-24 17:46:59,714 - log
        - local_rank : 0 - 2025-03-24 17:46:59,714 - log
        - rank : 0 - 2025-03-24 17:46:59,714 - log
        - device : cuda:0 - 2025-03-24 17:46:59,714 - log
        - world_size : 1 - 2025-03-24 17:46:59,714 - log
        - random_seed : 110 - 2025-03-24 17:46:59,714 - log
        - lr : 0.0002 - 2025-03-24 17:46:59,714 - log
        - weight_decay : 0.01 - 2025-03-24 17:46:59,714 - log
        - correct_bias : True - 2025-03-24 17:46:59,714 - log
        - adam_epislon : 1e-06 - 2025-03-24 17:46:59,714 - log
        - no_decay_bias : False - 2025-03-24 17:46:59,714 - log
        - adam_beta1 : 0.9 - 2025-03-24 17:46:59,714 - log
        - adam_beta2 : 0.999 - 2025-03-24 17:46:59,714 - log
        - scheduler : linear - 2025-03-24 17:46:59,714 - log
        - max_step : None - 2025-03-24 17:46:59,715 - log
        - max_epoch : 5 - 2025-03-24 17:46:59,715 - log
        - warmup_step : 500 - 2025-03-24 17:46:59,715 - log
        - i_steps : 0 - 2025-03-24 17:46:59,715 - log
        - i_lrs : 0.00025 - 2025-03-24 17:46:59,715 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 17:46:59,715 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 17:46:59,715 - log
        - train_batch_size : 4 - 2025-03-24 17:46:59,715 - log
        - valid_batch_size : 2 - 2025-03-24 17:46:59,715 - log
        - grad_acc : 2 - 2025-03-24 17:46:59,715 - log
        - clip : 0.0 - 2025-03-24 17:46:59,715 - log
        - seq_len : 256 - 2025-03-24 17:46:59,715 - log
        - model_card : gpt2.md - 2025-03-24 17:46:59,715 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 17:46:59,715 - log
        - fp16 : False - 2025-03-24 17:46:59,715 - log
        - log_interval : 100 - 2025-03-24 17:46:59,715 - log
        - eval_interval : 2000 - 2025-03-24 17:46:59,715 - log
        - save_interval : 1000 - 2025-03-24 17:46:59,715 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 17:46:59,715 - log
        - lora_dim : 4 - 2025-03-24 17:46:59,715 - log
        - lora_alpha : 32 - 2025-03-24 17:46:59,715 - log
        - obj : clm - 2025-03-24 17:46:59,715 - log
        - lora_dropout : 0.1 - 2025-03-24 17:46:59,715 - log
        - label_smooth : 0.1 - 2025-03-24 17:46:59,715 - log
        - roll_interval : -1 - 2025-03-24 17:46:59,715 - log
        - roll_lr : 1e-05 - 2025-03-24 17:46:59,715 - log
        - roll_step : 100 - 2025-03-24 17:46:59,715 - log
        - eval_epoch : 1 - 2025-03-24 17:46:59,715 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 17:46:59,715 - log
==================================================================================================== - 2025-03-24 17:46:59,715 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 17:46:59,715 - log
loading model pretrained weight. - 2025-03-24 17:47:02,250 - log
==================================================================================================== - 2025-03-24 17:48:43,952 - log
        - platform : local - 2025-03-24 17:48:43,953 - log
        - local_rank : 0 - 2025-03-24 17:48:43,953 - log
        - rank : 0 - 2025-03-24 17:48:43,953 - log
        - device : cuda:0 - 2025-03-24 17:48:43,953 - log
        - world_size : 1 - 2025-03-24 17:48:43,953 - log
        - random_seed : 110 - 2025-03-24 17:48:43,953 - log
        - lr : 0.0002 - 2025-03-24 17:48:43,953 - log
        - weight_decay : 0.01 - 2025-03-24 17:48:43,953 - log
        - correct_bias : True - 2025-03-24 17:48:43,953 - log
        - adam_epislon : 1e-06 - 2025-03-24 17:48:43,953 - log
        - no_decay_bias : False - 2025-03-24 17:48:43,953 - log
        - adam_beta1 : 0.9 - 2025-03-24 17:48:43,953 - log
        - adam_beta2 : 0.999 - 2025-03-24 17:48:43,953 - log
        - scheduler : linear - 2025-03-24 17:48:43,953 - log
        - max_step : None - 2025-03-24 17:48:43,953 - log
        - max_epoch : 5 - 2025-03-24 17:48:43,953 - log
        - warmup_step : 500 - 2025-03-24 17:48:43,953 - log
        - i_steps : 0 - 2025-03-24 17:48:43,953 - log
        - i_lrs : 0.00025 - 2025-03-24 17:48:43,953 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 17:48:43,953 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 17:48:43,953 - log
        - train_batch_size : 4 - 2025-03-24 17:48:43,953 - log
        - valid_batch_size : 2 - 2025-03-24 17:48:43,953 - log
        - grad_acc : 2 - 2025-03-24 17:48:43,953 - log
        - clip : 0.0 - 2025-03-24 17:48:43,953 - log
        - seq_len : 256 - 2025-03-24 17:48:43,953 - log
        - model_card : gpt2.md - 2025-03-24 17:48:43,953 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 17:48:43,953 - log
        - fp16 : False - 2025-03-24 17:48:43,953 - log
        - log_interval : 100 - 2025-03-24 17:48:43,953 - log
        - eval_interval : 2000 - 2025-03-24 17:48:43,953 - log
        - save_interval : 1000 - 2025-03-24 17:48:43,953 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 17:48:43,953 - log
        - lora_dim : 4 - 2025-03-24 17:48:43,953 - log
        - lora_alpha : 32 - 2025-03-24 17:48:43,953 - log
        - obj : clm - 2025-03-24 17:48:43,953 - log
        - lora_dropout : 0.1 - 2025-03-24 17:48:43,953 - log
        - label_smooth : 0.1 - 2025-03-24 17:48:43,953 - log
        - roll_interval : -1 - 2025-03-24 17:48:43,953 - log
        - roll_lr : 1e-05 - 2025-03-24 17:48:43,953 - log
        - roll_step : 100 - 2025-03-24 17:48:43,953 - log
        - eval_epoch : 1 - 2025-03-24 17:48:43,953 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 17:48:43,953 - log
==================================================================================================== - 2025-03-24 17:48:43,953 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 17:48:43,953 - log
loading model pretrained weight. - 2025-03-24 17:48:46,514 - log
set max_step: 525 - 2025-03-24 17:48:48,857 - log
start to train the model................ 1 - 2025-03-24 17:48:49,007 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 17:48:50,144 - log
Exiting from training early - 2025-03-24 17:48:50,144 - log
==================================================================================================== - 2025-03-24 17:50:31,335 - log
        - platform : local - 2025-03-24 17:50:31,335 - log
        - local_rank : 0 - 2025-03-24 17:50:31,335 - log
        - rank : 0 - 2025-03-24 17:50:31,335 - log
        - device : cuda:0 - 2025-03-24 17:50:31,335 - log
        - world_size : 1 - 2025-03-24 17:50:31,335 - log
        - random_seed : 110 - 2025-03-24 17:50:31,335 - log
        - lr : 0.0002 - 2025-03-24 17:50:31,335 - log
        - weight_decay : 0.01 - 2025-03-24 17:50:31,335 - log
        - correct_bias : True - 2025-03-24 17:50:31,335 - log
        - adam_epislon : 1e-06 - 2025-03-24 17:50:31,336 - log
        - no_decay_bias : False - 2025-03-24 17:50:31,336 - log
        - adam_beta1 : 0.9 - 2025-03-24 17:50:31,336 - log
        - adam_beta2 : 0.999 - 2025-03-24 17:50:31,336 - log
        - scheduler : linear - 2025-03-24 17:50:31,336 - log
        - max_step : None - 2025-03-24 17:50:31,336 - log
        - max_epoch : 5 - 2025-03-24 17:50:31,336 - log
        - warmup_step : 500 - 2025-03-24 17:50:31,336 - log
        - i_steps : 0 - 2025-03-24 17:50:31,336 - log
        - i_lrs : 0.00025 - 2025-03-24 17:50:31,336 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 17:50:31,336 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 17:50:31,336 - log
        - train_batch_size : 4 - 2025-03-24 17:50:31,336 - log
        - valid_batch_size : 2 - 2025-03-24 17:50:31,336 - log
        - grad_acc : 2 - 2025-03-24 17:50:31,336 - log
        - clip : 0.0 - 2025-03-24 17:50:31,336 - log
        - seq_len : 256 - 2025-03-24 17:50:31,336 - log
        - model_card : gpt2.md - 2025-03-24 17:50:31,336 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 17:50:31,336 - log
        - fp16 : False - 2025-03-24 17:50:31,336 - log
        - log_interval : 100 - 2025-03-24 17:50:31,336 - log
        - eval_interval : 2000 - 2025-03-24 17:50:31,336 - log
        - save_interval : 1000 - 2025-03-24 17:50:31,336 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 17:50:31,336 - log
        - lora_dim : 4 - 2025-03-24 17:50:31,336 - log
        - lora_alpha : 32 - 2025-03-24 17:50:31,336 - log
        - obj : clm - 2025-03-24 17:50:31,336 - log
        - lora_dropout : 0.1 - 2025-03-24 17:50:31,336 - log
        - label_smooth : 0.1 - 2025-03-24 17:50:31,336 - log
        - roll_interval : -1 - 2025-03-24 17:50:31,336 - log
        - roll_lr : 1e-05 - 2025-03-24 17:50:31,336 - log
        - roll_step : 100 - 2025-03-24 17:50:31,336 - log
        - eval_epoch : 1 - 2025-03-24 17:50:31,336 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 17:50:31,336 - log
==================================================================================================== - 2025-03-24 17:50:31,336 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 17:50:31,336 - log
loading model pretrained weight. - 2025-03-24 17:50:33,890 - log
set max_step: 525 - 2025-03-24 17:50:36,270 - log
start to train the model................ 1 - 2025-03-24 17:50:36,401 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 17:50:53,667 - log
Exiting from training early - 2025-03-24 17:50:53,667 - log
==================================================================================================== - 2025-03-24 17:51:03,884 - log
        - platform : local - 2025-03-24 17:51:03,884 - log
        - local_rank : 0 - 2025-03-24 17:51:03,884 - log
        - rank : 0 - 2025-03-24 17:51:03,884 - log
        - device : cuda:0 - 2025-03-24 17:51:03,884 - log
        - world_size : 1 - 2025-03-24 17:51:03,884 - log
        - random_seed : 110 - 2025-03-24 17:51:03,884 - log
        - lr : 0.0002 - 2025-03-24 17:51:03,884 - log
        - weight_decay : 0.01 - 2025-03-24 17:51:03,884 - log
        - correct_bias : True - 2025-03-24 17:51:03,884 - log
        - adam_epislon : 1e-06 - 2025-03-24 17:51:03,884 - log
        - no_decay_bias : False - 2025-03-24 17:51:03,885 - log
        - adam_beta1 : 0.9 - 2025-03-24 17:51:03,885 - log
        - adam_beta2 : 0.999 - 2025-03-24 17:51:03,885 - log
        - scheduler : linear - 2025-03-24 17:51:03,885 - log
        - max_step : None - 2025-03-24 17:51:03,885 - log
        - max_epoch : 5 - 2025-03-24 17:51:03,885 - log
        - warmup_step : 500 - 2025-03-24 17:51:03,885 - log
        - i_steps : 0 - 2025-03-24 17:51:03,885 - log
        - i_lrs : 0.00025 - 2025-03-24 17:51:03,885 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 17:51:03,885 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 17:51:03,885 - log
        - train_batch_size : 4 - 2025-03-24 17:51:03,885 - log
        - valid_batch_size : 2 - 2025-03-24 17:51:03,885 - log
        - grad_acc : 2 - 2025-03-24 17:51:03,885 - log
        - clip : 0.0 - 2025-03-24 17:51:03,885 - log
        - seq_len : 256 - 2025-03-24 17:51:03,885 - log
        - model_card : gpt2.md - 2025-03-24 17:51:03,885 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 17:51:03,885 - log
        - fp16 : False - 2025-03-24 17:51:03,885 - log
        - log_interval : 100 - 2025-03-24 17:51:03,885 - log
        - eval_interval : 2000 - 2025-03-24 17:51:03,885 - log
        - save_interval : 1000 - 2025-03-24 17:51:03,885 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 17:51:03,885 - log
        - lora_dim : 4 - 2025-03-24 17:51:03,885 - log
        - lora_alpha : 32 - 2025-03-24 17:51:03,885 - log
        - obj : clm - 2025-03-24 17:51:03,885 - log
        - lora_dropout : 0.1 - 2025-03-24 17:51:03,885 - log
        - label_smooth : 0.1 - 2025-03-24 17:51:03,885 - log
        - roll_interval : -1 - 2025-03-24 17:51:03,885 - log
        - roll_lr : 1e-05 - 2025-03-24 17:51:03,885 - log
        - roll_step : 100 - 2025-03-24 17:51:03,885 - log
        - eval_epoch : 1 - 2025-03-24 17:51:03,885 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 17:51:03,885 - log
==================================================================================================== - 2025-03-24 17:51:03,885 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 17:51:03,885 - log
==================================================================================================== - 2025-03-24 17:54:01,707 - log
        - platform : local - 2025-03-24 17:54:01,707 - log
        - local_rank : 0 - 2025-03-24 17:54:01,707 - log
        - rank : 0 - 2025-03-24 17:54:01,707 - log
        - device : cuda:0 - 2025-03-24 17:54:01,707 - log
        - world_size : 1 - 2025-03-24 17:54:01,707 - log
        - random_seed : 110 - 2025-03-24 17:54:01,707 - log
        - lr : 0.0002 - 2025-03-24 17:54:01,707 - log
        - weight_decay : 0.01 - 2025-03-24 17:54:01,707 - log
        - correct_bias : True - 2025-03-24 17:54:01,707 - log
        - adam_epislon : 1e-06 - 2025-03-24 17:54:01,707 - log
        - no_decay_bias : False - 2025-03-24 17:54:01,707 - log
        - adam_beta1 : 0.9 - 2025-03-24 17:54:01,707 - log
        - adam_beta2 : 0.999 - 2025-03-24 17:54:01,707 - log
        - scheduler : linear - 2025-03-24 17:54:01,707 - log
        - max_step : None - 2025-03-24 17:54:01,707 - log
        - max_epoch : 5 - 2025-03-24 17:54:01,707 - log
        - warmup_step : 500 - 2025-03-24 17:54:01,707 - log
        - i_steps : 0 - 2025-03-24 17:54:01,707 - log
        - i_lrs : 0.00025 - 2025-03-24 17:54:01,708 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 17:54:01,708 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 17:54:01,708 - log
        - train_batch_size : 4 - 2025-03-24 17:54:01,708 - log
        - valid_batch_size : 2 - 2025-03-24 17:54:01,708 - log
        - grad_acc : 2 - 2025-03-24 17:54:01,708 - log
        - clip : 0.0 - 2025-03-24 17:54:01,708 - log
        - seq_len : 256 - 2025-03-24 17:54:01,708 - log
        - model_card : gpt2.md - 2025-03-24 17:54:01,708 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 17:54:01,708 - log
        - fp16 : False - 2025-03-24 17:54:01,708 - log
        - log_interval : 100 - 2025-03-24 17:54:01,708 - log
        - eval_interval : 2000 - 2025-03-24 17:54:01,708 - log
        - save_interval : 1000 - 2025-03-24 17:54:01,708 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 17:54:01,708 - log
        - lora_dim : 4 - 2025-03-24 17:54:01,708 - log
        - lora_alpha : 32 - 2025-03-24 17:54:01,708 - log
        - obj : clm - 2025-03-24 17:54:01,708 - log
        - lora_dropout : 0.1 - 2025-03-24 17:54:01,708 - log
        - label_smooth : 0.1 - 2025-03-24 17:54:01,708 - log
        - roll_interval : -1 - 2025-03-24 17:54:01,708 - log
        - roll_lr : 1e-05 - 2025-03-24 17:54:01,708 - log
        - roll_step : 100 - 2025-03-24 17:54:01,708 - log
        - eval_epoch : 1 - 2025-03-24 17:54:01,708 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 17:54:01,708 - log
==================================================================================================== - 2025-03-24 17:54:01,708 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 17:54:01,708 - log
==================================================================================================== - 2025-03-24 17:59:38,108 - log
        - platform : local - 2025-03-24 17:59:38,108 - log
        - local_rank : 0 - 2025-03-24 17:59:38,108 - log
        - rank : 0 - 2025-03-24 17:59:38,108 - log
        - device : cuda:0 - 2025-03-24 17:59:38,108 - log
        - world_size : 1 - 2025-03-24 17:59:38,108 - log
        - random_seed : 110 - 2025-03-24 17:59:38,108 - log
        - lr : 0.0002 - 2025-03-24 17:59:38,108 - log
        - weight_decay : 0.01 - 2025-03-24 17:59:38,108 - log
        - correct_bias : True - 2025-03-24 17:59:38,108 - log
        - adam_epislon : 1e-06 - 2025-03-24 17:59:38,108 - log
        - no_decay_bias : False - 2025-03-24 17:59:38,108 - log
        - adam_beta1 : 0.9 - 2025-03-24 17:59:38,108 - log
        - adam_beta2 : 0.999 - 2025-03-24 17:59:38,108 - log
        - scheduler : linear - 2025-03-24 17:59:38,108 - log
        - max_step : None - 2025-03-24 17:59:38,108 - log
        - max_epoch : 5 - 2025-03-24 17:59:38,108 - log
        - warmup_step : 500 - 2025-03-24 17:59:38,108 - log
        - i_steps : 0 - 2025-03-24 17:59:38,108 - log
        - i_lrs : 0.00025 - 2025-03-24 17:59:38,108 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 17:59:38,108 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 17:59:38,108 - log
        - train_batch_size : 4 - 2025-03-24 17:59:38,108 - log
        - valid_batch_size : 2 - 2025-03-24 17:59:38,108 - log
        - grad_acc : 2 - 2025-03-24 17:59:38,108 - log
        - clip : 0.0 - 2025-03-24 17:59:38,108 - log
        - seq_len : 256 - 2025-03-24 17:59:38,108 - log
        - model_card : gpt2.md - 2025-03-24 17:59:38,108 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 17:59:38,108 - log
        - fp16 : False - 2025-03-24 17:59:38,108 - log
        - log_interval : 100 - 2025-03-24 17:59:38,108 - log
        - eval_interval : 2000 - 2025-03-24 17:59:38,108 - log
        - save_interval : 1000 - 2025-03-24 17:59:38,108 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 17:59:38,108 - log
        - lora_dim : 4 - 2025-03-24 17:59:38,108 - log
        - lora_alpha : 32 - 2025-03-24 17:59:38,108 - log
        - obj : clm - 2025-03-24 17:59:38,108 - log
        - lora_dropout : 0.1 - 2025-03-24 17:59:38,108 - log
        - label_smooth : 0.1 - 2025-03-24 17:59:38,108 - log
        - roll_interval : -1 - 2025-03-24 17:59:38,108 - log
        - roll_lr : 1e-05 - 2025-03-24 17:59:38,109 - log
        - roll_step : 100 - 2025-03-24 17:59:38,109 - log
        - eval_epoch : 1 - 2025-03-24 17:59:38,109 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 17:59:38,109 - log
==================================================================================================== - 2025-03-24 17:59:38,109 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 17:59:38,109 - log
loading model pretrained weight. - 2025-03-24 17:59:40,663 - log
==================================================================================================== - 2025-03-24 18:01:07,152 - log
        - platform : local - 2025-03-24 18:01:07,152 - log
        - local_rank : 0 - 2025-03-24 18:01:07,152 - log
        - rank : 0 - 2025-03-24 18:01:07,152 - log
        - device : cuda:0 - 2025-03-24 18:01:07,152 - log
        - world_size : 1 - 2025-03-24 18:01:07,152 - log
        - random_seed : 110 - 2025-03-24 18:01:07,152 - log
        - lr : 0.0002 - 2025-03-24 18:01:07,152 - log
        - weight_decay : 0.01 - 2025-03-24 18:01:07,152 - log
        - correct_bias : True - 2025-03-24 18:01:07,152 - log
        - adam_epislon : 1e-06 - 2025-03-24 18:01:07,152 - log
        - no_decay_bias : False - 2025-03-24 18:01:07,152 - log
        - adam_beta1 : 0.9 - 2025-03-24 18:01:07,152 - log
        - adam_beta2 : 0.999 - 2025-03-24 18:01:07,152 - log
        - scheduler : linear - 2025-03-24 18:01:07,152 - log
        - max_step : None - 2025-03-24 18:01:07,152 - log
        - max_epoch : 5 - 2025-03-24 18:01:07,153 - log
        - warmup_step : 500 - 2025-03-24 18:01:07,153 - log
        - i_steps : 0 - 2025-03-24 18:01:07,153 - log
        - i_lrs : 0.00025 - 2025-03-24 18:01:07,153 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 18:01:07,153 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 18:01:07,153 - log
        - train_batch_size : 4 - 2025-03-24 18:01:07,153 - log
        - valid_batch_size : 2 - 2025-03-24 18:01:07,153 - log
        - grad_acc : 2 - 2025-03-24 18:01:07,153 - log
        - clip : 0.0 - 2025-03-24 18:01:07,153 - log
        - seq_len : 256 - 2025-03-24 18:01:07,153 - log
        - model_card : gpt2.md - 2025-03-24 18:01:07,153 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin - 2025-03-24 18:01:07,153 - log
        - fp16 : False - 2025-03-24 18:01:07,153 - log
        - log_interval : 100 - 2025-03-24 18:01:07,153 - log
        - eval_interval : 2000 - 2025-03-24 18:01:07,153 - log
        - save_interval : 1000 - 2025-03-24 18:01:07,153 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 18:01:07,153 - log
        - lora_dim : 4 - 2025-03-24 18:01:07,153 - log
        - lora_alpha : 32 - 2025-03-24 18:01:07,153 - log
        - obj : clm - 2025-03-24 18:01:07,153 - log
        - lora_dropout : 0.1 - 2025-03-24 18:01:07,153 - log
        - label_smooth : 0.1 - 2025-03-24 18:01:07,153 - log
        - roll_interval : -1 - 2025-03-24 18:01:07,153 - log
        - roll_lr : 1e-05 - 2025-03-24 18:01:07,153 - log
        - roll_step : 100 - 2025-03-24 18:01:07,153 - log
        - eval_epoch : 1 - 2025-03-24 18:01:07,153 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 18:01:07,153 - log
==================================================================================================== - 2025-03-24 18:01:07,153 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 18:01:07,153 - log
==================================================================================================== - 2025-03-24 21:33:21,098 - log
        - platform : local - 2025-03-24 21:33:21,098 - log
        - local_rank : 0 - 2025-03-24 21:33:21,098 - log
        - rank : 0 - 2025-03-24 21:33:21,098 - log
        - device : cuda:0 - 2025-03-24 21:33:21,098 - log
        - world_size : 1 - 2025-03-24 21:33:21,098 - log
        - random_seed : 2025 - 2025-03-24 21:33:21,099 - log
        - lr : 0.0002 - 2025-03-24 21:33:21,099 - log
        - weight_decay : 0.01 - 2025-03-24 21:33:21,099 - log
        - correct_bias : True - 2025-03-24 21:33:21,099 - log
        - adam_epislon : 1e-06 - 2025-03-24 21:33:21,099 - log
        - no_decay_bias : False - 2025-03-24 21:33:21,099 - log
        - adam_beta1 : 0.9 - 2025-03-24 21:33:21,099 - log
        - adam_beta2 : 0.999 - 2025-03-24 21:33:21,099 - log
        - scheduler : linear - 2025-03-24 21:33:21,099 - log
        - max_step : None - 2025-03-24 21:33:21,099 - log
        - max_epoch : 5 - 2025-03-24 21:33:21,099 - log
        - warmup_step : 500 - 2025-03-24 21:33:21,099 - log
        - i_steps : 0 - 2025-03-24 21:33:21,099 - log
        - i_lrs : 0.00025 - 2025-03-24 21:33:21,099 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-24 21:33:21,099 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-24 21:33:21,099 - log
        - train_batch_size : 2 - 2025-03-24 21:33:21,099 - log
        - valid_batch_size : 1 - 2025-03-24 21:33:21,099 - log
        - grad_acc : 2 - 2025-03-24 21:33:21,099 - log
        - clip : 0.0 - 2025-03-24 21:33:21,099 - log
        - seq_len : 64 - 2025-03-24 21:33:21,099 - log
        - model_card : gpt2.sm - 2025-03-24 21:33:21,099 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-24 21:33:21,099 - log
        - fp16 : False - 2025-03-24 21:33:21,099 - log
        - log_interval : 100 - 2025-03-24 21:33:21,099 - log
        - eval_interval : 2000 - 2025-03-24 21:33:21,099 - log
        - save_interval : 1000 - 2025-03-24 21:33:21,099 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-24 21:33:21,099 - log
        - lora_dim : 4 - 2025-03-24 21:33:21,099 - log
        - lora_alpha : 32 - 2025-03-24 21:33:21,099 - log
        - obj : clm - 2025-03-24 21:33:21,099 - log
        - lora_dropout : 0.1 - 2025-03-24 21:33:21,099 - log
        - label_smooth : 0.1 - 2025-03-24 21:33:21,099 - log
        - roll_interval : -1 - 2025-03-24 21:33:21,099 - log
        - roll_lr : 1e-05 - 2025-03-24 21:33:21,099 - log
        - roll_step : 100 - 2025-03-24 21:33:21,099 - log
        - eval_epoch : 1 - 2025-03-24 21:33:21,099 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-24 21:33:21,099 - log
==================================================================================================== - 2025-03-24 21:33:21,099 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-24 21:33:21,099 - log
loading model pretrained weight. - 2025-03-24 21:33:22,220 - log
set max_step: 1050 - 2025-03-24 21:33:24,064 - log
start to train the model................ 1 - 2025-03-24 21:33:24,226 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 92.57 | loss  5.81 | avg loss  5.96 | ppl 387.42 - 2025-03-24 21:33:33,484 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 86.91 | loss  4.43 | avg loss  5.24 | ppl 187.98 - 2025-03-24 21:33:42,174 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-24 21:33:43,067 - log
start to train the model................ 2 - 2025-03-24 21:33:43,842 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 79.60 | loss  3.60 | avg loss  3.71 | ppl 40.93 - 2025-03-24 21:33:51,802 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 87.72 | loss  2.60 | avg loss  3.45 | ppl 31.44 - 2025-03-24 21:34:00,574 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-24 21:34:02,337 - log
start to train the model................ 3 - 2025-03-24 21:34:03,015 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 70.16 | loss  2.98 | avg loss  3.26 | ppl 26.10 - 2025-03-24 21:34:10,031 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 87.80 | loss  3.26 | avg loss  3.16 | ppl 23.56 - 2025-03-24 21:34:18,812 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-24 21:34:21,517 - log
start to train the model................ 4 - 2025-03-24 21:34:21,986 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 61.82 | loss  2.76 | avg loss  3.07 | ppl 21.55 - 2025-03-24 21:34:28,168 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 90.58 | loss  3.25 | avg loss  3.10 | ppl 22.09 - 2025-03-24 21:34:37,226 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-24 21:34:40,765 - log
start to train the model................ 5 - 2025-03-24 21:34:41,240 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 52.90 | loss  3.31 | avg loss  3.03 | ppl 20.65 - 2025-03-24 21:34:46,530 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 90.57 | loss  3.42 | avg loss  3.01 | ppl 20.39 - 2025-03-24 21:34:55,587 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-24 21:34:55,588 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-24 21:35:00,114 - log
---------------------------------------------------------------------------------------------------- - 2025-03-24 21:35:00,615 - log
End of training - 2025-03-24 21:35:00,615 - log
cleanup dist ... - 2025-03-24 21:35:00,791 - log
==================================================================================================== - 2025-03-25 17:10:20,104 - log
        - platform : local - 2025-03-25 17:10:20,105 - log
        - local_rank : 0 - 2025-03-25 17:10:20,105 - log
        - rank : 0 - 2025-03-25 17:10:20,105 - log
        - device : cuda:0 - 2025-03-25 17:10:20,105 - log
        - world_size : 1 - 2025-03-25 17:10:20,105 - log
        - random_seed : 2025 - 2025-03-25 17:10:20,105 - log
        - lr : 0.0002 - 2025-03-25 17:10:20,105 - log
        - weight_decay : 0.01 - 2025-03-25 17:10:20,105 - log
        - correct_bias : True - 2025-03-25 17:10:20,105 - log
        - adam_epislon : 1e-06 - 2025-03-25 17:10:20,105 - log
        - no_decay_bias : False - 2025-03-25 17:10:20,105 - log
        - adam_beta1 : 0.9 - 2025-03-25 17:10:20,105 - log
        - adam_beta2 : 0.999 - 2025-03-25 17:10:20,105 - log
        - scheduler : linear - 2025-03-25 17:10:20,105 - log
        - max_step : None - 2025-03-25 17:10:20,105 - log
        - max_epoch : 5 - 2025-03-25 17:10:20,105 - log
        - warmup_step : 500 - 2025-03-25 17:10:20,105 - log
        - i_steps : 0 - 2025-03-25 17:10:20,105 - log
        - i_lrs : 0.00025 - 2025-03-25 17:10:20,105 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 17:10:20,105 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 17:10:20,105 - log
        - train_batch_size : 2 - 2025-03-25 17:10:20,105 - log
        - valid_batch_size : 1 - 2025-03-25 17:10:20,105 - log
        - grad_acc : 2 - 2025-03-25 17:10:20,105 - log
        - clip : 0.0 - 2025-03-25 17:10:20,105 - log
        - seq_len : 64 - 2025-03-25 17:10:20,105 - log
        - model_card : gpt2.sm - 2025-03-25 17:10:20,105 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 17:10:20,105 - log
        - fp16 : False - 2025-03-25 17:10:20,105 - log
        - log_interval : 100 - 2025-03-25 17:10:20,105 - log
        - eval_interval : 2000 - 2025-03-25 17:10:20,105 - log
        - save_interval : 1000 - 2025-03-25 17:10:20,105 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 17:10:20,105 - log
        - lora_dim : 4 - 2025-03-25 17:10:20,105 - log
        - lora_alpha : 32 - 2025-03-25 17:10:20,105 - log
        - obj : clm - 2025-03-25 17:10:20,105 - log
        - lora_dropout : 0.1 - 2025-03-25 17:10:20,105 - log
        - label_smooth : 0.1 - 2025-03-25 17:10:20,105 - log
        - roll_interval : -1 - 2025-03-25 17:10:20,105 - log
        - roll_lr : 1e-05 - 2025-03-25 17:10:20,105 - log
        - roll_step : 100 - 2025-03-25 17:10:20,105 - log
        - eval_epoch : 1 - 2025-03-25 17:10:20,105 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 17:10:20,105 - log
==================================================================================================== - 2025-03-25 17:10:20,105 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 17:10:20,105 - log
loading model pretrained weight. - 2025-03-25 17:10:21,178 - log
set max_step: 1050 - 2025-03-25 17:10:23,078 - log
start to train the model................ 1 - 2025-03-25 17:10:23,243 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 30.93 | loss  5.81 | avg loss  5.96 | ppl 387.42 - 2025-03-25 17:10:26,336 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 24.60 | loss  4.43 | avg loss  5.24 | ppl 187.98 - 2025-03-25 17:10:28,796 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 17:10:29,045 - log
start to train the model................ 2 - 2025-03-25 17:10:29,518 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 22.03 | loss  3.60 | avg loss  3.71 | ppl 40.93 - 2025-03-25 17:10:31,722 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 24.74 | loss  2.60 | avg loss  3.45 | ppl 31.44 - 2025-03-25 17:10:34,196 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 17:10:34,687 - log
start to train the model................ 3 - 2025-03-25 17:10:35,185 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 19.57 | loss  2.98 | avg loss  3.26 | ppl 26.10 - 2025-03-25 17:10:37,142 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 24.64 | loss  3.26 | avg loss  3.16 | ppl 23.56 - 2025-03-25 17:10:39,606 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 17:10:40,347 - log
start to train the model................ 4 - 2025-03-25 17:10:40,857 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 17.00 | loss  2.76 | avg loss  3.07 | ppl 21.55 - 2025-03-25 17:10:42,557 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 24.78 | loss  3.25 | avg loss  3.10 | ppl 22.09 - 2025-03-25 17:10:45,035 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 17:10:46,028 - log
start to train the model................ 5 - 2025-03-25 17:10:46,546 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 14.79 | loss  3.31 | avg loss  3.03 | ppl 20.65 - 2025-03-25 17:10:48,025 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 24.94 | loss  3.42 | avg loss  3.01 | ppl 20.39 - 2025-03-25 17:10:50,519 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 17:10:50,519 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 17:10:51,770 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 17:10:52,300 - log
End of training - 2025-03-25 17:10:52,301 - log
cleanup dist ... - 2025-03-25 17:10:52,520 - log
==================================================================================================== - 2025-03-25 17:19:05,040 - log
        - platform : local - 2025-03-25 17:19:05,040 - log
        - local_rank : 0 - 2025-03-25 17:19:05,040 - log
        - rank : 0 - 2025-03-25 17:19:05,040 - log
        - device : cuda:0 - 2025-03-25 17:19:05,040 - log
        - world_size : 1 - 2025-03-25 17:19:05,040 - log
        - random_seed : 2025 - 2025-03-25 17:19:05,040 - log
        - lr : 0.0002 - 2025-03-25 17:19:05,040 - log
        - weight_decay : 0.01 - 2025-03-25 17:19:05,040 - log
        - correct_bias : True - 2025-03-25 17:19:05,040 - log
        - adam_epislon : 1e-06 - 2025-03-25 17:19:05,040 - log
        - no_decay_bias : False - 2025-03-25 17:19:05,040 - log
        - adam_beta1 : 0.9 - 2025-03-25 17:19:05,040 - log
        - adam_beta2 : 0.999 - 2025-03-25 17:19:05,040 - log
        - scheduler : linear - 2025-03-25 17:19:05,040 - log
        - max_step : None - 2025-03-25 17:19:05,040 - log
        - max_epoch : 5 - 2025-03-25 17:19:05,040 - log
        - warmup_step : 500 - 2025-03-25 17:19:05,040 - log
        - i_steps : 0 - 2025-03-25 17:19:05,040 - log
        - i_lrs : 0.00025 - 2025-03-25 17:19:05,040 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 17:19:05,040 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 17:19:05,040 - log
        - train_batch_size : 2 - 2025-03-25 17:19:05,040 - log
        - valid_batch_size : 1 - 2025-03-25 17:19:05,040 - log
        - grad_acc : 2 - 2025-03-25 17:19:05,040 - log
        - clip : 0.0 - 2025-03-25 17:19:05,040 - log
        - seq_len : 64 - 2025-03-25 17:19:05,040 - log
        - model_card : gpt2.sm - 2025-03-25 17:19:05,040 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 17:19:05,040 - log
        - fp16 : False - 2025-03-25 17:19:05,040 - log
        - log_interval : 100 - 2025-03-25 17:19:05,040 - log
        - eval_interval : 2000 - 2025-03-25 17:19:05,040 - log
        - save_interval : 1000 - 2025-03-25 17:19:05,040 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 17:19:05,040 - log
        - lora_dim : 4 - 2025-03-25 17:19:05,040 - log
        - lora_alpha : 32 - 2025-03-25 17:19:05,040 - log
        - obj : clm - 2025-03-25 17:19:05,040 - log
        - lora_dropout : 0.1 - 2025-03-25 17:19:05,041 - log
        - label_smooth : 0.1 - 2025-03-25 17:19:05,041 - log
        - roll_interval : -1 - 2025-03-25 17:19:05,041 - log
        - roll_lr : 1e-05 - 2025-03-25 17:19:05,041 - log
        - roll_step : 100 - 2025-03-25 17:19:05,041 - log
        - eval_epoch : 1 - 2025-03-25 17:19:05,041 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 17:19:05,041 - log
==================================================================================================== - 2025-03-25 17:19:05,041 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 17:19:05,041 - log
loading model pretrained weight. - 2025-03-25 17:19:06,080 - log
set max_step: 1050 - 2025-03-25 17:19:07,977 - log
start to train the model................ 1 - 2025-03-25 17:19:08,089 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 17:19:09,444 - log
Exiting from training early - 2025-03-25 17:19:09,444 - log
cleanup dist ... - 2025-03-25 17:19:09,632 - log
==================================================================================================== - 2025-03-25 17:47:25,325 - log
        - platform : local - 2025-03-25 17:47:25,330 - log
        - local_rank : 0 - 2025-03-25 17:47:25,330 - log
        - rank : 0 - 2025-03-25 17:47:25,330 - log
        - device : cuda:0 - 2025-03-25 17:47:25,330 - log
        - world_size : 1 - 2025-03-25 17:47:25,330 - log
        - random_seed : 2025 - 2025-03-25 17:47:25,330 - log
        - lr : 0.0002 - 2025-03-25 17:47:25,330 - log
        - weight_decay : 0.01 - 2025-03-25 17:47:25,330 - log
        - correct_bias : True - 2025-03-25 17:47:25,330 - log
        - adam_epislon : 1e-06 - 2025-03-25 17:47:25,330 - log
        - no_decay_bias : False - 2025-03-25 17:47:25,330 - log
        - adam_beta1 : 0.9 - 2025-03-25 17:47:25,330 - log
        - adam_beta2 : 0.999 - 2025-03-25 17:47:25,330 - log
        - scheduler : linear - 2025-03-25 17:47:25,330 - log
        - max_step : None - 2025-03-25 17:47:25,330 - log
        - max_epoch : 5 - 2025-03-25 17:47:25,330 - log
        - warmup_step : 500 - 2025-03-25 17:47:25,330 - log
        - i_steps : 0 - 2025-03-25 17:47:25,330 - log
        - i_lrs : 0.00025 - 2025-03-25 17:47:25,330 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 17:47:25,330 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 17:47:25,330 - log
        - train_batch_size : 2 - 2025-03-25 17:47:25,330 - log
        - valid_batch_size : 1 - 2025-03-25 17:47:25,330 - log
        - grad_acc : 2 - 2025-03-25 17:47:25,330 - log
        - clip : 0.0 - 2025-03-25 17:47:25,330 - log
        - seq_len : 64 - 2025-03-25 17:47:25,330 - log
        - model_card : gpt2.sm - 2025-03-25 17:47:25,330 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 17:47:25,330 - log
        - fp16 : False - 2025-03-25 17:47:25,330 - log
        - log_interval : 100 - 2025-03-25 17:47:25,330 - log
        - eval_interval : 2000 - 2025-03-25 17:47:25,330 - log
        - save_interval : 1000 - 2025-03-25 17:47:25,330 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 17:47:25,330 - log
        - lora_dim : 4 - 2025-03-25 17:47:25,330 - log
        - lora_alpha : 32 - 2025-03-25 17:47:25,330 - log
        - obj : clm - 2025-03-25 17:47:25,330 - log
        - lora_dropout : 0.1 - 2025-03-25 17:47:25,330 - log
        - label_smooth : 0.1 - 2025-03-25 17:47:25,330 - log
        - roll_interval : -1 - 2025-03-25 17:47:25,330 - log
        - roll_lr : 1e-05 - 2025-03-25 17:47:25,330 - log
        - roll_step : 100 - 2025-03-25 17:47:25,330 - log
        - eval_epoch : 1 - 2025-03-25 17:47:25,330 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 17:47:25,330 - log
==================================================================================================== - 2025-03-25 17:47:25,330 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 17:47:25,330 - log
loading model pretrained weight. - 2025-03-25 17:47:26,400 - log
set max_step: 1050 - 2025-03-25 17:47:28,339 - log
start to train the model................ 1 - 2025-03-25 17:47:28,454 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 25.99 | loss  5.81 | avg loss  5.96 | ppl 387.42 - 2025-03-25 17:47:31,053 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 32.99 | loss  4.43 | avg loss  5.24 | ppl 187.98 - 2025-03-25 17:47:34,352 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 17:47:34,792 - log
start to train the model................ 2 - 2025-03-25 17:47:35,406 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 17:47:36,859 - log
Exiting from training early - 2025-03-25 17:47:36,868 - log
cleanup dist ... - 2025-03-25 17:47:37,073 - log
==================================================================================================== - 2025-03-25 17:53:21,300 - log
        - platform : local - 2025-03-25 17:53:21,300 - log
        - local_rank : 0 - 2025-03-25 17:53:21,300 - log
        - rank : 0 - 2025-03-25 17:53:21,300 - log
        - device : cuda:0 - 2025-03-25 17:53:21,300 - log
        - world_size : 1 - 2025-03-25 17:53:21,300 - log
        - random_seed : 2025 - 2025-03-25 17:53:21,300 - log
        - lr : 0.0002 - 2025-03-25 17:53:21,300 - log
        - weight_decay : 0.01 - 2025-03-25 17:53:21,300 - log
        - correct_bias : True - 2025-03-25 17:53:21,300 - log
        - adam_epislon : 1e-06 - 2025-03-25 17:53:21,300 - log
        - no_decay_bias : False - 2025-03-25 17:53:21,300 - log
        - adam_beta1 : 0.9 - 2025-03-25 17:53:21,300 - log
        - adam_beta2 : 0.999 - 2025-03-25 17:53:21,300 - log
        - scheduler : linear - 2025-03-25 17:53:21,300 - log
        - max_step : None - 2025-03-25 17:53:21,300 - log
        - max_epoch : 5 - 2025-03-25 17:53:21,300 - log
        - warmup_step : 500 - 2025-03-25 17:53:21,300 - log
        - i_steps : 0 - 2025-03-25 17:53:21,300 - log
        - i_lrs : 0.00025 - 2025-03-25 17:53:21,300 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 17:53:21,300 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 17:53:21,300 - log
        - train_batch_size : 2 - 2025-03-25 17:53:21,300 - log
        - valid_batch_size : 1 - 2025-03-25 17:53:21,300 - log
        - grad_acc : 2 - 2025-03-25 17:53:21,300 - log
        - clip : 0.0 - 2025-03-25 17:53:21,300 - log
        - seq_len : 64 - 2025-03-25 17:53:21,300 - log
        - model_card : gpt2.sm - 2025-03-25 17:53:21,300 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 17:53:21,300 - log
        - fp16 : False - 2025-03-25 17:53:21,300 - log
        - log_interval : 100 - 2025-03-25 17:53:21,300 - log
        - eval_interval : 2000 - 2025-03-25 17:53:21,300 - log
        - save_interval : 1000 - 2025-03-25 17:53:21,300 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 17:53:21,300 - log
        - lora_dim : 4 - 2025-03-25 17:53:21,300 - log
        - lora_alpha : 32 - 2025-03-25 17:53:21,300 - log
        - obj : clm - 2025-03-25 17:53:21,300 - log
        - lora_dropout : 0.1 - 2025-03-25 17:53:21,300 - log
        - label_smooth : 0.1 - 2025-03-25 17:53:21,300 - log
        - roll_interval : -1 - 2025-03-25 17:53:21,301 - log
        - roll_lr : 1e-05 - 2025-03-25 17:53:21,301 - log
        - roll_step : 100 - 2025-03-25 17:53:21,301 - log
        - eval_epoch : 1 - 2025-03-25 17:53:21,301 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 17:53:21,301 - log
==================================================================================================== - 2025-03-25 17:53:21,301 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 17:53:21,301 - log
loading model pretrained weight. - 2025-03-25 17:53:22,446 - log
set max_step: 1050 - 2025-03-25 17:53:24,786 - log
start to train the model................ 1 - 2025-03-25 17:53:24,910 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 17:53:26,526 - log
Exiting from training early - 2025-03-25 17:53:26,526 - log
cleanup dist ... - 2025-03-25 17:53:26,700 - log
==================================================================================================== - 2025-03-25 17:55:54,886 - log
        - platform : local - 2025-03-25 17:55:54,886 - log
        - local_rank : 0 - 2025-03-25 17:55:54,886 - log
        - rank : 0 - 2025-03-25 17:55:54,886 - log
        - device : cuda:0 - 2025-03-25 17:55:54,886 - log
        - world_size : 1 - 2025-03-25 17:55:54,886 - log
        - random_seed : 2025 - 2025-03-25 17:55:54,886 - log
        - lr : 0.0002 - 2025-03-25 17:55:54,886 - log
        - weight_decay : 0.01 - 2025-03-25 17:55:54,886 - log
        - correct_bias : True - 2025-03-25 17:55:54,886 - log
        - adam_epislon : 1e-06 - 2025-03-25 17:55:54,886 - log
        - no_decay_bias : False - 2025-03-25 17:55:54,886 - log
        - adam_beta1 : 0.9 - 2025-03-25 17:55:54,886 - log
        - adam_beta2 : 0.999 - 2025-03-25 17:55:54,886 - log
        - scheduler : linear - 2025-03-25 17:55:54,886 - log
        - max_step : None - 2025-03-25 17:55:54,886 - log
        - max_epoch : 5 - 2025-03-25 17:55:54,886 - log
        - warmup_step : 500 - 2025-03-25 17:55:54,886 - log
        - i_steps : 0 - 2025-03-25 17:55:54,886 - log
        - i_lrs : 0.00025 - 2025-03-25 17:55:54,886 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 17:55:54,886 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 17:55:54,886 - log
        - train_batch_size : 2 - 2025-03-25 17:55:54,886 - log
        - valid_batch_size : 1 - 2025-03-25 17:55:54,886 - log
        - grad_acc : 2 - 2025-03-25 17:55:54,886 - log
        - clip : 0.0 - 2025-03-25 17:55:54,886 - log
        - seq_len : 64 - 2025-03-25 17:55:54,886 - log
        - model_card : gpt2.sm - 2025-03-25 17:55:54,886 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 17:55:54,886 - log
        - fp16 : False - 2025-03-25 17:55:54,887 - log
        - log_interval : 100 - 2025-03-25 17:55:54,887 - log
        - eval_interval : 2000 - 2025-03-25 17:55:54,887 - log
        - save_interval : 1000 - 2025-03-25 17:55:54,887 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 17:55:54,887 - log
        - lora_dim : 4 - 2025-03-25 17:55:54,887 - log
        - lora_alpha : 32 - 2025-03-25 17:55:54,887 - log
        - obj : clm - 2025-03-25 17:55:54,887 - log
        - lora_dropout : 0.1 - 2025-03-25 17:55:54,887 - log
        - label_smooth : 0.1 - 2025-03-25 17:55:54,887 - log
        - roll_interval : -1 - 2025-03-25 17:55:54,887 - log
        - roll_lr : 1e-05 - 2025-03-25 17:55:54,887 - log
        - roll_step : 100 - 2025-03-25 17:55:54,887 - log
        - eval_epoch : 1 - 2025-03-25 17:55:54,887 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 17:55:54,887 - log
==================================================================================================== - 2025-03-25 17:55:54,887 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 17:55:54,887 - log
loading model pretrained weight. - 2025-03-25 17:55:55,965 - log
set max_step: 1050 - 2025-03-25 17:55:57,064 - log
start to train the model................ 1 - 2025-03-25 17:55:57,183 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 27.32 | loss  5.28 | avg loss  6.00 | ppl 403.55 - 2025-03-25 17:55:59,915 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 24.84 | loss  4.30 | avg loss  5.33 | ppl 206.28 - 2025-03-25 17:56:02,400 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 17:56:02,647 - log
start to train the model................ 2 - 2025-03-25 17:56:03,378 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 22.15 | loss  3.37 | avg loss  3.79 | ppl 44.31 - 2025-03-25 17:56:05,594 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 24.83 | loss  3.43 | avg loss  3.47 | ppl 32.21 - 2025-03-25 17:56:08,077 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 17:56:08,575 - log
start to train the model................ 3 - 2025-03-25 17:56:09,308 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 19.86 | loss  2.82 | avg loss  3.25 | ppl 25.73 - 2025-03-25 17:56:11,294 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 24.66 | loss  3.75 | avg loss  3.27 | ppl 26.34 - 2025-03-25 17:56:13,760 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 17:56:14,502 - log
start to train the model................ 4 - 2025-03-25 17:56:15,236 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 17.10 | loss  3.87 | avg loss  3.14 | ppl 23.02 - 2025-03-25 17:56:16,947 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 24.91 | loss  2.79 | avg loss  3.12 | ppl 22.70 - 2025-03-25 17:56:19,438 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 17:56:20,429 - log
start to train the model................ 5 - 2025-03-25 17:56:21,131 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 14.84 | loss  2.92 | avg loss  3.08 | ppl 21.78 - 2025-03-25 17:56:22,616 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 24.99 | loss  2.68 | avg loss  3.04 | ppl 20.97 - 2025-03-25 17:56:25,115 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 17:56:25,116 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 17:56:26,368 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 17:56:27,058 - log
End of training - 2025-03-25 17:56:27,058 - log
cleanup dist ... - 2025-03-25 17:56:27,228 - log
==================================================================================================== - 2025-03-25 18:00:13,650 - log
        - platform : local - 2025-03-25 18:00:13,651 - log
        - local_rank : 0 - 2025-03-25 18:00:13,651 - log
        - rank : 0 - 2025-03-25 18:00:13,651 - log
        - device : cuda:0 - 2025-03-25 18:00:13,651 - log
        - world_size : 1 - 2025-03-25 18:00:13,651 - log
        - random_seed : 2025 - 2025-03-25 18:00:13,651 - log
        - lr : 0.0002 - 2025-03-25 18:00:13,651 - log
        - weight_decay : 0.01 - 2025-03-25 18:00:13,651 - log
        - correct_bias : True - 2025-03-25 18:00:13,651 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:00:13,651 - log
        - no_decay_bias : False - 2025-03-25 18:00:13,651 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:00:13,651 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:00:13,651 - log
        - scheduler : linear - 2025-03-25 18:00:13,651 - log
        - max_step : None - 2025-03-25 18:00:13,651 - log
        - max_epoch : 5 - 2025-03-25 18:00:13,651 - log
        - warmup_step : 500 - 2025-03-25 18:00:13,651 - log
        - i_steps : 0 - 2025-03-25 18:00:13,651 - log
        - i_lrs : 0.00025 - 2025-03-25 18:00:13,651 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:00:13,651 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:00:13,651 - log
        - train_batch_size : 2 - 2025-03-25 18:00:13,651 - log
        - valid_batch_size : 1 - 2025-03-25 18:00:13,651 - log
        - grad_acc : 2 - 2025-03-25 18:00:13,651 - log
        - clip : 0.0 - 2025-03-25 18:00:13,651 - log
        - seq_len : 64 - 2025-03-25 18:00:13,651 - log
        - model_card : gpt2.sm - 2025-03-25 18:00:13,651 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:00:13,651 - log
        - fp16 : False - 2025-03-25 18:00:13,651 - log
        - log_interval : 100 - 2025-03-25 18:00:13,651 - log
        - eval_interval : 2000 - 2025-03-25 18:00:13,651 - log
        - save_interval : 1000 - 2025-03-25 18:00:13,651 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:00:13,651 - log
        - lora_dim : 4 - 2025-03-25 18:00:13,651 - log
        - lora_alpha : 32 - 2025-03-25 18:00:13,651 - log
        - obj : clm - 2025-03-25 18:00:13,651 - log
        - lora_dropout : 0.1 - 2025-03-25 18:00:13,651 - log
        - label_smooth : 0.1 - 2025-03-25 18:00:13,651 - log
        - roll_interval : -1 - 2025-03-25 18:00:13,651 - log
        - roll_lr : 1e-05 - 2025-03-25 18:00:13,651 - log
        - roll_step : 100 - 2025-03-25 18:00:13,651 - log
        - eval_epoch : 1 - 2025-03-25 18:00:13,651 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:00:13,651 - log
==================================================================================================== - 2025-03-25 18:00:13,651 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:00:13,651 - log
loading model pretrained weight. - 2025-03-25 18:00:14,728 - log
set max_step: 1050 - 2025-03-25 18:00:15,833 - log
start to train the model................ 1 - 2025-03-25 18:00:15,949 - log
==================================================================================================== - 2025-03-25 18:00:41,668 - log
        - platform : local - 2025-03-25 18:00:41,669 - log
        - local_rank : 0 - 2025-03-25 18:00:41,669 - log
        - rank : 0 - 2025-03-25 18:00:41,669 - log
        - device : cuda:0 - 2025-03-25 18:00:41,669 - log
        - world_size : 1 - 2025-03-25 18:00:41,669 - log
        - random_seed : 2025 - 2025-03-25 18:00:41,669 - log
        - lr : 0.0002 - 2025-03-25 18:00:41,669 - log
        - weight_decay : 0.01 - 2025-03-25 18:00:41,669 - log
        - correct_bias : True - 2025-03-25 18:00:41,669 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:00:41,669 - log
        - no_decay_bias : False - 2025-03-25 18:00:41,669 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:00:41,669 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:00:41,669 - log
        - scheduler : linear - 2025-03-25 18:00:41,669 - log
        - max_step : None - 2025-03-25 18:00:41,669 - log
        - max_epoch : 5 - 2025-03-25 18:00:41,669 - log
        - warmup_step : 500 - 2025-03-25 18:00:41,669 - log
        - i_steps : 0 - 2025-03-25 18:00:41,669 - log
        - i_lrs : 0.00025 - 2025-03-25 18:00:41,669 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:00:41,669 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:00:41,669 - log
        - train_batch_size : 2 - 2025-03-25 18:00:41,669 - log
        - valid_batch_size : 1 - 2025-03-25 18:00:41,669 - log
        - grad_acc : 2 - 2025-03-25 18:00:41,669 - log
        - clip : 0.0 - 2025-03-25 18:00:41,669 - log
        - seq_len : 64 - 2025-03-25 18:00:41,669 - log
        - model_card : gpt2.sm - 2025-03-25 18:00:41,669 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:00:41,669 - log
        - fp16 : False - 2025-03-25 18:00:41,669 - log
        - log_interval : 100 - 2025-03-25 18:00:41,669 - log
        - eval_interval : 2000 - 2025-03-25 18:00:41,669 - log
        - save_interval : 1000 - 2025-03-25 18:00:41,669 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:00:41,669 - log
        - lora_dim : 4 - 2025-03-25 18:00:41,669 - log
        - lora_alpha : 32 - 2025-03-25 18:00:41,670 - log
        - obj : clm - 2025-03-25 18:00:41,670 - log
        - lora_dropout : 0.1 - 2025-03-25 18:00:41,670 - log
        - label_smooth : 0.1 - 2025-03-25 18:00:41,670 - log
        - roll_interval : -1 - 2025-03-25 18:00:41,670 - log
        - roll_lr : 1e-05 - 2025-03-25 18:00:41,670 - log
        - roll_step : 100 - 2025-03-25 18:00:41,670 - log
        - eval_epoch : 1 - 2025-03-25 18:00:41,670 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:00:41,670 - log
==================================================================================================== - 2025-03-25 18:00:41,670 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:00:41,670 - log
loading model pretrained weight. - 2025-03-25 18:00:42,711 - log
set max_step: 1050 - 2025-03-25 18:00:43,755 - log
start to train the model................ 1 - 2025-03-25 18:00:43,876 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:00:45,845 - log
Exiting from training early - 2025-03-25 18:00:45,845 - log
cleanup dist ... - 2025-03-25 18:00:46,012 - log
==================================================================================================== - 2025-03-25 18:09:18,859 - log
        - platform : local - 2025-03-25 18:09:18,859 - log
        - local_rank : 0 - 2025-03-25 18:09:18,859 - log
        - rank : 0 - 2025-03-25 18:09:18,859 - log
        - device : cuda:0 - 2025-03-25 18:09:18,859 - log
        - world_size : 1 - 2025-03-25 18:09:18,859 - log
        - random_seed : 2025 - 2025-03-25 18:09:18,859 - log
        - lr : 0.0002 - 2025-03-25 18:09:18,859 - log
        - weight_decay : 0.01 - 2025-03-25 18:09:18,859 - log
        - correct_bias : True - 2025-03-25 18:09:18,859 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:09:18,859 - log
        - no_decay_bias : False - 2025-03-25 18:09:18,859 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:09:18,859 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:09:18,859 - log
        - scheduler : linear - 2025-03-25 18:09:18,859 - log
        - max_step : None - 2025-03-25 18:09:18,859 - log
        - max_epoch : 5 - 2025-03-25 18:09:18,859 - log
        - warmup_step : 500 - 2025-03-25 18:09:18,859 - log
        - i_steps : 0 - 2025-03-25 18:09:18,859 - log
        - i_lrs : 0.00025 - 2025-03-25 18:09:18,860 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:09:18,860 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:09:18,860 - log
        - train_batch_size : 2 - 2025-03-25 18:09:18,860 - log
        - valid_batch_size : 1 - 2025-03-25 18:09:18,860 - log
        - grad_acc : 2 - 2025-03-25 18:09:18,860 - log
        - clip : 0.0 - 2025-03-25 18:09:18,860 - log
        - seq_len : 64 - 2025-03-25 18:09:18,860 - log
        - model_card : gpt2.sm - 2025-03-25 18:09:18,860 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:09:18,860 - log
        - fp16 : False - 2025-03-25 18:09:18,860 - log
        - log_interval : 100 - 2025-03-25 18:09:18,860 - log
        - eval_interval : 2000 - 2025-03-25 18:09:18,860 - log
        - save_interval : 1000 - 2025-03-25 18:09:18,860 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:09:18,860 - log
        - lora_dim : 4 - 2025-03-25 18:09:18,860 - log
        - lora_alpha : 32 - 2025-03-25 18:09:18,860 - log
        - obj : clm - 2025-03-25 18:09:18,860 - log
        - lora_dropout : 0.1 - 2025-03-25 18:09:18,860 - log
        - label_smooth : 0.1 - 2025-03-25 18:09:18,860 - log
        - roll_interval : -1 - 2025-03-25 18:09:18,860 - log
        - roll_lr : 1e-05 - 2025-03-25 18:09:18,860 - log
        - roll_step : 100 - 2025-03-25 18:09:18,860 - log
        - eval_epoch : 1 - 2025-03-25 18:09:18,860 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:09:18,860 - log
==================================================================================================== - 2025-03-25 18:09:18,860 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:09:18,860 - log
loading model pretrained weight. - 2025-03-25 18:09:19,909 - log
set max_step: 1050 - 2025-03-25 18:09:21,031 - log
start to train the model................ 1 - 2025-03-25 18:09:21,142 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:09:22,610 - log
Exiting from training early - 2025-03-25 18:09:22,610 - log
cleanup dist ... - 2025-03-25 18:09:22,777 - log
==================================================================================================== - 2025-03-25 18:26:54,637 - log
        - platform : local - 2025-03-25 18:26:54,641 - log
        - local_rank : 0 - 2025-03-25 18:26:54,641 - log
        - rank : 0 - 2025-03-25 18:26:54,641 - log
        - device : cuda:0 - 2025-03-25 18:26:54,641 - log
        - world_size : 1 - 2025-03-25 18:26:54,641 - log
        - random_seed : 2025 - 2025-03-25 18:26:54,641 - log
        - lr : 0.0002 - 2025-03-25 18:26:54,641 - log
        - weight_decay : 0.01 - 2025-03-25 18:26:54,641 - log
        - correct_bias : True - 2025-03-25 18:26:54,641 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:26:54,641 - log
        - no_decay_bias : False - 2025-03-25 18:26:54,641 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:26:54,641 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:26:54,641 - log
        - scheduler : linear - 2025-03-25 18:26:54,641 - log
        - max_step : None - 2025-03-25 18:26:54,641 - log
        - max_epoch : 5 - 2025-03-25 18:26:54,641 - log
        - warmup_step : 500 - 2025-03-25 18:26:54,641 - log
        - i_steps : 0 - 2025-03-25 18:26:54,641 - log
        - i_lrs : 0.00025 - 2025-03-25 18:26:54,641 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:26:54,641 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:26:54,641 - log
        - train_batch_size : 2 - 2025-03-25 18:26:54,641 - log
        - valid_batch_size : 1 - 2025-03-25 18:26:54,641 - log
        - grad_acc : 2 - 2025-03-25 18:26:54,641 - log
        - clip : 0.0 - 2025-03-25 18:26:54,641 - log
        - seq_len : 64 - 2025-03-25 18:26:54,642 - log
        - model_card : gpt2.sm - 2025-03-25 18:26:54,642 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:26:54,642 - log
        - fp16 : False - 2025-03-25 18:26:54,642 - log
        - log_interval : 100 - 2025-03-25 18:26:54,642 - log
        - eval_interval : 2000 - 2025-03-25 18:26:54,642 - log
        - save_interval : 1000 - 2025-03-25 18:26:54,642 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:26:54,642 - log
        - lora_dim : 4 - 2025-03-25 18:26:54,642 - log
        - lora_alpha : 32 - 2025-03-25 18:26:54,642 - log
        - obj : clm - 2025-03-25 18:26:54,642 - log
        - lora_dropout : 0.1 - 2025-03-25 18:26:54,642 - log
        - label_smooth : 0.1 - 2025-03-25 18:26:54,642 - log
        - roll_interval : -1 - 2025-03-25 18:26:54,642 - log
        - roll_lr : 1e-05 - 2025-03-25 18:26:54,642 - log
        - roll_step : 100 - 2025-03-25 18:26:54,642 - log
        - eval_epoch : 1 - 2025-03-25 18:26:54,642 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:26:54,642 - log
==================================================================================================== - 2025-03-25 18:26:54,642 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:26:54,642 - log
loading model pretrained weight. - 2025-03-25 18:26:55,716 - log
set max_step: 1050 - 2025-03-25 18:26:57,097 - log
start to train the model................ 1 - 2025-03-25 18:26:57,218 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:26:59,647 - log
Exiting from training early - 2025-03-25 18:26:59,647 - log
cleanup dist ... - 2025-03-25 18:26:59,849 - log
==================================================================================================== - 2025-03-25 18:28:58,053 - log
        - platform : local - 2025-03-25 18:28:58,053 - log
        - local_rank : 0 - 2025-03-25 18:28:58,053 - log
        - rank : 0 - 2025-03-25 18:28:58,053 - log
        - device : cuda:0 - 2025-03-25 18:28:58,053 - log
        - world_size : 1 - 2025-03-25 18:28:58,053 - log
        - random_seed : 2025 - 2025-03-25 18:28:58,053 - log
        - lr : 0.0002 - 2025-03-25 18:28:58,053 - log
        - weight_decay : 0.01 - 2025-03-25 18:28:58,053 - log
        - correct_bias : True - 2025-03-25 18:28:58,053 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:28:58,053 - log
        - no_decay_bias : False - 2025-03-25 18:28:58,053 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:28:58,053 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:28:58,053 - log
        - scheduler : linear - 2025-03-25 18:28:58,053 - log
        - max_step : None - 2025-03-25 18:28:58,053 - log
        - max_epoch : 5 - 2025-03-25 18:28:58,053 - log
        - warmup_step : 500 - 2025-03-25 18:28:58,053 - log
        - i_steps : 0 - 2025-03-25 18:28:58,053 - log
        - i_lrs : 0.00025 - 2025-03-25 18:28:58,053 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:28:58,053 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:28:58,053 - log
        - train_batch_size : 2 - 2025-03-25 18:28:58,053 - log
        - valid_batch_size : 1 - 2025-03-25 18:28:58,053 - log
        - grad_acc : 2 - 2025-03-25 18:28:58,053 - log
        - clip : 0.0 - 2025-03-25 18:28:58,053 - log
        - seq_len : 64 - 2025-03-25 18:28:58,053 - log
        - model_card : gpt2.sm - 2025-03-25 18:28:58,053 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:28:58,053 - log
        - fp16 : False - 2025-03-25 18:28:58,053 - log
        - log_interval : 100 - 2025-03-25 18:28:58,053 - log
        - eval_interval : 2000 - 2025-03-25 18:28:58,054 - log
        - save_interval : 1000 - 2025-03-25 18:28:58,054 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:28:58,054 - log
        - lora_dim : 4 - 2025-03-25 18:28:58,054 - log
        - lora_alpha : 32 - 2025-03-25 18:28:58,054 - log
        - obj : clm - 2025-03-25 18:28:58,054 - log
        - lora_dropout : 0.1 - 2025-03-25 18:28:58,054 - log
        - label_smooth : 0.1 - 2025-03-25 18:28:58,054 - log
        - roll_interval : -1 - 2025-03-25 18:28:58,054 - log
        - roll_lr : 1e-05 - 2025-03-25 18:28:58,054 - log
        - roll_step : 100 - 2025-03-25 18:28:58,054 - log
        - eval_epoch : 1 - 2025-03-25 18:28:58,054 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:28:58,054 - log
==================================================================================================== - 2025-03-25 18:28:58,054 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:28:58,054 - log
loading model pretrained weight. - 2025-03-25 18:28:59,093 - log
set max_step: 1050 - 2025-03-25 18:29:00,397 - log
start to train the model................ 1 - 2025-03-25 18:29:00,509 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:29:02,259 - log
Exiting from training early - 2025-03-25 18:29:02,259 - log
cleanup dist ... - 2025-03-25 18:29:02,453 - log
==================================================================================================== - 2025-03-25 18:29:44,865 - log
        - platform : local - 2025-03-25 18:29:44,865 - log
        - local_rank : 0 - 2025-03-25 18:29:44,865 - log
        - rank : 0 - 2025-03-25 18:29:44,865 - log
        - device : cuda:0 - 2025-03-25 18:29:44,865 - log
        - world_size : 1 - 2025-03-25 18:29:44,865 - log
        - random_seed : 2025 - 2025-03-25 18:29:44,865 - log
        - lr : 0.0002 - 2025-03-25 18:29:44,865 - log
        - weight_decay : 0.01 - 2025-03-25 18:29:44,865 - log
        - correct_bias : True - 2025-03-25 18:29:44,865 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:29:44,865 - log
        - no_decay_bias : False - 2025-03-25 18:29:44,865 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:29:44,865 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:29:44,865 - log
        - scheduler : linear - 2025-03-25 18:29:44,865 - log
        - max_step : None - 2025-03-25 18:29:44,865 - log
        - max_epoch : 5 - 2025-03-25 18:29:44,865 - log
        - warmup_step : 500 - 2025-03-25 18:29:44,865 - log
        - i_steps : 0 - 2025-03-25 18:29:44,865 - log
        - i_lrs : 0.00025 - 2025-03-25 18:29:44,865 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:29:44,865 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:29:44,865 - log
        - train_batch_size : 2 - 2025-03-25 18:29:44,865 - log
        - valid_batch_size : 1 - 2025-03-25 18:29:44,865 - log
        - grad_acc : 2 - 2025-03-25 18:29:44,865 - log
        - clip : 0.0 - 2025-03-25 18:29:44,865 - log
        - seq_len : 64 - 2025-03-25 18:29:44,865 - log
        - model_card : gpt2.sm - 2025-03-25 18:29:44,865 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:29:44,865 - log
        - fp16 : False - 2025-03-25 18:29:44,865 - log
        - log_interval : 100 - 2025-03-25 18:29:44,865 - log
        - eval_interval : 2000 - 2025-03-25 18:29:44,865 - log
        - save_interval : 1000 - 2025-03-25 18:29:44,865 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:29:44,865 - log
        - lora_dim : 4 - 2025-03-25 18:29:44,865 - log
        - lora_alpha : 32 - 2025-03-25 18:29:44,865 - log
        - obj : clm - 2025-03-25 18:29:44,865 - log
        - lora_dropout : 0.1 - 2025-03-25 18:29:44,865 - log
        - label_smooth : 0.1 - 2025-03-25 18:29:44,865 - log
        - roll_interval : -1 - 2025-03-25 18:29:44,865 - log
        - roll_lr : 1e-05 - 2025-03-25 18:29:44,865 - log
        - roll_step : 100 - 2025-03-25 18:29:44,865 - log
        - eval_epoch : 1 - 2025-03-25 18:29:44,865 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:29:44,865 - log
==================================================================================================== - 2025-03-25 18:29:44,865 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:29:44,865 - log
loading model pretrained weight. - 2025-03-25 18:29:45,960 - log
set max_step: 1050 - 2025-03-25 18:29:47,115 - log
start to train the model................ 1 - 2025-03-25 18:29:47,235 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 26.76 | loss  5.28 | avg loss  6.00 | ppl 403.55 - 2025-03-25 18:29:49,911 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 24.51 | loss  4.30 | avg loss  5.33 | ppl 206.28 - 2025-03-25 18:29:52,362 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 18:29:52,608 - log
start to train the model................ 2 - 2025-03-25 18:29:53,315 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 21.93 | loss  3.37 | avg loss  3.79 | ppl 44.31 - 2025-03-25 18:29:55,509 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 24.66 | loss  3.43 | avg loss  3.47 | ppl 32.21 - 2025-03-25 18:29:57,975 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 18:29:58,475 - log
start to train the model................ 3 - 2025-03-25 18:29:59,201 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:30:00,408 - log
Exiting from training early - 2025-03-25 18:30:00,408 - log
cleanup dist ... - 2025-03-25 18:30:00,587 - log
==================================================================================================== - 2025-03-25 18:32:50,760 - log
        - platform : local - 2025-03-25 18:32:50,760 - log
        - local_rank : 0 - 2025-03-25 18:32:50,760 - log
        - rank : 0 - 2025-03-25 18:32:50,760 - log
        - device : cuda:0 - 2025-03-25 18:32:50,760 - log
        - world_size : 1 - 2025-03-25 18:32:50,760 - log
        - random_seed : 2025 - 2025-03-25 18:32:50,760 - log
        - lr : 0.0002 - 2025-03-25 18:32:50,760 - log
        - weight_decay : 0.01 - 2025-03-25 18:32:50,760 - log
        - correct_bias : True - 2025-03-25 18:32:50,760 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:32:50,760 - log
        - no_decay_bias : False - 2025-03-25 18:32:50,760 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:32:50,760 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:32:50,760 - log
        - scheduler : linear - 2025-03-25 18:32:50,760 - log
        - max_step : None - 2025-03-25 18:32:50,760 - log
        - max_epoch : 5 - 2025-03-25 18:32:50,760 - log
        - warmup_step : 500 - 2025-03-25 18:32:50,760 - log
        - i_steps : 0 - 2025-03-25 18:32:50,760 - log
        - i_lrs : 0.00025 - 2025-03-25 18:32:50,760 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:32:50,760 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:32:50,760 - log
        - train_batch_size : 2 - 2025-03-25 18:32:50,760 - log
        - valid_batch_size : 1 - 2025-03-25 18:32:50,760 - log
        - grad_acc : 2 - 2025-03-25 18:32:50,760 - log
        - clip : 0.0 - 2025-03-25 18:32:50,760 - log
        - seq_len : 64 - 2025-03-25 18:32:50,760 - log
        - model_card : gpt2.sm - 2025-03-25 18:32:50,760 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:32:50,760 - log
        - fp16 : False - 2025-03-25 18:32:50,760 - log
        - log_interval : 100 - 2025-03-25 18:32:50,760 - log
        - eval_interval : 2000 - 2025-03-25 18:32:50,760 - log
        - save_interval : 1000 - 2025-03-25 18:32:50,760 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:32:50,760 - log
        - lora_dim : 4 - 2025-03-25 18:32:50,760 - log
        - lora_alpha : 32 - 2025-03-25 18:32:50,760 - log
        - obj : clm - 2025-03-25 18:32:50,760 - log
        - lora_dropout : 0.1 - 2025-03-25 18:32:50,760 - log
        - label_smooth : 0.1 - 2025-03-25 18:32:50,761 - log
        - roll_interval : -1 - 2025-03-25 18:32:50,761 - log
        - roll_lr : 1e-05 - 2025-03-25 18:32:50,761 - log
        - roll_step : 100 - 2025-03-25 18:32:50,761 - log
        - eval_epoch : 1 - 2025-03-25 18:32:50,761 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:32:50,761 - log
==================================================================================================== - 2025-03-25 18:32:50,761 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:32:50,761 - log
loading model pretrained weight. - 2025-03-25 18:32:51,829 - log
set max_step: 1050 - 2025-03-25 18:32:52,950 - log
start to train the model................ 1 - 2025-03-25 18:32:53,079 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:32:54,778 - log
Exiting from training early - 2025-03-25 18:32:54,778 - log
cleanup dist ... - 2025-03-25 18:32:54,945 - log
==================================================================================================== - 2025-03-25 18:34:48,368 - log
        - platform : local - 2025-03-25 18:34:48,368 - log
        - local_rank : 0 - 2025-03-25 18:34:48,368 - log
        - rank : 0 - 2025-03-25 18:34:48,368 - log
        - device : cuda:0 - 2025-03-25 18:34:48,368 - log
        - world_size : 1 - 2025-03-25 18:34:48,368 - log
        - random_seed : 2025 - 2025-03-25 18:34:48,368 - log
        - lr : 0.0002 - 2025-03-25 18:34:48,368 - log
        - weight_decay : 0.01 - 2025-03-25 18:34:48,368 - log
        - correct_bias : True - 2025-03-25 18:34:48,368 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:34:48,368 - log
        - no_decay_bias : False - 2025-03-25 18:34:48,368 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:34:48,368 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:34:48,368 - log
        - scheduler : linear - 2025-03-25 18:34:48,368 - log
        - max_step : None - 2025-03-25 18:34:48,369 - log
        - max_epoch : 5 - 2025-03-25 18:34:48,369 - log
        - warmup_step : 500 - 2025-03-25 18:34:48,369 - log
        - i_steps : 0 - 2025-03-25 18:34:48,369 - log
        - i_lrs : 0.00025 - 2025-03-25 18:34:48,369 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:34:48,369 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:34:48,369 - log
        - train_batch_size : 2 - 2025-03-25 18:34:48,369 - log
        - valid_batch_size : 1 - 2025-03-25 18:34:48,369 - log
        - grad_acc : 2 - 2025-03-25 18:34:48,369 - log
        - clip : 0.0 - 2025-03-25 18:34:48,369 - log
        - seq_len : 64 - 2025-03-25 18:34:48,369 - log
        - model_card : gpt2.sm - 2025-03-25 18:34:48,369 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:34:48,369 - log
        - fp16 : False - 2025-03-25 18:34:48,369 - log
        - log_interval : 100 - 2025-03-25 18:34:48,369 - log
        - eval_interval : 2000 - 2025-03-25 18:34:48,369 - log
        - save_interval : 1000 - 2025-03-25 18:34:48,369 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:34:48,369 - log
        - lora_dim : 4 - 2025-03-25 18:34:48,369 - log
        - lora_alpha : 32 - 2025-03-25 18:34:48,369 - log
        - obj : clm - 2025-03-25 18:34:48,369 - log
        - lora_dropout : 0.1 - 2025-03-25 18:34:48,369 - log
        - label_smooth : 0.1 - 2025-03-25 18:34:48,369 - log
        - roll_interval : -1 - 2025-03-25 18:34:48,369 - log
        - roll_lr : 1e-05 - 2025-03-25 18:34:48,369 - log
        - roll_step : 100 - 2025-03-25 18:34:48,369 - log
        - eval_epoch : 1 - 2025-03-25 18:34:48,369 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:34:48,369 - log
==================================================================================================== - 2025-03-25 18:34:48,369 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:34:48,369 - log
loading model pretrained weight. - 2025-03-25 18:34:49,435 - log
set max_step: 1050 - 2025-03-25 18:34:50,522 - log
start to train the model................ 1 - 2025-03-25 18:34:50,634 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:34:51,136 - log
Exiting from training early - 2025-03-25 18:34:51,136 - log
cleanup dist ... - 2025-03-25 18:34:51,297 - log
==================================================================================================== - 2025-03-25 18:36:49,795 - log
        - platform : local - 2025-03-25 18:36:49,795 - log
        - local_rank : 0 - 2025-03-25 18:36:49,795 - log
        - rank : 0 - 2025-03-25 18:36:49,795 - log
        - device : cuda:0 - 2025-03-25 18:36:49,795 - log
        - world_size : 1 - 2025-03-25 18:36:49,795 - log
        - random_seed : 2025 - 2025-03-25 18:36:49,795 - log
        - lr : 0.0002 - 2025-03-25 18:36:49,795 - log
        - weight_decay : 0.01 - 2025-03-25 18:36:49,796 - log
        - correct_bias : True - 2025-03-25 18:36:49,796 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:36:49,796 - log
        - no_decay_bias : False - 2025-03-25 18:36:49,796 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:36:49,796 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:36:49,796 - log
        - scheduler : linear - 2025-03-25 18:36:49,796 - log
        - max_step : None - 2025-03-25 18:36:49,796 - log
        - max_epoch : 5 - 2025-03-25 18:36:49,796 - log
        - warmup_step : 500 - 2025-03-25 18:36:49,796 - log
        - i_steps : 0 - 2025-03-25 18:36:49,796 - log
        - i_lrs : 0.00025 - 2025-03-25 18:36:49,796 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:36:49,796 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:36:49,796 - log
        - train_batch_size : 2 - 2025-03-25 18:36:49,796 - log
        - valid_batch_size : 1 - 2025-03-25 18:36:49,796 - log
        - grad_acc : 2 - 2025-03-25 18:36:49,796 - log
        - clip : 0.0 - 2025-03-25 18:36:49,796 - log
        - seq_len : 64 - 2025-03-25 18:36:49,796 - log
        - model_card : gpt2.sm - 2025-03-25 18:36:49,796 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:36:49,796 - log
        - fp16 : False - 2025-03-25 18:36:49,796 - log
        - log_interval : 100 - 2025-03-25 18:36:49,796 - log
        - eval_interval : 2000 - 2025-03-25 18:36:49,796 - log
        - save_interval : 1000 - 2025-03-25 18:36:49,796 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:36:49,796 - log
        - lora_dim : 4 - 2025-03-25 18:36:49,796 - log
        - lora_alpha : 32 - 2025-03-25 18:36:49,796 - log
        - obj : clm - 2025-03-25 18:36:49,796 - log
        - lora_dropout : 0.1 - 2025-03-25 18:36:49,796 - log
        - label_smooth : 0.1 - 2025-03-25 18:36:49,796 - log
        - roll_interval : -1 - 2025-03-25 18:36:49,796 - log
        - roll_lr : 1e-05 - 2025-03-25 18:36:49,796 - log
        - roll_step : 100 - 2025-03-25 18:36:49,796 - log
        - eval_epoch : 1 - 2025-03-25 18:36:49,796 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:36:49,796 - log
==================================================================================================== - 2025-03-25 18:36:49,796 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:36:49,796 - log
loading model pretrained weight. - 2025-03-25 18:36:50,822 - log
set max_step: 1050 - 2025-03-25 18:36:51,873 - log
start to train the model................ 1 - 2025-03-25 18:36:51,989 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:36:52,848 - log
Exiting from training early - 2025-03-25 18:36:52,848 - log
==================================================================================================== - 2025-03-25 18:38:05,524 - log
        - platform : local - 2025-03-25 18:38:05,524 - log
        - local_rank : 0 - 2025-03-25 18:38:05,524 - log
        - rank : 0 - 2025-03-25 18:38:05,524 - log
        - device : cuda:0 - 2025-03-25 18:38:05,524 - log
        - world_size : 1 - 2025-03-25 18:38:05,524 - log
        - random_seed : 2025 - 2025-03-25 18:38:05,524 - log
        - lr : 0.0002 - 2025-03-25 18:38:05,524 - log
        - weight_decay : 0.01 - 2025-03-25 18:38:05,524 - log
        - correct_bias : True - 2025-03-25 18:38:05,524 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:38:05,524 - log
        - no_decay_bias : False - 2025-03-25 18:38:05,524 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:38:05,524 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:38:05,524 - log
        - scheduler : linear - 2025-03-25 18:38:05,524 - log
        - max_step : None - 2025-03-25 18:38:05,524 - log
        - max_epoch : 5 - 2025-03-25 18:38:05,524 - log
        - warmup_step : 500 - 2025-03-25 18:38:05,524 - log
        - i_steps : 0 - 2025-03-25 18:38:05,524 - log
        - i_lrs : 0.00025 - 2025-03-25 18:38:05,524 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:38:05,524 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:38:05,524 - log
        - train_batch_size : 2 - 2025-03-25 18:38:05,524 - log
        - valid_batch_size : 1 - 2025-03-25 18:38:05,524 - log
        - grad_acc : 2 - 2025-03-25 18:38:05,524 - log
        - clip : 0.0 - 2025-03-25 18:38:05,524 - log
        - seq_len : 64 - 2025-03-25 18:38:05,524 - log
        - model_card : gpt2.sm - 2025-03-25 18:38:05,524 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:38:05,524 - log
        - fp16 : False - 2025-03-25 18:38:05,524 - log
        - log_interval : 100 - 2025-03-25 18:38:05,524 - log
        - eval_interval : 2000 - 2025-03-25 18:38:05,524 - log
        - save_interval : 1000 - 2025-03-25 18:38:05,524 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:38:05,524 - log
        - lora_dim : 4 - 2025-03-25 18:38:05,524 - log
        - lora_alpha : 32 - 2025-03-25 18:38:05,524 - log
        - obj : clm - 2025-03-25 18:38:05,524 - log
        - lora_dropout : 0.1 - 2025-03-25 18:38:05,524 - log
        - label_smooth : 0.1 - 2025-03-25 18:38:05,524 - log
        - roll_interval : -1 - 2025-03-25 18:38:05,524 - log
        - roll_lr : 1e-05 - 2025-03-25 18:38:05,524 - log
        - roll_step : 100 - 2025-03-25 18:38:05,524 - log
        - eval_epoch : 1 - 2025-03-25 18:38:05,524 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:38:05,524 - log
==================================================================================================== - 2025-03-25 18:38:05,524 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:38:05,524 - log
loading model pretrained weight. - 2025-03-25 18:38:06,597 - log
set max_step: 1050 - 2025-03-25 18:38:07,676 - log
start to train the model................ 1 - 2025-03-25 18:38:07,787 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:38:09,173 - log
Exiting from training early - 2025-03-25 18:38:09,174 - log
==================================================================================================== - 2025-03-25 18:39:10,007 - log
        - platform : local - 2025-03-25 18:39:10,007 - log
        - local_rank : 0 - 2025-03-25 18:39:10,007 - log
        - rank : 0 - 2025-03-25 18:39:10,007 - log
        - device : cuda:0 - 2025-03-25 18:39:10,007 - log
        - world_size : 1 - 2025-03-25 18:39:10,007 - log
        - random_seed : 2025 - 2025-03-25 18:39:10,007 - log
        - lr : 0.0002 - 2025-03-25 18:39:10,007 - log
        - weight_decay : 0.01 - 2025-03-25 18:39:10,007 - log
        - correct_bias : True - 2025-03-25 18:39:10,007 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:39:10,007 - log
        - no_decay_bias : False - 2025-03-25 18:39:10,007 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:39:10,007 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:39:10,007 - log
        - scheduler : linear - 2025-03-25 18:39:10,007 - log
        - max_step : None - 2025-03-25 18:39:10,007 - log
        - max_epoch : 5 - 2025-03-25 18:39:10,007 - log
        - warmup_step : 500 - 2025-03-25 18:39:10,007 - log
        - i_steps : 0 - 2025-03-25 18:39:10,007 - log
        - i_lrs : 0.00025 - 2025-03-25 18:39:10,007 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:39:10,007 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:39:10,007 - log
        - train_batch_size : 2 - 2025-03-25 18:39:10,007 - log
        - valid_batch_size : 1 - 2025-03-25 18:39:10,007 - log
        - grad_acc : 2 - 2025-03-25 18:39:10,007 - log
        - clip : 0.0 - 2025-03-25 18:39:10,007 - log
        - seq_len : 64 - 2025-03-25 18:39:10,007 - log
        - model_card : gpt2.sm - 2025-03-25 18:39:10,007 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:39:10,007 - log
        - fp16 : False - 2025-03-25 18:39:10,007 - log
        - log_interval : 100 - 2025-03-25 18:39:10,008 - log
        - eval_interval : 2000 - 2025-03-25 18:39:10,008 - log
        - save_interval : 1000 - 2025-03-25 18:39:10,008 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:39:10,008 - log
        - lora_dim : 4 - 2025-03-25 18:39:10,008 - log
        - lora_alpha : 32 - 2025-03-25 18:39:10,008 - log
        - obj : clm - 2025-03-25 18:39:10,008 - log
        - lora_dropout : 0.1 - 2025-03-25 18:39:10,008 - log
        - label_smooth : 0.1 - 2025-03-25 18:39:10,008 - log
        - roll_interval : -1 - 2025-03-25 18:39:10,008 - log
        - roll_lr : 1e-05 - 2025-03-25 18:39:10,008 - log
        - roll_step : 100 - 2025-03-25 18:39:10,008 - log
        - eval_epoch : 1 - 2025-03-25 18:39:10,008 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:39:10,008 - log
==================================================================================================== - 2025-03-25 18:39:10,008 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:39:10,008 - log
loading model pretrained weight. - 2025-03-25 18:39:11,034 - log
set max_step: 1050 - 2025-03-25 18:39:12,097 - log
start to train the model................ 1 - 2025-03-25 18:39:12,210 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:39:12,728 - log
Exiting from training early - 2025-03-25 18:39:12,728 - log
cleanup dist ... - 2025-03-25 18:39:12,878 - log
==================================================================================================== - 2025-03-25 18:39:20,433 - log
        - platform : local - 2025-03-25 18:39:20,433 - log
        - local_rank : 0 - 2025-03-25 18:39:20,433 - log
        - rank : 0 - 2025-03-25 18:39:20,433 - log
        - device : cuda:0 - 2025-03-25 18:39:20,433 - log
        - world_size : 1 - 2025-03-25 18:39:20,433 - log
        - random_seed : 2025 - 2025-03-25 18:39:20,433 - log
        - lr : 0.0002 - 2025-03-25 18:39:20,433 - log
        - weight_decay : 0.01 - 2025-03-25 18:39:20,433 - log
        - correct_bias : True - 2025-03-25 18:39:20,433 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:39:20,433 - log
        - no_decay_bias : False - 2025-03-25 18:39:20,433 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:39:20,434 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:39:20,434 - log
        - scheduler : linear - 2025-03-25 18:39:20,434 - log
        - max_step : None - 2025-03-25 18:39:20,434 - log
        - max_epoch : 5 - 2025-03-25 18:39:20,434 - log
        - warmup_step : 500 - 2025-03-25 18:39:20,434 - log
        - i_steps : 0 - 2025-03-25 18:39:20,434 - log
        - i_lrs : 0.00025 - 2025-03-25 18:39:20,434 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:39:20,434 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:39:20,434 - log
        - train_batch_size : 2 - 2025-03-25 18:39:20,434 - log
        - valid_batch_size : 1 - 2025-03-25 18:39:20,434 - log
        - grad_acc : 2 - 2025-03-25 18:39:20,434 - log
        - clip : 0.0 - 2025-03-25 18:39:20,434 - log
        - seq_len : 64 - 2025-03-25 18:39:20,434 - log
        - model_card : gpt2.sm - 2025-03-25 18:39:20,434 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:39:20,434 - log
        - fp16 : False - 2025-03-25 18:39:20,434 - log
        - log_interval : 100 - 2025-03-25 18:39:20,434 - log
        - eval_interval : 2000 - 2025-03-25 18:39:20,434 - log
        - save_interval : 1000 - 2025-03-25 18:39:20,434 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:39:20,434 - log
        - lora_dim : 4 - 2025-03-25 18:39:20,434 - log
        - lora_alpha : 32 - 2025-03-25 18:39:20,434 - log
        - obj : clm - 2025-03-25 18:39:20,434 - log
        - lora_dropout : 0.1 - 2025-03-25 18:39:20,434 - log
        - label_smooth : 0.1 - 2025-03-25 18:39:20,434 - log
        - roll_interval : -1 - 2025-03-25 18:39:20,434 - log
        - roll_lr : 1e-05 - 2025-03-25 18:39:20,434 - log
        - roll_step : 100 - 2025-03-25 18:39:20,434 - log
        - eval_epoch : 1 - 2025-03-25 18:39:20,434 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:39:20,434 - log
==================================================================================================== - 2025-03-25 18:39:20,434 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:39:20,434 - log
loading model pretrained weight. - 2025-03-25 18:39:21,507 - log
set max_step: 1050 - 2025-03-25 18:39:22,611 - log
start to train the model................ 1 - 2025-03-25 18:39:22,722 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:39:24,153 - log
Exiting from training early - 2025-03-25 18:39:24,153 - log
==================================================================================================== - 2025-03-25 18:39:55,168 - log
        - platform : local - 2025-03-25 18:39:55,168 - log
        - local_rank : 0 - 2025-03-25 18:39:55,168 - log
        - rank : 0 - 2025-03-25 18:39:55,168 - log
        - device : cuda:0 - 2025-03-25 18:39:55,168 - log
        - world_size : 1 - 2025-03-25 18:39:55,168 - log
        - random_seed : 2025 - 2025-03-25 18:39:55,168 - log
        - lr : 0.0002 - 2025-03-25 18:39:55,168 - log
        - weight_decay : 0.01 - 2025-03-25 18:39:55,168 - log
        - correct_bias : True - 2025-03-25 18:39:55,168 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:39:55,168 - log
        - no_decay_bias : False - 2025-03-25 18:39:55,168 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:39:55,168 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:39:55,168 - log
        - scheduler : linear - 2025-03-25 18:39:55,168 - log
        - max_step : None - 2025-03-25 18:39:55,168 - log
        - max_epoch : 5 - 2025-03-25 18:39:55,168 - log
        - warmup_step : 500 - 2025-03-25 18:39:55,168 - log
        - i_steps : 0 - 2025-03-25 18:39:55,168 - log
        - i_lrs : 0.00025 - 2025-03-25 18:39:55,168 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:39:55,168 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:39:55,168 - log
        - train_batch_size : 2 - 2025-03-25 18:39:55,168 - log
        - valid_batch_size : 1 - 2025-03-25 18:39:55,168 - log
        - grad_acc : 2 - 2025-03-25 18:39:55,169 - log
        - clip : 0.0 - 2025-03-25 18:39:55,169 - log
        - seq_len : 64 - 2025-03-25 18:39:55,169 - log
        - model_card : gpt2.sm - 2025-03-25 18:39:55,169 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:39:55,169 - log
        - fp16 : False - 2025-03-25 18:39:55,169 - log
        - log_interval : 100 - 2025-03-25 18:39:55,169 - log
        - eval_interval : 2000 - 2025-03-25 18:39:55,169 - log
        - save_interval : 1000 - 2025-03-25 18:39:55,169 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:39:55,169 - log
        - lora_dim : 4 - 2025-03-25 18:39:55,169 - log
        - lora_alpha : 32 - 2025-03-25 18:39:55,169 - log
        - obj : clm - 2025-03-25 18:39:55,169 - log
        - lora_dropout : 0.1 - 2025-03-25 18:39:55,169 - log
        - label_smooth : 0.1 - 2025-03-25 18:39:55,169 - log
        - roll_interval : -1 - 2025-03-25 18:39:55,169 - log
        - roll_lr : 1e-05 - 2025-03-25 18:39:55,169 - log
        - roll_step : 100 - 2025-03-25 18:39:55,169 - log
        - eval_epoch : 1 - 2025-03-25 18:39:55,169 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:39:55,169 - log
==================================================================================================== - 2025-03-25 18:39:55,169 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:39:55,169 - log
loading model pretrained weight. - 2025-03-25 18:39:56,234 - log
set max_step: 1050 - 2025-03-25 18:39:57,334 - log
start to train the model................ 1 - 2025-03-25 18:39:57,447 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:39:59,474 - log
Exiting from training early - 2025-03-25 18:39:59,474 - log
cleanup dist ... - 2025-03-25 18:39:59,626 - log
==================================================================================================== - 2025-03-25 18:40:55,595 - log
        - platform : local - 2025-03-25 18:40:55,595 - log
        - local_rank : 0 - 2025-03-25 18:40:55,595 - log
        - rank : 0 - 2025-03-25 18:40:55,595 - log
        - device : cuda:0 - 2025-03-25 18:40:55,595 - log
        - world_size : 1 - 2025-03-25 18:40:55,596 - log
        - random_seed : 2025 - 2025-03-25 18:40:55,596 - log
        - lr : 0.0002 - 2025-03-25 18:40:55,596 - log
        - weight_decay : 0.01 - 2025-03-25 18:40:55,596 - log
        - correct_bias : True - 2025-03-25 18:40:55,596 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:40:55,596 - log
        - no_decay_bias : False - 2025-03-25 18:40:55,596 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:40:55,596 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:40:55,596 - log
        - scheduler : linear - 2025-03-25 18:40:55,596 - log
        - max_step : None - 2025-03-25 18:40:55,596 - log
        - max_epoch : 5 - 2025-03-25 18:40:55,596 - log
        - warmup_step : 500 - 2025-03-25 18:40:55,596 - log
        - i_steps : 0 - 2025-03-25 18:40:55,596 - log
        - i_lrs : 0.00025 - 2025-03-25 18:40:55,596 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:40:55,596 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:40:55,596 - log
        - train_batch_size : 2 - 2025-03-25 18:40:55,596 - log
        - valid_batch_size : 1 - 2025-03-25 18:40:55,596 - log
        - grad_acc : 2 - 2025-03-25 18:40:55,596 - log
        - clip : 0.0 - 2025-03-25 18:40:55,596 - log
        - seq_len : 64 - 2025-03-25 18:40:55,596 - log
        - model_card : gpt2.sm - 2025-03-25 18:40:55,596 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:40:55,596 - log
        - fp16 : False - 2025-03-25 18:40:55,596 - log
        - log_interval : 100 - 2025-03-25 18:40:55,596 - log
        - eval_interval : 2000 - 2025-03-25 18:40:55,596 - log
        - save_interval : 1000 - 2025-03-25 18:40:55,596 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:40:55,596 - log
        - lora_dim : 4 - 2025-03-25 18:40:55,596 - log
        - lora_alpha : 32 - 2025-03-25 18:40:55,596 - log
        - obj : clm - 2025-03-25 18:40:55,596 - log
        - lora_dropout : 0.1 - 2025-03-25 18:40:55,596 - log
        - label_smooth : 0.1 - 2025-03-25 18:40:55,596 - log
        - roll_interval : -1 - 2025-03-25 18:40:55,596 - log
        - roll_lr : 1e-05 - 2025-03-25 18:40:55,596 - log
        - roll_step : 100 - 2025-03-25 18:40:55,596 - log
        - eval_epoch : 1 - 2025-03-25 18:40:55,596 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:40:55,596 - log
==================================================================================================== - 2025-03-25 18:40:55,596 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:40:55,596 - log
loading model pretrained weight. - 2025-03-25 18:40:56,675 - log
set max_step: 1050 - 2025-03-25 18:40:57,782 - log
start to train the model................ 1 - 2025-03-25 18:40:57,894 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:40:58,585 - log
Exiting from training early - 2025-03-25 18:40:58,585 - log
cleanup dist ... - 2025-03-25 18:40:58,749 - log
==================================================================================================== - 2025-03-25 18:42:20,196 - log
        - platform : local - 2025-03-25 18:42:20,196 - log
        - local_rank : 0 - 2025-03-25 18:42:20,196 - log
        - rank : 0 - 2025-03-25 18:42:20,196 - log
        - device : cuda:0 - 2025-03-25 18:42:20,196 - log
        - world_size : 1 - 2025-03-25 18:42:20,196 - log
        - random_seed : 2025 - 2025-03-25 18:42:20,196 - log
        - lr : 0.0002 - 2025-03-25 18:42:20,196 - log
        - weight_decay : 0.01 - 2025-03-25 18:42:20,196 - log
        - correct_bias : True - 2025-03-25 18:42:20,196 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:42:20,196 - log
        - no_decay_bias : False - 2025-03-25 18:42:20,196 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:42:20,196 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:42:20,197 - log
        - scheduler : linear - 2025-03-25 18:42:20,197 - log
        - max_step : None - 2025-03-25 18:42:20,197 - log
        - max_epoch : 5 - 2025-03-25 18:42:20,197 - log
        - warmup_step : 500 - 2025-03-25 18:42:20,197 - log
        - i_steps : 0 - 2025-03-25 18:42:20,197 - log
        - i_lrs : 0.00025 - 2025-03-25 18:42:20,197 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:42:20,197 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:42:20,197 - log
        - train_batch_size : 2 - 2025-03-25 18:42:20,197 - log
        - valid_batch_size : 1 - 2025-03-25 18:42:20,197 - log
        - grad_acc : 2 - 2025-03-25 18:42:20,197 - log
        - clip : 0.0 - 2025-03-25 18:42:20,197 - log
        - seq_len : 64 - 2025-03-25 18:42:20,197 - log
        - model_card : gpt2.sm - 2025-03-25 18:42:20,197 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:42:20,197 - log
        - fp16 : False - 2025-03-25 18:42:20,197 - log
        - log_interval : 100 - 2025-03-25 18:42:20,197 - log
        - eval_interval : 2000 - 2025-03-25 18:42:20,197 - log
        - save_interval : 1000 - 2025-03-25 18:42:20,197 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:42:20,197 - log
        - lora_dim : 4 - 2025-03-25 18:42:20,197 - log
        - lora_alpha : 32 - 2025-03-25 18:42:20,197 - log
        - obj : clm - 2025-03-25 18:42:20,197 - log
        - lora_dropout : 0.1 - 2025-03-25 18:42:20,197 - log
        - label_smooth : 0.1 - 2025-03-25 18:42:20,197 - log
        - roll_interval : -1 - 2025-03-25 18:42:20,197 - log
        - roll_lr : 1e-05 - 2025-03-25 18:42:20,197 - log
        - roll_step : 100 - 2025-03-25 18:42:20,197 - log
        - eval_epoch : 1 - 2025-03-25 18:42:20,197 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:42:20,197 - log
==================================================================================================== - 2025-03-25 18:42:20,197 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:42:20,197 - log
loading model pretrained weight. - 2025-03-25 18:42:21,291 - log
==================================================================================================== - 2025-03-25 18:42:24,796 - log
        - platform : local - 2025-03-25 18:42:24,796 - log
        - local_rank : 0 - 2025-03-25 18:42:24,796 - log
        - rank : 0 - 2025-03-25 18:42:24,796 - log
        - device : cuda:0 - 2025-03-25 18:42:24,796 - log
        - world_size : 1 - 2025-03-25 18:42:24,796 - log
        - random_seed : 2025 - 2025-03-25 18:42:24,796 - log
        - lr : 0.0002 - 2025-03-25 18:42:24,796 - log
        - weight_decay : 0.01 - 2025-03-25 18:42:24,796 - log
        - correct_bias : True - 2025-03-25 18:42:24,797 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:42:24,797 - log
        - no_decay_bias : False - 2025-03-25 18:42:24,797 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:42:24,797 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:42:24,797 - log
        - scheduler : linear - 2025-03-25 18:42:24,797 - log
        - max_step : None - 2025-03-25 18:42:24,797 - log
        - max_epoch : 5 - 2025-03-25 18:42:24,797 - log
        - warmup_step : 500 - 2025-03-25 18:42:24,797 - log
        - i_steps : 0 - 2025-03-25 18:42:24,797 - log
        - i_lrs : 0.00025 - 2025-03-25 18:42:24,797 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:42:24,797 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:42:24,797 - log
        - train_batch_size : 2 - 2025-03-25 18:42:24,797 - log
        - valid_batch_size : 1 - 2025-03-25 18:42:24,797 - log
        - grad_acc : 2 - 2025-03-25 18:42:24,797 - log
        - clip : 0.0 - 2025-03-25 18:42:24,797 - log
        - seq_len : 64 - 2025-03-25 18:42:24,797 - log
        - model_card : gpt2.sm - 2025-03-25 18:42:24,797 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:42:24,797 - log
        - fp16 : False - 2025-03-25 18:42:24,797 - log
        - log_interval : 100 - 2025-03-25 18:42:24,797 - log
        - eval_interval : 2000 - 2025-03-25 18:42:24,797 - log
        - save_interval : 1000 - 2025-03-25 18:42:24,797 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:42:24,797 - log
        - lora_dim : 4 - 2025-03-25 18:42:24,797 - log
        - lora_alpha : 32 - 2025-03-25 18:42:24,797 - log
        - obj : clm - 2025-03-25 18:42:24,797 - log
        - lora_dropout : 0.1 - 2025-03-25 18:42:24,797 - log
        - label_smooth : 0.1 - 2025-03-25 18:42:24,797 - log
        - roll_interval : -1 - 2025-03-25 18:42:24,797 - log
        - roll_lr : 1e-05 - 2025-03-25 18:42:24,797 - log
        - roll_step : 100 - 2025-03-25 18:42:24,797 - log
        - eval_epoch : 1 - 2025-03-25 18:42:24,797 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:42:24,797 - log
==================================================================================================== - 2025-03-25 18:42:24,797 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:42:24,797 - log
loading model pretrained weight. - 2025-03-25 18:42:26,010 - log
set max_step: 1050 - 2025-03-25 18:42:27,187 - log
start to train the model................ 1 - 2025-03-25 18:42:27,300 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:42:28,841 - log
Exiting from training early - 2025-03-25 18:42:28,841 - log
cleanup dist ... - 2025-03-25 18:42:29,007 - log
==================================================================================================== - 2025-03-25 18:50:59,512 - log
        - platform : local - 2025-03-25 18:50:59,512 - log
        - local_rank : 0 - 2025-03-25 18:50:59,512 - log
        - rank : 0 - 2025-03-25 18:50:59,512 - log
        - device : cuda:0 - 2025-03-25 18:50:59,512 - log
        - world_size : 1 - 2025-03-25 18:50:59,512 - log
        - random_seed : 2025 - 2025-03-25 18:50:59,512 - log
        - lr : 0.0002 - 2025-03-25 18:50:59,512 - log
        - weight_decay : 0.01 - 2025-03-25 18:50:59,512 - log
        - correct_bias : True - 2025-03-25 18:50:59,512 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:50:59,513 - log
        - no_decay_bias : False - 2025-03-25 18:50:59,513 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:50:59,513 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:50:59,513 - log
        - scheduler : linear - 2025-03-25 18:50:59,513 - log
        - max_step : None - 2025-03-25 18:50:59,513 - log
        - max_epoch : 5 - 2025-03-25 18:50:59,513 - log
        - warmup_step : 500 - 2025-03-25 18:50:59,513 - log
        - i_steps : 0 - 2025-03-25 18:50:59,513 - log
        - i_lrs : 0.00025 - 2025-03-25 18:50:59,513 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:50:59,513 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:50:59,513 - log
        - train_batch_size : 2 - 2025-03-25 18:50:59,513 - log
        - valid_batch_size : 1 - 2025-03-25 18:50:59,513 - log
        - grad_acc : 2 - 2025-03-25 18:50:59,513 - log
        - clip : 0.0 - 2025-03-25 18:50:59,513 - log
        - seq_len : 64 - 2025-03-25 18:50:59,513 - log
        - model_card : gpt2.sm - 2025-03-25 18:50:59,513 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:50:59,513 - log
        - fp16 : False - 2025-03-25 18:50:59,513 - log
        - log_interval : 100 - 2025-03-25 18:50:59,513 - log
        - eval_interval : 2000 - 2025-03-25 18:50:59,513 - log
        - save_interval : 1000 - 2025-03-25 18:50:59,513 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:50:59,513 - log
        - lora_dim : 4 - 2025-03-25 18:50:59,513 - log
        - lora_alpha : 32 - 2025-03-25 18:50:59,513 - log
        - obj : clm - 2025-03-25 18:50:59,513 - log
        - lora_dropout : 0.1 - 2025-03-25 18:50:59,513 - log
        - label_smooth : 0.1 - 2025-03-25 18:50:59,513 - log
        - roll_interval : -1 - 2025-03-25 18:50:59,513 - log
        - roll_lr : 1e-05 - 2025-03-25 18:50:59,513 - log
        - roll_step : 100 - 2025-03-25 18:50:59,513 - log
        - eval_epoch : 1 - 2025-03-25 18:50:59,513 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:50:59,513 - log
==================================================================================================== - 2025-03-25 18:50:59,513 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:50:59,513 - log
loading model pretrained weight. - 2025-03-25 18:51:00,580 - log
set max_step: 1050 - 2025-03-25 18:51:01,686 - log
start to train the model................ 1 - 2025-03-25 18:51:01,806 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 26.42 | loss  5.28 | avg loss  6.00 | ppl 403.55 - 2025-03-25 18:51:04,448 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 24.60 | loss  4.30 | avg loss  5.33 | ppl 206.28 - 2025-03-25 18:51:06,908 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 18:51:07,157 - log
start to train the model................ 2 - 2025-03-25 18:51:07,884 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 22.17 | loss  3.37 | avg loss  3.79 | ppl 44.31 - 2025-03-25 18:51:10,101 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 25.13 | loss  3.43 | avg loss  3.47 | ppl 32.21 - 2025-03-25 18:51:12,613 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 18:51:13,099 - log
start to train the model................ 3 - 2025-03-25 18:51:13,828 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 19.38 | loss  2.82 | avg loss  3.25 | ppl 25.73 - 2025-03-25 18:51:15,767 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 24.56 | loss  3.75 | avg loss  3.27 | ppl 26.34 - 2025-03-25 18:51:18,223 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 18:51:18,958 - log
start to train the model................ 4 - 2025-03-25 18:51:19,635 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 17.09 | loss  3.87 | avg loss  3.14 | ppl 23.02 - 2025-03-25 18:51:21,344 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:51:22,698 - log
Exiting from training early - 2025-03-25 18:51:22,698 - log
cleanup dist ... - 2025-03-25 18:51:22,879 - log
==================================================================================================== - 2025-03-25 18:52:27,066 - log
        - platform : local - 2025-03-25 18:52:27,067 - log
        - local_rank : 0 - 2025-03-25 18:52:27,067 - log
        - rank : 0 - 2025-03-25 18:52:27,067 - log
        - device : cuda:0 - 2025-03-25 18:52:27,067 - log
        - world_size : 1 - 2025-03-25 18:52:27,067 - log
        - random_seed : 2025 - 2025-03-25 18:52:27,067 - log
        - lr : 0.0002 - 2025-03-25 18:52:27,067 - log
        - weight_decay : 0.01 - 2025-03-25 18:52:27,067 - log
        - correct_bias : True - 2025-03-25 18:52:27,067 - log
        - adam_epislon : 1e-06 - 2025-03-25 18:52:27,067 - log
        - no_decay_bias : False - 2025-03-25 18:52:27,067 - log
        - adam_beta1 : 0.9 - 2025-03-25 18:52:27,067 - log
        - adam_beta2 : 0.999 - 2025-03-25 18:52:27,067 - log
        - scheduler : linear - 2025-03-25 18:52:27,067 - log
        - max_step : None - 2025-03-25 18:52:27,067 - log
        - max_epoch : 5 - 2025-03-25 18:52:27,067 - log
        - warmup_step : 500 - 2025-03-25 18:52:27,067 - log
        - i_steps : 0 - 2025-03-25 18:52:27,067 - log
        - i_lrs : 0.00025 - 2025-03-25 18:52:27,067 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 18:52:27,067 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 18:52:27,067 - log
        - train_batch_size : 2 - 2025-03-25 18:52:27,067 - log
        - valid_batch_size : 1 - 2025-03-25 18:52:27,067 - log
        - grad_acc : 2 - 2025-03-25 18:52:27,067 - log
        - clip : 0.0 - 2025-03-25 18:52:27,067 - log
        - seq_len : 64 - 2025-03-25 18:52:27,067 - log
        - model_card : gpt2.sm - 2025-03-25 18:52:27,067 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 18:52:27,067 - log
        - fp16 : False - 2025-03-25 18:52:27,067 - log
        - log_interval : 100 - 2025-03-25 18:52:27,067 - log
        - eval_interval : 2000 - 2025-03-25 18:52:27,067 - log
        - save_interval : 1000 - 2025-03-25 18:52:27,067 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 18:52:27,067 - log
        - lora_dim : 4 - 2025-03-25 18:52:27,067 - log
        - lora_alpha : 32 - 2025-03-25 18:52:27,067 - log
        - obj : clm - 2025-03-25 18:52:27,067 - log
        - lora_dropout : 0.1 - 2025-03-25 18:52:27,067 - log
        - label_smooth : 0.1 - 2025-03-25 18:52:27,067 - log
        - roll_interval : -1 - 2025-03-25 18:52:27,067 - log
        - roll_lr : 1e-05 - 2025-03-25 18:52:27,067 - log
        - roll_step : 100 - 2025-03-25 18:52:27,067 - log
        - eval_epoch : 1 - 2025-03-25 18:52:27,067 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 18:52:27,067 - log
==================================================================================================== - 2025-03-25 18:52:27,067 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 18:52:27,067 - log
loading model pretrained weight. - 2025-03-25 18:52:28,140 - log
set max_step: 1050 - 2025-03-25 18:52:29,295 - log
start to train the model................ 1 - 2025-03-25 18:52:29,422 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 18:52:30,056 - log
Exiting from training early - 2025-03-25 18:52:30,056 - log
cleanup dist ... - 2025-03-25 18:52:30,218 - log
==================================================================================================== - 2025-03-25 19:56:51,638 - log
        - platform : local - 2025-03-25 19:56:51,638 - log
        - local_rank : 0 - 2025-03-25 19:56:51,638 - log
        - rank : 0 - 2025-03-25 19:56:51,638 - log
        - device : cuda:0 - 2025-03-25 19:56:51,638 - log
        - world_size : 1 - 2025-03-25 19:56:51,638 - log
        - random_seed : 2025 - 2025-03-25 19:56:51,638 - log
        - lr : 0.0002 - 2025-03-25 19:56:51,638 - log
        - weight_decay : 0.01 - 2025-03-25 19:56:51,638 - log
        - correct_bias : True - 2025-03-25 19:56:51,638 - log
        - adam_epislon : 1e-06 - 2025-03-25 19:56:51,638 - log
        - no_decay_bias : False - 2025-03-25 19:56:51,638 - log
        - adam_beta1 : 0.9 - 2025-03-25 19:56:51,638 - log
        - adam_beta2 : 0.999 - 2025-03-25 19:56:51,638 - log
        - scheduler : linear - 2025-03-25 19:56:51,638 - log
        - max_step : None - 2025-03-25 19:56:51,638 - log
        - max_epoch : 5 - 2025-03-25 19:56:51,638 - log
        - warmup_step : 500 - 2025-03-25 19:56:51,638 - log
        - i_steps : 0 - 2025-03-25 19:56:51,638 - log
        - i_lrs : 0.00025 - 2025-03-25 19:56:51,638 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 19:56:51,638 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 19:56:51,638 - log
        - train_batch_size : 2 - 2025-03-25 19:56:51,638 - log
        - valid_batch_size : 1 - 2025-03-25 19:56:51,638 - log
        - grad_acc : 2 - 2025-03-25 19:56:51,638 - log
        - clip : 0.0 - 2025-03-25 19:56:51,639 - log
        - seq_len : 64 - 2025-03-25 19:56:51,639 - log
        - model_card : gpt2.sm - 2025-03-25 19:56:51,639 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 19:56:51,639 - log
        - fp16 : False - 2025-03-25 19:56:51,639 - log
        - log_interval : 100 - 2025-03-25 19:56:51,639 - log
        - eval_interval : 2000 - 2025-03-25 19:56:51,639 - log
        - save_interval : 1000 - 2025-03-25 19:56:51,639 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 19:56:51,639 - log
        - lora_dim : 4 - 2025-03-25 19:56:51,639 - log
        - lora_alpha : 32 - 2025-03-25 19:56:51,639 - log
        - obj : clm - 2025-03-25 19:56:51,639 - log
        - lora_dropout : 0.1 - 2025-03-25 19:56:51,639 - log
        - label_smooth : 0.1 - 2025-03-25 19:56:51,639 - log
        - roll_interval : -1 - 2025-03-25 19:56:51,639 - log
        - roll_lr : 1e-05 - 2025-03-25 19:56:51,639 - log
        - roll_step : 100 - 2025-03-25 19:56:51,639 - log
        - eval_epoch : 1 - 2025-03-25 19:56:51,639 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 19:56:51,639 - log
==================================================================================================== - 2025-03-25 19:56:51,639 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 19:56:51,639 - log
loading model pretrained weight. - 2025-03-25 19:56:52,692 - log
set max_step: 1050 - 2025-03-25 19:56:53,804 - log
start to train the model................ 1 - 2025-03-25 19:56:53,927 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 19:56:55,012 - log
Exiting from training early - 2025-03-25 19:56:55,012 - log
cleanup dist ... - 2025-03-25 19:56:55,219 - log
==================================================================================================== - 2025-03-25 20:01:11,978 - log
        - platform : local - 2025-03-25 20:01:11,978 - log
        - local_rank : 0 - 2025-03-25 20:01:11,978 - log
        - rank : 0 - 2025-03-25 20:01:11,978 - log
        - device : cuda:0 - 2025-03-25 20:01:11,978 - log
        - world_size : 1 - 2025-03-25 20:01:11,978 - log
        - random_seed : 2025 - 2025-03-25 20:01:11,978 - log
        - lr : 0.0002 - 2025-03-25 20:01:11,978 - log
        - weight_decay : 0.01 - 2025-03-25 20:01:11,978 - log
        - correct_bias : True - 2025-03-25 20:01:11,978 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:01:11,978 - log
        - no_decay_bias : False - 2025-03-25 20:01:11,978 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:01:11,978 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:01:11,978 - log
        - scheduler : linear - 2025-03-25 20:01:11,978 - log
        - max_step : None - 2025-03-25 20:01:11,978 - log
        - max_epoch : 5 - 2025-03-25 20:01:11,978 - log
        - warmup_step : 500 - 2025-03-25 20:01:11,978 - log
        - i_steps : 0 - 2025-03-25 20:01:11,978 - log
        - i_lrs : 0.00025 - 2025-03-25 20:01:11,978 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:01:11,978 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:01:11,978 - log
        - train_batch_size : 2 - 2025-03-25 20:01:11,978 - log
        - valid_batch_size : 1 - 2025-03-25 20:01:11,979 - log
        - grad_acc : 2 - 2025-03-25 20:01:11,979 - log
        - clip : 0.0 - 2025-03-25 20:01:11,979 - log
        - seq_len : 64 - 2025-03-25 20:01:11,979 - log
        - model_card : gpt2.sm - 2025-03-25 20:01:11,979 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:01:11,979 - log
        - fp16 : False - 2025-03-25 20:01:11,979 - log
        - log_interval : 100 - 2025-03-25 20:01:11,979 - log
        - eval_interval : 2000 - 2025-03-25 20:01:11,979 - log
        - save_interval : 1000 - 2025-03-25 20:01:11,979 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:01:11,979 - log
        - lora_dim : 4 - 2025-03-25 20:01:11,979 - log
        - lora_alpha : 32 - 2025-03-25 20:01:11,979 - log
        - obj : clm - 2025-03-25 20:01:11,979 - log
        - lora_dropout : 0.1 - 2025-03-25 20:01:11,979 - log
        - label_smooth : 0.1 - 2025-03-25 20:01:11,979 - log
        - roll_interval : -1 - 2025-03-25 20:01:11,979 - log
        - roll_lr : 1e-05 - 2025-03-25 20:01:11,979 - log
        - roll_step : 100 - 2025-03-25 20:01:11,979 - log
        - eval_epoch : 1 - 2025-03-25 20:01:11,979 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 20:01:11,979 - log
==================================================================================================== - 2025-03-25 20:01:11,979 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:01:11,979 - log
loading model pretrained weight. - 2025-03-25 20:01:13,002 - log
set max_step: 1050 - 2025-03-25 20:01:14,063 - log
start to train the model................ 1 - 2025-03-25 20:01:14,179 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 20:01:15,427 - log
Exiting from training early - 2025-03-25 20:01:15,427 - log
cleanup dist ... - 2025-03-25 20:01:15,591 - log
==================================================================================================== - 2025-03-25 20:03:24,526 - log
        - platform : local - 2025-03-25 20:03:24,526 - log
        - local_rank : 0 - 2025-03-25 20:03:24,526 - log
        - rank : 0 - 2025-03-25 20:03:24,526 - log
        - device : cuda:0 - 2025-03-25 20:03:24,526 - log
        - world_size : 1 - 2025-03-25 20:03:24,526 - log
        - random_seed : 2025 - 2025-03-25 20:03:24,526 - log
        - lr : 0.0002 - 2025-03-25 20:03:24,526 - log
        - weight_decay : 0.01 - 2025-03-25 20:03:24,526 - log
        - correct_bias : True - 2025-03-25 20:03:24,526 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:03:24,526 - log
        - no_decay_bias : False - 2025-03-25 20:03:24,526 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:03:24,526 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:03:24,526 - log
        - scheduler : linear - 2025-03-25 20:03:24,526 - log
        - max_step : None - 2025-03-25 20:03:24,526 - log
        - max_epoch : 5 - 2025-03-25 20:03:24,526 - log
        - warmup_step : 500 - 2025-03-25 20:03:24,526 - log
        - i_steps : 0 - 2025-03-25 20:03:24,526 - log
        - i_lrs : 0.00025 - 2025-03-25 20:03:24,526 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:03:24,526 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:03:24,526 - log
        - train_batch_size : 2 - 2025-03-25 20:03:24,526 - log
        - valid_batch_size : 1 - 2025-03-25 20:03:24,526 - log
        - grad_acc : 2 - 2025-03-25 20:03:24,526 - log
        - clip : 0.0 - 2025-03-25 20:03:24,526 - log
        - seq_len : 64 - 2025-03-25 20:03:24,526 - log
        - model_card : gpt2.sm - 2025-03-25 20:03:24,526 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:03:24,526 - log
        - fp16 : False - 2025-03-25 20:03:24,526 - log
        - log_interval : 100 - 2025-03-25 20:03:24,526 - log
        - eval_interval : 2000 - 2025-03-25 20:03:24,526 - log
        - save_interval : 1000 - 2025-03-25 20:03:24,526 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:03:24,526 - log
        - lora_dim : 4 - 2025-03-25 20:03:24,526 - log
        - lora_alpha : 32 - 2025-03-25 20:03:24,526 - log
        - obj : clm - 2025-03-25 20:03:24,526 - log
        - lora_dropout : 0.1 - 2025-03-25 20:03:24,526 - log
        - label_smooth : 0.1 - 2025-03-25 20:03:24,526 - log
        - roll_interval : -1 - 2025-03-25 20:03:24,526 - log
        - roll_lr : 1e-05 - 2025-03-25 20:03:24,526 - log
        - roll_step : 100 - 2025-03-25 20:03:24,526 - log
        - eval_epoch : 1 - 2025-03-25 20:03:24,526 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 20:03:24,527 - log
==================================================================================================== - 2025-03-25 20:03:24,527 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:03:24,527 - log
loading model pretrained weight. - 2025-03-25 20:03:25,593 - log
set max_step: 1050 - 2025-03-25 20:03:26,638 - log
start to train the model................ 1 - 2025-03-25 20:03:26,753 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 20:03:29,120 - log
Exiting from training early - 2025-03-25 20:03:29,120 - log
cleanup dist ... - 2025-03-25 20:03:29,281 - log
==================================================================================================== - 2025-03-25 20:06:13,693 - log
        - platform : local - 2025-03-25 20:06:13,693 - log
        - local_rank : 0 - 2025-03-25 20:06:13,693 - log
        - rank : 0 - 2025-03-25 20:06:13,693 - log
        - device : cuda:0 - 2025-03-25 20:06:13,693 - log
        - world_size : 1 - 2025-03-25 20:06:13,693 - log
        - random_seed : 2025 - 2025-03-25 20:06:13,693 - log
        - lr : 0.0002 - 2025-03-25 20:06:13,693 - log
        - weight_decay : 0.01 - 2025-03-25 20:06:13,693 - log
        - correct_bias : True - 2025-03-25 20:06:13,693 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:06:13,693 - log
        - no_decay_bias : False - 2025-03-25 20:06:13,693 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:06:13,693 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:06:13,693 - log
        - scheduler : linear - 2025-03-25 20:06:13,693 - log
        - max_step : None - 2025-03-25 20:06:13,693 - log
        - max_epoch : 5 - 2025-03-25 20:06:13,693 - log
        - warmup_step : 500 - 2025-03-25 20:06:13,693 - log
        - i_steps : 0 - 2025-03-25 20:06:13,693 - log
        - i_lrs : 0.00025 - 2025-03-25 20:06:13,693 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:06:13,693 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:06:13,693 - log
        - train_batch_size : 2 - 2025-03-25 20:06:13,693 - log
        - valid_batch_size : 1 - 2025-03-25 20:06:13,693 - log
        - grad_acc : 2 - 2025-03-25 20:06:13,693 - log
        - clip : 0.0 - 2025-03-25 20:06:13,693 - log
        - seq_len : 64 - 2025-03-25 20:06:13,693 - log
        - model_card : gpt2.sm - 2025-03-25 20:06:13,693 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:06:13,693 - log
        - fp16 : False - 2025-03-25 20:06:13,693 - log
        - log_interval : 100 - 2025-03-25 20:06:13,693 - log
        - eval_interval : 2000 - 2025-03-25 20:06:13,693 - log
        - save_interval : 1000 - 2025-03-25 20:06:13,693 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:06:13,693 - log
        - lora_dim : 4 - 2025-03-25 20:06:13,693 - log
        - lora_alpha : 32 - 2025-03-25 20:06:13,693 - log
        - obj : clm - 2025-03-25 20:06:13,693 - log
        - lora_dropout : 0.1 - 2025-03-25 20:06:13,693 - log
        - label_smooth : 0.1 - 2025-03-25 20:06:13,693 - log
        - roll_interval : -1 - 2025-03-25 20:06:13,693 - log
        - roll_lr : 1e-05 - 2025-03-25 20:06:13,693 - log
        - roll_step : 100 - 2025-03-25 20:06:13,693 - log
        - eval_epoch : 1 - 2025-03-25 20:06:13,693 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 20:06:13,693 - log
==================================================================================================== - 2025-03-25 20:06:13,693 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:06:13,693 - log
loading model pretrained weight. - 2025-03-25 20:06:14,762 - log
set max_step: 1050 - 2025-03-25 20:06:15,881 - log
start to train the model................ 1 - 2025-03-25 20:06:15,997 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 20:06:18,318 - log
Exiting from training early - 2025-03-25 20:06:18,318 - log
cleanup dist ... - 2025-03-25 20:06:18,478 - log
==================================================================================================== - 2025-03-25 20:29:06,608 - log
        - platform : local - 2025-03-25 20:29:06,608 - log
        - local_rank : 0 - 2025-03-25 20:29:06,608 - log
        - rank : 0 - 2025-03-25 20:29:06,608 - log
        - device : cuda:0 - 2025-03-25 20:29:06,608 - log
        - world_size : 1 - 2025-03-25 20:29:06,608 - log
        - random_seed : 2025 - 2025-03-25 20:29:06,608 - log
        - lr : 0.0002 - 2025-03-25 20:29:06,608 - log
        - weight_decay : 0.01 - 2025-03-25 20:29:06,608 - log
        - correct_bias : True - 2025-03-25 20:29:06,608 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:29:06,608 - log
        - no_decay_bias : False - 2025-03-25 20:29:06,608 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:29:06,608 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:29:06,608 - log
        - scheduler : linear - 2025-03-25 20:29:06,608 - log
        - max_step : None - 2025-03-25 20:29:06,608 - log
        - max_epoch : 5 - 2025-03-25 20:29:06,609 - log
        - warmup_step : 500 - 2025-03-25 20:29:06,609 - log
        - i_steps : 0 - 2025-03-25 20:29:06,609 - log
        - i_lrs : 0.00025 - 2025-03-25 20:29:06,609 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:29:06,609 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:29:06,609 - log
        - train_batch_size : 2 - 2025-03-25 20:29:06,609 - log
        - valid_batch_size : 1 - 2025-03-25 20:29:06,609 - log
        - grad_acc : 2 - 2025-03-25 20:29:06,609 - log
        - clip : 0.0 - 2025-03-25 20:29:06,609 - log
        - seq_len : 64 - 2025-03-25 20:29:06,609 - log
        - model_card : gpt2.sm - 2025-03-25 20:29:06,609 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:29:06,609 - log
        - fp16 : False - 2025-03-25 20:29:06,609 - log
        - log_interval : 100 - 2025-03-25 20:29:06,609 - log
        - eval_interval : 2000 - 2025-03-25 20:29:06,609 - log
        - save_interval : 1000 - 2025-03-25 20:29:06,609 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:29:06,609 - log
        - lora_dim : 4 - 2025-03-25 20:29:06,609 - log
        - lora_alpha : 32 - 2025-03-25 20:29:06,609 - log
        - obj : clm - 2025-03-25 20:29:06,609 - log
        - lora_dropout : 0.1 - 2025-03-25 20:29:06,609 - log
        - label_smooth : 0.1 - 2025-03-25 20:29:06,609 - log
        - roll_interval : -1 - 2025-03-25 20:29:06,609 - log
        - roll_lr : 1e-05 - 2025-03-25 20:29:06,609 - log
        - roll_step : 100 - 2025-03-25 20:29:06,609 - log
        - eval_epoch : 1 - 2025-03-25 20:29:06,609 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 20:29:06,609 - log
==================================================================================================== - 2025-03-25 20:29:06,609 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:29:06,609 - log
loading model pretrained weight. - 2025-03-25 20:29:07,658 - log
set max_step: 1050 - 2025-03-25 20:29:09,573 - log
start to train the model................ 1 - 2025-03-25 20:29:09,713 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 20:29:11,872 - log
Exiting from training early - 2025-03-25 20:29:11,872 - log
==================================================================================================== - 2025-03-25 20:29:19,228 - log
        - platform : local - 2025-03-25 20:29:19,228 - log
        - local_rank : 0 - 2025-03-25 20:29:19,228 - log
        - rank : 0 - 2025-03-25 20:29:19,228 - log
        - device : cuda:0 - 2025-03-25 20:29:19,228 - log
        - world_size : 1 - 2025-03-25 20:29:19,228 - log
        - random_seed : 2025 - 2025-03-25 20:29:19,228 - log
        - lr : 0.0002 - 2025-03-25 20:29:19,228 - log
        - weight_decay : 0.01 - 2025-03-25 20:29:19,228 - log
        - correct_bias : True - 2025-03-25 20:29:19,228 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:29:19,228 - log
        - no_decay_bias : False - 2025-03-25 20:29:19,228 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:29:19,228 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:29:19,228 - log
        - scheduler : linear - 2025-03-25 20:29:19,228 - log
        - max_step : None - 2025-03-25 20:29:19,228 - log
        - max_epoch : 5 - 2025-03-25 20:29:19,228 - log
        - warmup_step : 500 - 2025-03-25 20:29:19,228 - log
        - i_steps : 0 - 2025-03-25 20:29:19,228 - log
        - i_lrs : 0.00025 - 2025-03-25 20:29:19,228 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:29:19,228 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:29:19,228 - log
        - train_batch_size : 2 - 2025-03-25 20:29:19,228 - log
        - valid_batch_size : 1 - 2025-03-25 20:29:19,228 - log
        - grad_acc : 2 - 2025-03-25 20:29:19,228 - log
        - clip : 0.0 - 2025-03-25 20:29:19,228 - log
        - seq_len : 64 - 2025-03-25 20:29:19,228 - log
        - model_card : gpt2.sm - 2025-03-25 20:29:19,228 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:29:19,228 - log
        - fp16 : False - 2025-03-25 20:29:19,228 - log
        - log_interval : 100 - 2025-03-25 20:29:19,228 - log
        - eval_interval : 2000 - 2025-03-25 20:29:19,228 - log
        - save_interval : 1000 - 2025-03-25 20:29:19,228 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:29:19,228 - log
        - lora_dim : 4 - 2025-03-25 20:29:19,228 - log
        - lora_alpha : 32 - 2025-03-25 20:29:19,228 - log
        - obj : clm - 2025-03-25 20:29:19,228 - log
        - lora_dropout : 0.1 - 2025-03-25 20:29:19,228 - log
        - label_smooth : 0.1 - 2025-03-25 20:29:19,228 - log
        - roll_interval : -1 - 2025-03-25 20:29:19,228 - log
        - roll_lr : 1e-05 - 2025-03-25 20:29:19,229 - log
        - roll_step : 100 - 2025-03-25 20:29:19,229 - log
        - eval_epoch : 1 - 2025-03-25 20:29:19,229 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 20:29:19,229 - log
==================================================================================================== - 2025-03-25 20:29:19,229 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:29:19,229 - log
loading model pretrained weight. - 2025-03-25 20:29:20,406 - log
set max_step: 1050 - 2025-03-25 20:29:21,551 - log
start to train the model................ 1 - 2025-03-25 20:29:21,660 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 20:29:24,118 - log
Exiting from training early - 2025-03-25 20:29:24,118 - log
cleanup dist ... - 2025-03-25 20:29:24,275 - log
==================================================================================================== - 2025-03-25 20:33:30,662 - log
        - platform : local - 2025-03-25 20:33:30,662 - log
        - local_rank : 0 - 2025-03-25 20:33:30,662 - log
        - rank : 0 - 2025-03-25 20:33:30,662 - log
        - device : cuda:0 - 2025-03-25 20:33:30,662 - log
        - world_size : 1 - 2025-03-25 20:33:30,662 - log
        - random_seed : 2025 - 2025-03-25 20:33:30,662 - log
        - lr : 0.0002 - 2025-03-25 20:33:30,662 - log
        - weight_decay : 0.01 - 2025-03-25 20:33:30,662 - log
        - correct_bias : True - 2025-03-25 20:33:30,662 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:33:30,662 - log
        - no_decay_bias : False - 2025-03-25 20:33:30,662 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:33:30,662 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:33:30,662 - log
        - scheduler : linear - 2025-03-25 20:33:30,662 - log
        - max_step : None - 2025-03-25 20:33:30,662 - log
        - max_epoch : 5 - 2025-03-25 20:33:30,662 - log
        - warmup_step : 500 - 2025-03-25 20:33:30,662 - log
        - i_steps : 0 - 2025-03-25 20:33:30,662 - log
        - i_lrs : 0.00025 - 2025-03-25 20:33:30,662 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:33:30,662 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:33:30,662 - log
        - train_batch_size : 2 - 2025-03-25 20:33:30,662 - log
        - valid_batch_size : 1 - 2025-03-25 20:33:30,662 - log
        - grad_acc : 2 - 2025-03-25 20:33:30,662 - log
        - clip : 0.0 - 2025-03-25 20:33:30,662 - log
        - seq_len : 64 - 2025-03-25 20:33:30,662 - log
        - model_card : gpt2.sm - 2025-03-25 20:33:30,662 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:33:30,662 - log
        - fp16 : False - 2025-03-25 20:33:30,662 - log
        - log_interval : 100 - 2025-03-25 20:33:30,662 - log
        - eval_interval : 2000 - 2025-03-25 20:33:30,662 - log
        - save_interval : 1000 - 2025-03-25 20:33:30,662 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:33:30,662 - log
        - lora_dim : 4 - 2025-03-25 20:33:30,662 - log
        - lora_alpha : 32 - 2025-03-25 20:33:30,662 - log
        - obj : clm - 2025-03-25 20:33:30,663 - log
        - lora_dropout : 0.1 - 2025-03-25 20:33:30,663 - log
        - label_smooth : 0.1 - 2025-03-25 20:33:30,663 - log
        - roll_interval : -1 - 2025-03-25 20:33:30,663 - log
        - roll_lr : 1e-05 - 2025-03-25 20:33:30,663 - log
        - roll_step : 100 - 2025-03-25 20:33:30,663 - log
        - eval_epoch : 1 - 2025-03-25 20:33:30,663 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 20:33:30,663 - log
==================================================================================================== - 2025-03-25 20:33:30,663 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:33:30,663 - log
loading model pretrained weight. - 2025-03-25 20:33:31,713 - log
set max_step: 1050 - 2025-03-25 20:33:32,746 - log
start to train the model................ 1 - 2025-03-25 20:33:32,858 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 20:33:34,901 - log
Exiting from training early - 2025-03-25 20:33:34,901 - log
cleanup dist ... - 2025-03-25 20:33:35,057 - log
==================================================================================================== - 2025-03-25 20:49:07,026 - log
        - platform : local - 2025-03-25 20:49:07,026 - log
        - local_rank : 0 - 2025-03-25 20:49:07,026 - log
        - rank : 0 - 2025-03-25 20:49:07,026 - log
        - device : cuda:0 - 2025-03-25 20:49:07,026 - log
        - world_size : 1 - 2025-03-25 20:49:07,026 - log
        - random_seed : 2025 - 2025-03-25 20:49:07,026 - log
        - lr : 0.0002 - 2025-03-25 20:49:07,026 - log
        - weight_decay : 0.01 - 2025-03-25 20:49:07,027 - log
        - correct_bias : True - 2025-03-25 20:49:07,027 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:49:07,027 - log
        - no_decay_bias : False - 2025-03-25 20:49:07,027 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:49:07,027 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:49:07,027 - log
        - scheduler : linear - 2025-03-25 20:49:07,027 - log
        - max_step : None - 2025-03-25 20:49:07,027 - log
        - max_epoch : 5 - 2025-03-25 20:49:07,027 - log
        - warmup_step : 500 - 2025-03-25 20:49:07,027 - log
        - i_steps : 0 - 2025-03-25 20:49:07,027 - log
        - i_lrs : 0.00025 - 2025-03-25 20:49:07,027 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:49:07,027 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:49:07,027 - log
        - train_batch_size : 2 - 2025-03-25 20:49:07,027 - log
        - valid_batch_size : 1 - 2025-03-25 20:49:07,027 - log
        - grad_acc : 2 - 2025-03-25 20:49:07,027 - log
        - clip : 0.0 - 2025-03-25 20:49:07,027 - log
        - seq_len : 64 - 2025-03-25 20:49:07,027 - log
        - model_card : gpt2.sm - 2025-03-25 20:49:07,027 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:49:07,027 - log
        - fp16 : False - 2025-03-25 20:49:07,027 - log
        - log_interval : 100 - 2025-03-25 20:49:07,027 - log
        - eval_interval : 2000 - 2025-03-25 20:49:07,027 - log
        - save_interval : 1000 - 2025-03-25 20:49:07,027 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:49:07,027 - log
        - lora_dim : 4 - 2025-03-25 20:49:07,027 - log
        - lora_alpha : 32 - 2025-03-25 20:49:07,027 - log
        - obj : clm - 2025-03-25 20:49:07,027 - log
        - lora_dropout : 0.1 - 2025-03-25 20:49:07,027 - log
        - label_smooth : 0.1 - 2025-03-25 20:49:07,027 - log
        - roll_interval : -1 - 2025-03-25 20:49:07,027 - log
        - roll_lr : 1e-05 - 2025-03-25 20:49:07,027 - log
        - roll_step : 100 - 2025-03-25 20:49:07,027 - log
        - eval_epoch : 1 - 2025-03-25 20:49:07,027 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 20:49:07,027 - log
==================================================================================================== - 2025-03-25 20:49:07,027 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:49:07,027 - log
loading model pretrained weight. - 2025-03-25 20:49:08,057 - log
set max_step: 1050 - 2025-03-25 20:49:09,137 - log
start to train the model................ 1 - 2025-03-25 20:49:09,254 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 20:49:11,496 - log
Exiting from training early - 2025-03-25 20:49:11,496 - log
cleanup dist ... - 2025-03-25 20:49:11,706 - log
==================================================================================================== - 2025-03-25 20:50:09,267 - log
        - platform : local - 2025-03-25 20:50:09,267 - log
        - local_rank : 0 - 2025-03-25 20:50:09,267 - log
        - rank : 0 - 2025-03-25 20:50:09,267 - log
        - device : cuda:0 - 2025-03-25 20:50:09,267 - log
        - world_size : 1 - 2025-03-25 20:50:09,267 - log
        - random_seed : 2025 - 2025-03-25 20:50:09,267 - log
        - lr : 0.0002 - 2025-03-25 20:50:09,267 - log
        - weight_decay : 0.01 - 2025-03-25 20:50:09,267 - log
        - correct_bias : True - 2025-03-25 20:50:09,267 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:50:09,267 - log
        - no_decay_bias : False - 2025-03-25 20:50:09,267 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:50:09,267 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:50:09,267 - log
        - scheduler : linear - 2025-03-25 20:50:09,267 - log
        - max_step : None - 2025-03-25 20:50:09,267 - log
        - max_epoch : 5 - 2025-03-25 20:50:09,267 - log
        - warmup_step : 500 - 2025-03-25 20:50:09,267 - log
        - i_steps : 0 - 2025-03-25 20:50:09,267 - log
        - i_lrs : 0.00025 - 2025-03-25 20:50:09,267 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:50:09,267 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:50:09,267 - log
        - train_batch_size : 2 - 2025-03-25 20:50:09,267 - log
        - valid_batch_size : 1 - 2025-03-25 20:50:09,267 - log
        - grad_acc : 2 - 2025-03-25 20:50:09,267 - log
        - clip : 0.0 - 2025-03-25 20:50:09,267 - log
        - seq_len : 64 - 2025-03-25 20:50:09,267 - log
        - model_card : gpt2.sm - 2025-03-25 20:50:09,267 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:50:09,267 - log
        - fp16 : False - 2025-03-25 20:50:09,267 - log
        - log_interval : 100 - 2025-03-25 20:50:09,267 - log
        - eval_interval : 2000 - 2025-03-25 20:50:09,267 - log
        - save_interval : 1000 - 2025-03-25 20:50:09,267 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:50:09,267 - log
        - lora_dim : 4 - 2025-03-25 20:50:09,267 - log
        - lora_alpha : 32 - 2025-03-25 20:50:09,267 - log
        - obj : clm - 2025-03-25 20:50:09,267 - log
        - lora_dropout : 0.1 - 2025-03-25 20:50:09,268 - log
        - label_smooth : 0.1 - 2025-03-25 20:50:09,268 - log
        - roll_interval : -1 - 2025-03-25 20:50:09,268 - log
        - roll_lr : 1e-05 - 2025-03-25 20:50:09,268 - log
        - roll_step : 100 - 2025-03-25 20:50:09,268 - log
        - eval_epoch : 1 - 2025-03-25 20:50:09,268 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 20:50:09,268 - log
==================================================================================================== - 2025-03-25 20:50:09,268 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:50:09,268 - log
loading model pretrained weight. - 2025-03-25 20:50:10,323 - log
set max_step: 1050 - 2025-03-25 20:50:11,388 - log
start to train the model................ 1 - 2025-03-25 20:50:11,499 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 20:50:13,741 - log
Exiting from training early - 2025-03-25 20:50:13,741 - log
cleanup dist ... - 2025-03-25 20:50:13,898 - log
==================================================================================================== - 2025-03-25 20:50:28,813 - log
        - platform : local - 2025-03-25 20:50:28,813 - log
        - local_rank : 0 - 2025-03-25 20:50:28,813 - log
        - rank : 0 - 2025-03-25 20:50:28,813 - log
        - device : cuda:0 - 2025-03-25 20:50:28,813 - log
        - world_size : 1 - 2025-03-25 20:50:28,813 - log
        - random_seed : 2025 - 2025-03-25 20:50:28,813 - log
        - lr : 0.0002 - 2025-03-25 20:50:28,813 - log
        - weight_decay : 0.01 - 2025-03-25 20:50:28,813 - log
        - correct_bias : True - 2025-03-25 20:50:28,813 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:50:28,813 - log
        - no_decay_bias : False - 2025-03-25 20:50:28,813 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:50:28,813 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:50:28,813 - log
        - scheduler : linear - 2025-03-25 20:50:28,813 - log
        - max_step : None - 2025-03-25 20:50:28,813 - log
        - max_epoch : 5 - 2025-03-25 20:50:28,813 - log
        - warmup_step : 500 - 2025-03-25 20:50:28,813 - log
        - i_steps : 0 - 2025-03-25 20:50:28,813 - log
        - i_lrs : 0.00025 - 2025-03-25 20:50:28,813 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:50:28,813 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:50:28,813 - log
        - train_batch_size : 2 - 2025-03-25 20:50:28,813 - log
        - valid_batch_size : 1 - 2025-03-25 20:50:28,813 - log
        - grad_acc : 2 - 2025-03-25 20:50:28,813 - log
        - clip : 0.0 - 2025-03-25 20:50:28,813 - log
        - seq_len : 64 - 2025-03-25 20:50:28,813 - log
        - model_card : gpt2.sm - 2025-03-25 20:50:28,813 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:50:28,813 - log
        - fp16 : False - 2025-03-25 20:50:28,814 - log
        - log_interval : 100 - 2025-03-25 20:50:28,814 - log
        - eval_interval : 2000 - 2025-03-25 20:50:28,814 - log
        - save_interval : 1000 - 2025-03-25 20:50:28,814 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:50:28,814 - log
        - lora_dim : 4 - 2025-03-25 20:50:28,814 - log
        - lora_alpha : 32 - 2025-03-25 20:50:28,814 - log
        - obj : clm - 2025-03-25 20:50:28,814 - log
        - lora_dropout : 0.1 - 2025-03-25 20:50:28,814 - log
        - label_smooth : 0.1 - 2025-03-25 20:50:28,814 - log
        - roll_interval : -1 - 2025-03-25 20:50:28,814 - log
        - roll_lr : 1e-05 - 2025-03-25 20:50:28,814 - log
        - roll_step : 100 - 2025-03-25 20:50:28,814 - log
        - eval_epoch : 1 - 2025-03-25 20:50:28,814 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 20:50:28,814 - log
==================================================================================================== - 2025-03-25 20:50:28,814 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:50:28,814 - log
loading model pretrained weight. - 2025-03-25 20:50:29,874 - log
set max_step: 1050 - 2025-03-25 20:50:30,949 - log
start to train the model................ 1 - 2025-03-25 20:50:31,059 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 20:50:32,339 - log
Exiting from training early - 2025-03-25 20:50:32,339 - log
cleanup dist ... - 2025-03-25 20:50:32,496 - log
==================================================================================================== - 2025-03-25 20:50:42,791 - log
        - platform : local - 2025-03-25 20:50:42,791 - log
        - local_rank : 0 - 2025-03-25 20:50:42,791 - log
        - rank : 0 - 2025-03-25 20:50:42,791 - log
        - device : cuda:0 - 2025-03-25 20:50:42,791 - log
        - world_size : 1 - 2025-03-25 20:50:42,791 - log
        - random_seed : 2025 - 2025-03-25 20:50:42,791 - log
        - lr : 0.0002 - 2025-03-25 20:50:42,791 - log
        - weight_decay : 0.01 - 2025-03-25 20:50:42,791 - log
        - correct_bias : True - 2025-03-25 20:50:42,791 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:50:42,791 - log
        - no_decay_bias : False - 2025-03-25 20:50:42,791 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:50:42,791 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:50:42,791 - log
        - scheduler : linear - 2025-03-25 20:50:42,791 - log
        - max_step : None - 2025-03-25 20:50:42,791 - log
        - max_epoch : 5 - 2025-03-25 20:50:42,791 - log
        - warmup_step : 500 - 2025-03-25 20:50:42,791 - log
        - i_steps : 0 - 2025-03-25 20:50:42,791 - log
        - i_lrs : 0.00025 - 2025-03-25 20:50:42,791 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:50:42,791 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:50:42,791 - log
        - train_batch_size : 2 - 2025-03-25 20:50:42,791 - log
        - valid_batch_size : 1 - 2025-03-25 20:50:42,791 - log
        - grad_acc : 2 - 2025-03-25 20:50:42,791 - log
        - clip : 0.0 - 2025-03-25 20:50:42,791 - log
        - seq_len : 64 - 2025-03-25 20:50:42,791 - log
        - model_card : gpt2.sm - 2025-03-25 20:50:42,791 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:50:42,791 - log
        - fp16 : False - 2025-03-25 20:50:42,791 - log
        - log_interval : 100 - 2025-03-25 20:50:42,791 - log
        - eval_interval : 2000 - 2025-03-25 20:50:42,791 - log
        - save_interval : 1000 - 2025-03-25 20:50:42,791 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:50:42,792 - log
        - lora_dim : 4 - 2025-03-25 20:50:42,792 - log
        - lora_alpha : 32 - 2025-03-25 20:50:42,792 - log
        - obj : clm - 2025-03-25 20:50:42,792 - log
        - lora_dropout : 0.1 - 2025-03-25 20:50:42,792 - log
        - label_smooth : 0.1 - 2025-03-25 20:50:42,792 - log
        - roll_interval : -1 - 2025-03-25 20:50:42,792 - log
        - roll_lr : 1e-05 - 2025-03-25 20:50:42,792 - log
        - roll_step : 100 - 2025-03-25 20:50:42,792 - log
        - eval_epoch : 1 - 2025-03-25 20:50:42,792 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 20:50:42,792 - log
==================================================================================================== - 2025-03-25 20:50:42,792 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:50:42,792 - log
loading model pretrained weight. - 2025-03-25 20:50:43,871 - log
set max_step: 1050 - 2025-03-25 20:50:44,950 - log
start to train the model................ 1 - 2025-03-25 20:50:45,060 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 20:50:48,054 - log
Exiting from training early - 2025-03-25 20:50:48,054 - log
cleanup dist ... - 2025-03-25 20:50:48,211 - log
==================================================================================================== - 2025-03-25 20:50:52,457 - log
        - platform : local - 2025-03-25 20:50:52,457 - log
        - local_rank : 0 - 2025-03-25 20:50:52,457 - log
        - rank : 0 - 2025-03-25 20:50:52,457 - log
        - device : cuda:0 - 2025-03-25 20:50:52,457 - log
        - world_size : 1 - 2025-03-25 20:50:52,457 - log
        - random_seed : 2025 - 2025-03-25 20:50:52,457 - log
        - lr : 0.0002 - 2025-03-25 20:50:52,457 - log
        - weight_decay : 0.01 - 2025-03-25 20:50:52,457 - log
        - correct_bias : True - 2025-03-25 20:50:52,457 - log
        - adam_epislon : 1e-06 - 2025-03-25 20:50:52,457 - log
        - no_decay_bias : False - 2025-03-25 20:50:52,457 - log
        - adam_beta1 : 0.9 - 2025-03-25 20:50:52,457 - log
        - adam_beta2 : 0.999 - 2025-03-25 20:50:52,457 - log
        - scheduler : linear - 2025-03-25 20:50:52,457 - log
        - max_step : None - 2025-03-25 20:50:52,457 - log
        - max_epoch : 5 - 2025-03-25 20:50:52,457 - log
        - warmup_step : 500 - 2025-03-25 20:50:52,458 - log
        - i_steps : 0 - 2025-03-25 20:50:52,458 - log
        - i_lrs : 0.00025 - 2025-03-25 20:50:52,458 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 20:50:52,458 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 20:50:52,458 - log
        - train_batch_size : 2 - 2025-03-25 20:50:52,458 - log
        - valid_batch_size : 1 - 2025-03-25 20:50:52,458 - log
        - grad_acc : 2 - 2025-03-25 20:50:52,458 - log
        - clip : 0.0 - 2025-03-25 20:50:52,458 - log
        - seq_len : 64 - 2025-03-25 20:50:52,458 - log
        - model_card : gpt2.sm - 2025-03-25 20:50:52,458 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 20:50:52,458 - log
        - fp16 : False - 2025-03-25 20:50:52,458 - log
        - log_interval : 100 - 2025-03-25 20:50:52,458 - log
        - eval_interval : 2000 - 2025-03-25 20:50:52,458 - log
        - save_interval : 1000 - 2025-03-25 20:50:52,458 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 20:50:52,458 - log
        - lora_dim : 4 - 2025-03-25 20:50:52,458 - log
        - lora_alpha : 32 - 2025-03-25 20:50:52,458 - log
        - obj : clm - 2025-03-25 20:50:52,458 - log
        - lora_dropout : 0.1 - 2025-03-25 20:50:52,458 - log
        - label_smooth : 0.1 - 2025-03-25 20:50:52,458 - log
        - roll_interval : -1 - 2025-03-25 20:50:52,458 - log
        - roll_lr : 1e-05 - 2025-03-25 20:50:52,458 - log
        - roll_step : 100 - 2025-03-25 20:50:52,458 - log
        - eval_epoch : 1 - 2025-03-25 20:50:52,458 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 20:50:52,458 - log
==================================================================================================== - 2025-03-25 20:50:52,458 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 20:50:52,458 - log
loading model pretrained weight. - 2025-03-25 20:50:53,525 - log
set max_step: 1050 - 2025-03-25 20:50:54,605 - log
start to train the model................ 1 - 2025-03-25 20:50:54,715 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 20:50:55,157 - log
Exiting from training early - 2025-03-25 20:50:55,157 - log
==================================================================================================== - 2025-03-25 21:42:57,370 - log
        - platform : local - 2025-03-25 21:42:57,371 - log
        - local_rank : 0 - 2025-03-25 21:42:57,371 - log
        - rank : 0 - 2025-03-25 21:42:57,371 - log
        - device : cuda:0 - 2025-03-25 21:42:57,371 - log
        - world_size : 1 - 2025-03-25 21:42:57,371 - log
        - random_seed : 2025 - 2025-03-25 21:42:57,371 - log
        - lr : 0.0002 - 2025-03-25 21:42:57,371 - log
        - weight_decay : 0.01 - 2025-03-25 21:42:57,371 - log
        - correct_bias : True - 2025-03-25 21:42:57,371 - log
        - adam_epislon : 1e-06 - 2025-03-25 21:42:57,371 - log
        - no_decay_bias : False - 2025-03-25 21:42:57,371 - log
        - adam_beta1 : 0.9 - 2025-03-25 21:42:57,371 - log
        - adam_beta2 : 0.999 - 2025-03-25 21:42:57,371 - log
        - scheduler : linear - 2025-03-25 21:42:57,371 - log
        - max_step : None - 2025-03-25 21:42:57,371 - log
        - max_epoch : 5 - 2025-03-25 21:42:57,371 - log
        - warmup_step : 500 - 2025-03-25 21:42:57,371 - log
        - i_steps : 0 - 2025-03-25 21:42:57,371 - log
        - i_lrs : 0.00025 - 2025-03-25 21:42:57,371 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 21:42:57,371 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 21:42:57,371 - log
        - train_batch_size : 2 - 2025-03-25 21:42:57,371 - log
        - valid_batch_size : 1 - 2025-03-25 21:42:57,371 - log
        - grad_acc : 2 - 2025-03-25 21:42:57,371 - log
        - clip : 0.0 - 2025-03-25 21:42:57,371 - log
        - seq_len : 64 - 2025-03-25 21:42:57,371 - log
        - model_card : gpt2.sm - 2025-03-25 21:42:57,371 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 21:42:57,371 - log
        - fp16 : False - 2025-03-25 21:42:57,371 - log
        - log_interval : 100 - 2025-03-25 21:42:57,371 - log
        - eval_interval : 2000 - 2025-03-25 21:42:57,371 - log
        - save_interval : 1000 - 2025-03-25 21:42:57,371 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 21:42:57,371 - log
        - lora_dim : 4 - 2025-03-25 21:42:57,371 - log
        - lora_alpha : 32 - 2025-03-25 21:42:57,371 - log
        - obj : clm - 2025-03-25 21:42:57,371 - log
        - lora_dropout : 0.1 - 2025-03-25 21:42:57,371 - log
        - label_smooth : 0.1 - 2025-03-25 21:42:57,371 - log
        - roll_interval : -1 - 2025-03-25 21:42:57,371 - log
        - roll_lr : 1e-05 - 2025-03-25 21:42:57,371 - log
        - roll_step : 100 - 2025-03-25 21:42:57,371 - log
        - eval_epoch : 1 - 2025-03-25 21:42:57,371 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 21:42:57,371 - log
==================================================================================================== - 2025-03-25 21:42:57,371 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 21:42:57,371 - log
loading model pretrained weight. - 2025-03-25 21:42:58,498 - log
set max_step: 1050 - 2025-03-25 21:43:00,465 - log
start to train the model................ 1 - 2025-03-25 21:43:00,645 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 36.46 | loss  5.28 | avg loss  6.00 | ppl 403.55 - 2025-03-25 21:43:04,291 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 46.15 | loss  4.30 | avg loss  5.33 | ppl 206.28 - 2025-03-25 21:43:08,907 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 21:43:09,358 - log
start to train the model................ 2 - 2025-03-25 21:43:10,077 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 41.40 | loss  3.37 | avg loss  3.79 | ppl 44.31 - 2025-03-25 21:43:14,217 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 35.74 | loss  3.43 | avg loss  3.47 | ppl 32.21 - 2025-03-25 21:43:17,791 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 21:43:18,461 - log
start to train the model................ 3 - 2025-03-25 21:43:19,123 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 37.13 | loss  2.82 | avg loss  3.25 | ppl 25.73 - 2025-03-25 21:43:22,836 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 46.42 | loss  3.75 | avg loss  3.27 | ppl 26.34 - 2025-03-25 21:43:27,478 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 21:43:28,823 - log
start to train the model................ 4 - 2025-03-25 21:43:29,489 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 26.38 | loss  3.87 | avg loss  3.14 | ppl 23.02 - 2025-03-25 21:43:32,127 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 25.50 | loss  2.79 | avg loss  3.12 | ppl 22.70 - 2025-03-25 21:43:34,677 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 21:43:35,677 - log
start to train the model................ 5 - 2025-03-25 21:43:36,312 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 14.85 | loss  2.92 | avg loss  3.08 | ppl 21.78 - 2025-03-25 21:43:37,798 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 25.62 | loss  2.68 | avg loss  3.04 | ppl 20.97 - 2025-03-25 21:43:40,360 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 21:43:40,360 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 21:43:41,629 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 21:43:42,284 - log
End of training - 2025-03-25 21:43:42,284 - log
cleanup dist ... - 2025-03-25 21:43:42,457 - log
==================================================================================================== - 2025-03-25 21:53:12,942 - log
        - platform : local - 2025-03-25 21:53:12,942 - log
        - local_rank : 0 - 2025-03-25 21:53:12,942 - log
        - rank : 0 - 2025-03-25 21:53:12,942 - log
        - device : cuda:0 - 2025-03-25 21:53:12,942 - log
        - world_size : 1 - 2025-03-25 21:53:12,942 - log
        - random_seed : 2025 - 2025-03-25 21:53:12,942 - log
        - lr : 0.0002 - 2025-03-25 21:53:12,942 - log
        - weight_decay : 0.01 - 2025-03-25 21:53:12,942 - log
        - correct_bias : True - 2025-03-25 21:53:12,942 - log
        - adam_epislon : 1e-06 - 2025-03-25 21:53:12,942 - log
        - no_decay_bias : False - 2025-03-25 21:53:12,942 - log
        - adam_beta1 : 0.9 - 2025-03-25 21:53:12,942 - log
        - adam_beta2 : 0.999 - 2025-03-25 21:53:12,942 - log
        - scheduler : linear - 2025-03-25 21:53:12,942 - log
        - max_step : None - 2025-03-25 21:53:12,942 - log
        - max_epoch : 5 - 2025-03-25 21:53:12,942 - log
        - warmup_step : 500 - 2025-03-25 21:53:12,942 - log
        - i_steps : 0 - 2025-03-25 21:53:12,942 - log
        - i_lrs : 0.00025 - 2025-03-25 21:53:12,942 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 21:53:12,942 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 21:53:12,942 - log
        - train_batch_size : 2 - 2025-03-25 21:53:12,942 - log
        - valid_batch_size : 1 - 2025-03-25 21:53:12,942 - log
        - grad_acc : 2 - 2025-03-25 21:53:12,942 - log
        - clip : 0.0 - 2025-03-25 21:53:12,942 - log
        - seq_len : 64 - 2025-03-25 21:53:12,942 - log
        - model_card : gpt2.sm - 2025-03-25 21:53:12,942 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 21:53:12,942 - log
        - fp16 : False - 2025-03-25 21:53:12,942 - log
        - log_interval : 100 - 2025-03-25 21:53:12,942 - log
        - eval_interval : 2000 - 2025-03-25 21:53:12,942 - log
        - save_interval : 1000 - 2025-03-25 21:53:12,942 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 21:53:12,942 - log
        - lora_dim : 4 - 2025-03-25 21:53:12,942 - log
        - lora_alpha : 32 - 2025-03-25 21:53:12,942 - log
        - obj : clm - 2025-03-25 21:53:12,942 - log
        - lora_dropout : 0.1 - 2025-03-25 21:53:12,943 - log
        - label_smooth : 0.1 - 2025-03-25 21:53:12,943 - log
        - roll_interval : -1 - 2025-03-25 21:53:12,943 - log
        - roll_lr : 1e-05 - 2025-03-25 21:53:12,943 - log
        - roll_step : 100 - 2025-03-25 21:53:12,943 - log
        - eval_epoch : 1 - 2025-03-25 21:53:12,943 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 21:53:12,943 - log
==================================================================================================== - 2025-03-25 21:53:12,943 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 21:53:12,943 - log
loading model pretrained weight. - 2025-03-25 21:53:14,041 - log
set max_step: 1050 - 2025-03-25 21:53:15,333 - log
start to train the model................ 1 - 2025-03-25 21:53:15,451 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 27.55 | loss  5.28 | avg loss  6.00 | ppl 403.55 - 2025-03-25 21:53:18,206 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 25.01 | loss  4.30 | avg loss  5.33 | ppl 206.28 - 2025-03-25 21:53:20,707 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 21:53:20,960 - log
start to train the model................ 2 - 2025-03-25 21:53:21,652 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 22.38 | loss  3.37 | avg loss  3.79 | ppl 44.31 - 2025-03-25 21:53:23,891 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 25.08 | loss  3.43 | avg loss  3.47 | ppl 32.21 - 2025-03-25 21:53:26,399 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 21:53:26,902 - log
start to train the model................ 3 - 2025-03-25 21:53:27,606 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 19.98 | loss  2.82 | avg loss  3.25 | ppl 25.73 - 2025-03-25 21:53:29,604 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 24.95 | loss  3.75 | avg loss  3.27 | ppl 26.34 - 2025-03-25 21:53:32,099 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 21:53:32,845 - log
start to train the model................ 4 - 2025-03-25 21:53:33,552 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 17.23 | loss  3.87 | avg loss  3.14 | ppl 23.02 - 2025-03-25 21:53:35,275 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 24.92 | loss  2.79 | avg loss  3.12 | ppl 22.70 - 2025-03-25 21:53:37,767 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 21:53:38,764 - log
start to train the model................ 5 - 2025-03-25 21:53:39,469 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 14.68 | loss  2.92 | avg loss  3.08 | ppl 21.78 - 2025-03-25 21:53:40,937 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 24.94 | loss  2.68 | avg loss  3.04 | ppl 20.97 - 2025-03-25 21:53:43,431 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 21:53:43,431 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 21:53:44,682 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 21:53:45,438 - log
End of training - 2025-03-25 21:53:45,438 - log
cleanup dist ... - 2025-03-25 21:53:45,613 - log
==================================================================================================== - 2025-03-25 22:09:21,140 - log
        - platform : local - 2025-03-25 22:09:21,145 - log
        - local_rank : 0 - 2025-03-25 22:09:21,145 - log
        - rank : 0 - 2025-03-25 22:09:21,145 - log
        - device : cuda:0 - 2025-03-25 22:09:21,145 - log
        - world_size : 1 - 2025-03-25 22:09:21,145 - log
        - random_seed : 2025 - 2025-03-25 22:09:21,145 - log
        - lr : 0.0002 - 2025-03-25 22:09:21,145 - log
        - weight_decay : 0.01 - 2025-03-25 22:09:21,145 - log
        - correct_bias : True - 2025-03-25 22:09:21,145 - log
        - adam_epislon : 1e-06 - 2025-03-25 22:09:21,145 - log
        - no_decay_bias : False - 2025-03-25 22:09:21,145 - log
        - adam_beta1 : 0.9 - 2025-03-25 22:09:21,145 - log
        - adam_beta2 : 0.999 - 2025-03-25 22:09:21,145 - log
        - scheduler : linear - 2025-03-25 22:09:21,145 - log
        - max_step : None - 2025-03-25 22:09:21,145 - log
        - max_epoch : 5 - 2025-03-25 22:09:21,145 - log
        - warmup_step : 500 - 2025-03-25 22:09:21,145 - log
        - i_steps : 0 - 2025-03-25 22:09:21,145 - log
        - i_lrs : 0.00025 - 2025-03-25 22:09:21,145 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 22:09:21,145 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 22:09:21,145 - log
        - train_batch_size : 2 - 2025-03-25 22:09:21,145 - log
        - valid_batch_size : 1 - 2025-03-25 22:09:21,145 - log
        - grad_acc : 2 - 2025-03-25 22:09:21,145 - log
        - clip : 0.0 - 2025-03-25 22:09:21,145 - log
        - seq_len : 64 - 2025-03-25 22:09:21,145 - log
        - model_card : gpt2.sm - 2025-03-25 22:09:21,145 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 22:09:21,145 - log
        - fp16 : False - 2025-03-25 22:09:21,145 - log
        - log_interval : 100 - 2025-03-25 22:09:21,145 - log
        - eval_interval : 2000 - 2025-03-25 22:09:21,146 - log
        - save_interval : 1000 - 2025-03-25 22:09:21,146 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 22:09:21,146 - log
        - lora_dim : 4 - 2025-03-25 22:09:21,146 - log
        - lora_alpha : 32 - 2025-03-25 22:09:21,146 - log
        - obj : clm - 2025-03-25 22:09:21,146 - log
        - lora_dropout : 0.1 - 2025-03-25 22:09:21,146 - log
        - label_smooth : 0.1 - 2025-03-25 22:09:21,146 - log
        - roll_interval : -1 - 2025-03-25 22:09:21,146 - log
        - roll_lr : 1e-05 - 2025-03-25 22:09:21,146 - log
        - roll_step : 100 - 2025-03-25 22:09:21,146 - log
        - eval_epoch : 1 - 2025-03-25 22:09:21,146 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 22:09:21,146 - log
==================================================================================================== - 2025-03-25 22:09:21,146 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 22:09:21,146 - log
loading model pretrained weight. - 2025-03-25 22:09:22,271 - log
set max_step: 1050 - 2025-03-25 22:09:23,660 - log
start to train the model................ 1 - 2025-03-25 22:09:23,783 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 32.97 | loss  5.28 | avg loss  6.00 | ppl 403.55 - 2025-03-25 22:09:27,080 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 31.26 | loss  4.30 | avg loss  5.33 | ppl 206.28 - 2025-03-25 22:09:30,206 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 22:09:30,513 - log
start to train the model................ 2 - 2025-03-25 22:09:31,272 - log
==================================================================================================== - 2025-03-25 22:10:21,565 - log
        - platform : local - 2025-03-25 22:10:21,565 - log
        - local_rank : 0 - 2025-03-25 22:10:21,565 - log
        - rank : 0 - 2025-03-25 22:10:21,565 - log
        - device : cuda:0 - 2025-03-25 22:10:21,565 - log
        - world_size : 1 - 2025-03-25 22:10:21,565 - log
        - random_seed : 2025 - 2025-03-25 22:10:21,565 - log
        - lr : 0.0002 - 2025-03-25 22:10:21,565 - log
        - weight_decay : 0.01 - 2025-03-25 22:10:21,565 - log
        - correct_bias : True - 2025-03-25 22:10:21,565 - log
        - adam_epislon : 1e-06 - 2025-03-25 22:10:21,565 - log
        - no_decay_bias : False - 2025-03-25 22:10:21,565 - log
        - adam_beta1 : 0.9 - 2025-03-25 22:10:21,565 - log
        - adam_beta2 : 0.999 - 2025-03-25 22:10:21,565 - log
        - scheduler : linear - 2025-03-25 22:10:21,565 - log
        - max_step : None - 2025-03-25 22:10:21,565 - log
        - max_epoch : 5 - 2025-03-25 22:10:21,565 - log
        - warmup_step : 500 - 2025-03-25 22:10:21,565 - log
        - i_steps : 0 - 2025-03-25 22:10:21,565 - log
        - i_lrs : 0.00025 - 2025-03-25 22:10:21,565 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 22:10:21,565 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 22:10:21,565 - log
        - train_batch_size : 2 - 2025-03-25 22:10:21,565 - log
        - valid_batch_size : 1 - 2025-03-25 22:10:21,565 - log
        - grad_acc : 2 - 2025-03-25 22:10:21,565 - log
        - clip : 0.0 - 2025-03-25 22:10:21,565 - log
        - seq_len : 64 - 2025-03-25 22:10:21,565 - log
        - model_card : gpt2.sm - 2025-03-25 22:10:21,565 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 22:10:21,565 - log
        - fp16 : False - 2025-03-25 22:10:21,565 - log
        - log_interval : 100 - 2025-03-25 22:10:21,565 - log
        - eval_interval : 2000 - 2025-03-25 22:10:21,565 - log
        - save_interval : 1000 - 2025-03-25 22:10:21,565 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 22:10:21,565 - log
        - lora_dim : 4 - 2025-03-25 22:10:21,565 - log
        - lora_alpha : 32 - 2025-03-25 22:10:21,565 - log
        - obj : clm - 2025-03-25 22:10:21,565 - log
        - lora_dropout : 0.1 - 2025-03-25 22:10:21,565 - log
        - label_smooth : 0.1 - 2025-03-25 22:10:21,565 - log
        - roll_interval : -1 - 2025-03-25 22:10:21,566 - log
        - roll_lr : 1e-05 - 2025-03-25 22:10:21,566 - log
        - roll_step : 100 - 2025-03-25 22:10:21,566 - log
        - eval_epoch : 1 - 2025-03-25 22:10:21,566 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 22:10:21,566 - log
==================================================================================================== - 2025-03-25 22:10:21,566 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 22:10:21,566 - log
loading model pretrained weight. - 2025-03-25 22:10:22,676 - log
set max_step: 1050 - 2025-03-25 22:10:23,903 - log
start to train the model................ 1 - 2025-03-25 22:10:24,025 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 32.99 | loss  5.28 | avg loss  6.00 | ppl 403.55 - 2025-03-25 22:10:27,324 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 31.04 | loss  4.30 | avg loss  5.33 | ppl 206.28 - 2025-03-25 22:10:30,428 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 22:10:30,737 - log
start to train the model................ 2 - 2025-03-25 22:10:31,523 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 27.83 | loss  3.37 | avg loss  3.79 | ppl 44.31 - 2025-03-25 22:10:34,306 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 31.16 | loss  3.43 | avg loss  3.47 | ppl 32.21 - 2025-03-25 22:10:37,423 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 22:10:38,043 - log
start to train the model................ 3 - 2025-03-25 22:10:38,829 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 25.29 | loss  2.82 | avg loss  3.25 | ppl 25.73 - 2025-03-25 22:10:41,359 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 31.22 | loss  3.75 | avg loss  3.27 | ppl 26.34 - 2025-03-25 22:10:44,481 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 22:10:45,419 - log
start to train the model................ 4 - 2025-03-25 22:10:46,193 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 21.73 | loss  3.87 | avg loss  3.14 | ppl 23.02 - 2025-03-25 22:10:48,366 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 31.21 | loss  2.79 | avg loss  3.12 | ppl 22.70 - 2025-03-25 22:10:51,488 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 22:10:52,737 - log
start to train the model................ 5 - 2025-03-25 22:10:53,481 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.07 | loss  2.92 | avg loss  3.08 | ppl 21.78 - 2025-03-25 22:10:55,389 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 31.11 | loss  2.68 | avg loss  3.04 | ppl 20.97 - 2025-03-25 22:10:58,500 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 22:10:58,500 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 22:11:00,045 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 22:11:00,799 - log
End of training - 2025-03-25 22:11:00,800 - log
cleanup dist ... - 2025-03-25 22:11:00,991 - log
==================================================================================================== - 2025-03-25 22:11:53,411 - log
        - platform : local - 2025-03-25 22:11:53,411 - log
        - local_rank : 0 - 2025-03-25 22:11:53,411 - log
        - rank : 0 - 2025-03-25 22:11:53,411 - log
        - device : cuda:0 - 2025-03-25 22:11:53,411 - log
        - world_size : 1 - 2025-03-25 22:11:53,411 - log
        - random_seed : 2025 - 2025-03-25 22:11:53,411 - log
        - lr : 0.0002 - 2025-03-25 22:11:53,411 - log
        - weight_decay : 0.01 - 2025-03-25 22:11:53,411 - log
        - correct_bias : True - 2025-03-25 22:11:53,411 - log
        - adam_epislon : 1e-06 - 2025-03-25 22:11:53,411 - log
        - no_decay_bias : False - 2025-03-25 22:11:53,411 - log
        - adam_beta1 : 0.9 - 2025-03-25 22:11:53,411 - log
        - adam_beta2 : 0.999 - 2025-03-25 22:11:53,411 - log
        - scheduler : linear - 2025-03-25 22:11:53,411 - log
        - max_step : None - 2025-03-25 22:11:53,411 - log
        - max_epoch : 5 - 2025-03-25 22:11:53,411 - log
        - warmup_step : 500 - 2025-03-25 22:11:53,411 - log
        - i_steps : 0 - 2025-03-25 22:11:53,411 - log
        - i_lrs : 0.00025 - 2025-03-25 22:11:53,411 - log
        - train_data : ./data/e2e/train.jsonl - 2025-03-25 22:11:53,412 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-03-25 22:11:53,412 - log
        - train_batch_size : 2 - 2025-03-25 22:11:53,412 - log
        - valid_batch_size : 1 - 2025-03-25 22:11:53,412 - log
        - grad_acc : 2 - 2025-03-25 22:11:53,412 - log
        - clip : 0.0 - 2025-03-25 22:11:53,412 - log
        - seq_len : 64 - 2025-03-25 22:11:53,412 - log
        - model_card : gpt2.sm - 2025-03-25 22:11:53,412 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-03-25 22:11:53,412 - log
        - fp16 : False - 2025-03-25 22:11:53,412 - log
        - log_interval : 100 - 2025-03-25 22:11:53,412 - log
        - eval_interval : 2000 - 2025-03-25 22:11:53,412 - log
        - save_interval : 1000 - 2025-03-25 22:11:53,412 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-03-25 22:11:53,412 - log
        - lora_dim : 4 - 2025-03-25 22:11:53,412 - log
        - lora_alpha : 32 - 2025-03-25 22:11:53,412 - log
        - obj : clm - 2025-03-25 22:11:53,412 - log
        - lora_dropout : 0.1 - 2025-03-25 22:11:53,412 - log
        - label_smooth : 0.1 - 2025-03-25 22:11:53,412 - log
        - roll_interval : -1 - 2025-03-25 22:11:53,412 - log
        - roll_lr : 1e-05 - 2025-03-25 22:11:53,412 - log
        - roll_step : 100 - 2025-03-25 22:11:53,412 - log
        - eval_epoch : 1 - 2025-03-25 22:11:53,412 - log
        - dist : <module 'torch.distributed' from '/home/gunsj/miniconda3/envs/lora_jittor/lib/python3.9/site-packages/torch/distributed/__init__.py'> - 2025-03-25 22:11:53,412 - log
==================================================================================================== - 2025-03-25 22:11:53,412 - log
--------------------------------------------------train-------------------------------------------------- - 2025-03-25 22:11:53,412 - log
loading model pretrained weight. - 2025-03-25 22:11:54,540 - log
set max_step: 1050 - 2025-03-25 22:11:55,752 - log
start to train the model................ 1 - 2025-03-25 22:11:55,874 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 33.24 | loss  5.28 | avg loss  6.00 | ppl 403.55 - 2025-03-25 22:11:59,198 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 31.25 | loss  4.30 | avg loss  5.33 | ppl 206.28 - 2025-03-25 22:12:02,324 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pt - 2025-03-25 22:12:02,638 - log
start to train the model................ 2 - 2025-03-25 22:12:03,422 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 28.39 | loss  3.37 | avg loss  3.79 | ppl 44.31 - 2025-03-25 22:12:06,261 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 31.31 | loss  3.43 | avg loss  3.47 | ppl 32.21 - 2025-03-25 22:12:09,392 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pt - 2025-03-25 22:12:10,016 - log
start to train the model................ 3 - 2025-03-25 22:12:10,944 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 25.20 | loss  2.82 | avg loss  3.25 | ppl 25.73 - 2025-03-25 22:12:13,464 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 31.31 | loss  3.75 | avg loss  3.27 | ppl 26.34 - 2025-03-25 22:12:16,595 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pt - 2025-03-25 22:12:17,533 - log
start to train the model................ 4 - 2025-03-25 22:12:18,356 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 21.82 | loss  3.87 | avg loss  3.14 | ppl 23.02 - 2025-03-25 22:12:20,538 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 31.34 | loss  2.79 | avg loss  3.12 | ppl 22.70 - 2025-03-25 22:12:23,671 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pt - 2025-03-25 22:12:24,926 - log
start to train the model................ 5 - 2025-03-25 22:12:25,746 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 19.08 | loss  2.92 | avg loss  3.08 | ppl 21.78 - 2025-03-25 22:12:27,654 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 31.37 | loss  2.68 | avg loss  3.04 | ppl 20.97 - 2025-03-25 22:12:30,791 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.pt - 2025-03-25 22:12:30,791 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pt - 2025-03-25 22:12:32,397 - log
---------------------------------------------------------------------------------------------------- - 2025-03-25 22:12:33,257 - log
End of training - 2025-03-25 22:12:33,257 - log
cleanup dist ... - 2025-03-25 22:12:33,440 - log
